{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3511f38a-da5e-4734-8bcd-2fa8c89515ca",
   "metadata": {},
   "source": [
    "# <center>大模型用于推荐系统的特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84296644-4ec2-4b12-b2e5-8cbda5053762",
   "metadata": {},
   "source": [
    "- **Step 1. 确定推荐系统的需求**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626d106-c607-4906-9355-2ede833afb0e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;开发一个推荐系统的功能链路，通过分析用户的历史对话记录，智能推荐最适合用户需求的课程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b583d1-2182-4930-b313-3781890b6301",
   "metadata": {},
   "source": [
    "- **Step 2. 数据说明**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333aa4e-8376-4590-8e66-b91a53e5b12f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们公司的课程分为四大类别，每类课程都通过PDF课件提供详细的教学内容，其中涵盖的技术领域：\n",
    "\n",
    "- 机器学习：涵盖从基础理论到高级应用的各个方面。\n",
    "- 深度学习：介绍深度学习的基础架构、算法及其在各行业中的应用。\n",
    "- 自然语言处理：涉及文本处理的技术和方法，包括文本挖掘和生成。\n",
    "- 在线大模型技术实战：探索大模型技术的部署、优化和应用案例。\n",
    "\n",
    "&emsp;&emsp;每个PDF课件包含多个一级和二级标题，这些标题明确区分不同的知识点，便于学习和复习：\n",
    "\n",
    "- 一级标题：代表主要的知识模块或课程大纲的核心部分。\n",
    "- 二级标题：在每个主要知识模块下细分更具体的话题或概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f426c95-ee47-4ec0-9de5-19c46c6668f2",
   "metadata": {},
   "source": [
    "- **Step 3. 数据处理**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e389ee-6b9a-4889-9313-08018a63bfea",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当大模型介入到数据处理流程的时候，我们需要考虑的就如RAG的Indexing过程一样：如何找到一种最合适的文本读取方式，更好的区分各个模块、内容，以保证收集到的数据都是相对完整且独立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6821cd3-1fc1-4bac-9af2-3dfcc9dc5117",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们推荐使用的是：Markdown 格式。 Markdown是一种轻量级标记语言，允许用户使用简单的语法格式化纯文本。它广泛用于创建结构化文档，特别是在 GitHub、Jupyter Notebook 和各种内容管理系统等平台上。当将数据输入 LLM 或 RAG 系统时，使用 Markdown 格式有几个好处："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69299c78-8c6e-47f2-92a7-4efaf3aecf84",
   "metadata": {},
   "source": [
    "1. 结构化内容：Markdown 会将内容组织为标题、列表、表格和其他结构化元素。这种结构有助于更好地理解和上下文保存。\n",
    "2. 富文本：Markdown 支持基本格式，例如粗体、斜体、链接和代码块。在输入数据中包含富文本可以增强语言模型的上下文。\n",
    "3. Markdown 允许嵌入超链接、脚注和参考。在构建元数据过程中，对于引用外部资源或提供额外的上下文至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda7fbd-98e8-484b-9d9f-81e88a01e179",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里，我们给大家推荐一个PDF --> Markdown的转换工具：PyMuPDF。官方：https://github.com/pymupdf/RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec437aa-57ab-44c7-be66-d90743fba03a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PyMuPDF能够从 PDF 页面中提取文本、图像、矢量图形，以及自 2023 年 8 月起还能够提取表格。每种对象类型都有自己的提取方法：一种用于文本，另一种用于表格、图像和矢量图形，同时合并了这些不同的提取，以生成一个通用的、统一的 Markdown 字符串，该字符串一致地表示整个页面的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84a4fe31-981f-4a5e-9210-dde0f9a3c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6520eaac-215c-4f35-b2a9-625a15eb146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "doc = pymupdf4llm.to_markdown(\"开源大模型课件/Ch 1 开源大模型本地部署硬件指南.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8311022-98f7-401f-ad31-0bba10d04b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 本地部署开源大模型\\n\\n## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型， **深入理解硬件与软件的需求至关重要。** 在硬件需求方面，关键\\n\\n是 **配置一台或多台高性能的个人计算机系统或租用配备了先进** **GPU** **的在线服务器** ，确保有足够的内存和存储\\n\\n空间来处理大数据和复杂模型。至于软件需求， **推荐使用** **Ubuntu** **操作系统** ，因其在机器学习领域的支持和\\n\\n兼容性优于 Windows 。编程语言建议以 Python 为主，结合 TensorFlow 或 PyTorch 等流行机器学习框架，并\\n\\n利用 DeepSpeed 等优化工具来提升大模型的运行效率和性能。\\n\\n所以在本系列课程中，我们将从硬件选择入手，逐步引导大家理解并掌握如何为大模型部署选择合适的\\n\\n硬件，以及如何高效地配置和运行这些模型，从零到一实现大模型的本地部署和应用。首先来看硬件方面，\\n\\n提前规划计算资源是必要的。目前，我们主要考虑以下两种途径：\\n\\n1. **配置个人计算机或服务器** ，组建一个适合大模型使用需求的计算机系统。\\n\\n2. **租用在线** **GPU** **服务** ，通过云计算平台获取大模型所需的计算能力。\\n\\n# 一、大模型应用需求分析\\n\\n大模型的本地部署主要应用于三个方面： **训练（** **train** **）、高效微调（** **fine-tune** **）和推理**\\n\\n**（** **inference** **）** 。这些过程在算力消耗上有显著差异：\\n\\n**训练** ：算力最密集，通常消耗的算力是推理过程的至少三个数量级以上。\\n\\n**微调** ：微调是在预训练模型的基础上对其进行进一步调整以适应特定任务的过程，其算力需求低于训\\n\\n练，但高于推理。\\n\\n**推理** ：推理指的是使用训练好的模型来进行预测或分析，是算力消耗最低的阶段。\\n\\n总的来说，在算力消耗上， **训练** **>** **微调** **>** **推理。**\\n\\n从头训练一个大模型并非易事，这不仅对个人用户，对于许多企业而言也同样困难。因此，如果个人使\\n\\n用，关注点应该放在 **推理和微调** 的性能上。在这两种应用需求下，对 **硬件的核心要求体现在** **GPU** **的选择上，**\\n\\n**对** **CPU** **和内存的要求并不高。** 无论是选择租用在线算力还是配置本地计算机，如果想在本地运行大模型，我\\n\\n们可以拆分成两个关注点：\\n\\n模型：选择什么基座模型或微调模型，这可以直接下载至本地。\\n\\n硬件：希望在什么硬件平台上来执行，可以分为 CPU 和 GPU 两大类。\\n\\n⼤部分开源⼤模型⽀持在 CPU 和 Mac M 系列芯片上运⾏，但较为繁琐且占⽤内存⾄少 32G 以上，因此\\n\\n更推荐在 GPU 上运⾏。针对本地部署大模型， **在选择** **GPU** **时，可以遵循的简单策略是：在满足具体的大模**\\n\\n**型的官方配置要求下，选择性价比最高的** **GPU** **。**\\n\\nGPU 的性能主要由以下三个核心参数决定：\\n\\n1. **计算能力** ：这是最关注的指标，尤其是 32 位浮点计算能力。随着技术发展， 16 位浮点训练也日渐普\\n\\n及。对于仅进行预测的任务， INT 8 量化版本也足够；\\n\\n2. **显存大小** ：大模型的规模和训练批量大小直接影响对显存的需求。更大的模型或更大的批量处理需要更\\n\\n多的显存；\\n\\n\\n-----\\n\\n处理大量数据时的性能通常也越好；\\n\\n注：显存带宽相对固定，选择空间较小。\\n\\n# 二、硬件配置的选择标准\\n\\n无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\n\\n（如微调或推理）都需要相应的硬件配置方案来支持。所以 **在选择硬件配置时应根据具体的模型需求和预期**\\n\\n**用途来确定。**\\n\\n因此，我们的建议是： **根据部署的大模型配置需求，先选择出最合适的** **GPU** **，然后再根据所选** **GPU** **的**\\n\\n**特性，进一步搭配计算机的其他组件，如** **CPU** **、内存和存储等，以确保整体系统的协调性和高效性能。最简**\\n\\n**单的匹配** **GPU** **的标准是显存大小和性价比。** 因为训练不纯粹看一个显存容量大小，而是和芯片的算力高度相\\n\\n关的。因为实际训练的过程当中，将海量的数据切块成不同的 batch size ，然后送入显卡进行训练。显存\\n\\n大，意味着一次可以送进更大的数据块。但是芯片算力如果不足，单个数据块就需要更长的等待时间显存和\\n\\n算力，必须要相辅相成。\\n\\n简单来说，在深度学习的训练和推理中， GPU 的显存主要用于以下几个方面：\\n\\n1. **权重存储** ：模型的参数，包括权重和偏置，都需要在显存中存储。这些参数是模型进行预测或分类所必\\n\\n需的。\\n\\n2. **中间过程数据存储** ：在模型计算的前向传播和反向传播过程中，会产生并且需要暂时存储大量的中间计\\n\\n算结果。这些数据同样存储在显存中。\\n\\n3. **计算过程** ： GPU 专门为并行处理大量的矩阵和向量运算而设计，这正是深度学习中常见的计算类型。这\\n\\n些计算直接在显存中进行，以利用 GPU 的高速运算能力。\\n\\n显存的大小和速度直接影响到模型的处理速度和能处理的模型大小。显存越大，意味着可以处理更大的\\n\\n模型和更复杂的计算任务，但同时也需要更多的能源和可能导致更高的成本。\\n\\n\" 芯片 \" 通常指的是集成电路，它们被集成到各种电脑硬件组件中，如 CPU 、 GPU 和主板等。 CPU 本身就\\n\\n是一种芯片。它是计算机的大脑，负责执行程序和处理数据。显卡上的核心组件是图形处理器\\n\\n（ GPU ），它也是一种芯片。 GPU 负责处理图形和视频渲染。\\n\\n**所谓的** **\"** **算力** **\"** **大小，通常指的是整个计算系统的处理能力，尽管在特定上下文中，它有时特指** **GPU** **的处**\\n\\n**理能力。**\\n\\n我们以 ChatGLM-6B 模型为例，官方给出的硬件配置说明如下：\\n\\n模型量化是一种用于优化模型的技术，特别是在推理时。它通过减少模型中使用的数值精度来减小模\\n\\n型的大小，加快推理速度，并降低内存和能源消耗。模型量化常用于将模型部署到资源受限的设备\\n\\n上，如手机或嵌入式系统，量化程度越高，对硬件的要求就会越低。 单精度通常指的是 32 位浮点数\\n\\n（ FP32 ），使用 32 位表示，包括 1 位符号位、 8 位指数位和 23 位尾数位。 FP32 是标准的训练和推理格\\n\\n式，但由于半精度（ FP16 ）提供了相似的结果且计算速度更快，更节省内存，因此在资源受限或需要\\n\\n\\n-----\\n\\n少，它的计算量就会越小，对应的输出结果的精度也就会越差。\\n\\n## 2.1 选择满足显存需求的 GPU\\n\\n关于如何选择 GPU ，当前市场 NVIDIA 和 AMD 是两大主要显卡生产商。但在人工智能、大数据、深度\\n\\n学习领域， NVIDIA （通常被称为 N 卡）几乎独占鳌头。主要原因还是 NVIDIA 在很早期就开始专注于 AI 和深度\\n\\n学习市场，开发了强大的软件工具和库，例如 cuDNN 、 TensorRT ，这些都是专门为深度学习优化的，与流\\n\\n行的深度学习框架（如 TensorFlow 、 PyTorch 等）紧密集成，同时 NVIDIA 的 CUDA （ Compute Unified\\n\\nDevice Architecture ）作为独特的平行计算平台和编程模型，它允许开发者利用 NVIDIA 的 GPU 进行高效的通\\n\\n用计算。这一点对于深度学习和大数据分析等需要大量并行处理的应用来说至关重要。\\n\\n**英伟达是一家什么公司？**\\n\\n这时候可能有小伙伴说了，英伟达是一家卖游戏显卡的，这个说法呢，对，但也不对，从财报来看，英\\n\\n伟达目前主要有四块业务，分别是游戏 GPU ，数据中心产品，自动驾驶芯片和其他业务。占比分别为\\n\\n33.6% ， 55.% ， 3.3% 和 7.4% 。游戏 GPU ，数据中心产品，自动驾驶芯片其实都可以归类为计算芯片这个门\\n\\n类下面，换句话说，如果从财报公布的业务情况来分析的话，英伟达确实就是一家卖计算芯片的公司。但如\\n\\n果真的把英伟达当做一家卖芯片的公司，那就大错特错了。英伟达的确是靠着游戏显卡起家，并且在人工智\\n\\n能爆发的现在靠着一手 AI 计算芯片市值突破了万亿美元，但其实它并不是一家卖芯片的公司，我对英伟达的\\n\\n定位是，它是一家卖 **人工智能系统** 的公司。这就有两个核心的概念，一个是英伟达的计算芯片，一个是英伟\\n\\n达针对自家芯片做的计算架构 CUDA ，二者缺一不可。这种定位就像智能手机时代的苹果公司，苹果依靠着 A\\n\\n系列芯片和 ios 操作系统收割了智能手机行业超过 80% 的利润。人工智能大发展的时代，英伟达就依靠着 GPU\\n\\n和计算芯片与 CUDA 计算架构，共同组成的 AI 生态系统赢得了市场青睐，根据相关机构的统计数据，在独立\\n\\n显卡领域，英伟达的市占率高达 85% ，在 AI 算力芯片领域，在未来可能达到 90% ，现在做深度学习，英伟达\\n\\n的卡就是刚需，没有其他的选择。\\n\\n因此，我们建议还是选择 NVIDIA 的显卡。如果对应的 ChatGLM-6B 模型的硬件配置说明，我们就可以\\n\\n这样选择 GPU 。理论上， **在进行少量对话时** **:**\\n\\n在选择显卡时，必须遵循的首要准则是：显卡的显存容量一定要高于大模型官方要求的最低显存配置。\\n\\n这是确保模型能够有效运行的基本要求。显存容量越大，其推理或微调的能力就会越强。当然，随着显存容\\n\\n量的增加，显卡的价格也相应提高。以下是目前最主流的几款大模型的显卡型号及其显存容量：\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|H100|80 GB|\\n|A100|80/40 GB|\\n|H800|80 GB|\\n\\n\\n\\nA800 80 GB\\n\\n\\n-----\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|4090|24 GB|\\n|3090|24 GB|\\n\\n\\n其组合形式可以分为以下四类：\\n\\n1. 纯 CPU ：基于不同架构的 CPU 配置，适用于不需要或不能使用 GPU 加速的场景。 **（不推荐）**\\n\\nx86 ( 如 Intel 或 AMD)\\n\\nARM ( 如 Apple 、 Qualcomm 、 MTK)\\n\\n2. 单机单卡：使用一块 GPU 进行计算，适用于大多数个人使用和一些中等计算负载的场景。 **（典型配置）**\\n\\nNvidia 系列 GPU\\n\\nAMD 系列 GPU\\n\\nApple 系列 GPU\\n\\nApple Neural Engine （较少见，支持有限）\\n\\n3. 单机多卡：在一台机器上使用多张 GPU 卡，适用于高计算负载的场景，如模型分割处理。 **（典型配置）**\\n\\n4. 多机配置：使用多台计算机进行集群计算，通常超出个人使用范围，主要用于从头预训练基座模型等高\\n\\n负载任务。\\n\\n所以，在单个显卡的显存容量不足以满足需求时，也可以采用多显卡配置来增加整体的显存容量。只要\\n\\n总显存超过官方推荐的配置要求就可以。此外，在选择显卡时，除了考虑整体显存容量，还要根据不同显卡\\n\\n的性能和成本进行权衡。根据具体需求和预算，决定是选择单张高性能显卡，还是部署多张成本效益更高的\\n\\n低版本显卡。实现最优的性价比。比如在理论上，在进行多轮对话时或需要微调时，采用单机多卡：\\n\\n## 2.2 主流显卡性能分析\\n\\n对于 NVIDIA 的显卡（ N 卡）卡来说，我们可以按照以下几个维度来划分：\\n\\n按照产品线划分：\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n\\n\\nGeForce\\n\\n系列（ G\\n\\n\\n消费级 GPU 产品线，注重提供高性能的图形处理能力和游戏\\n\\n特性 性价比高 适合游戏和深度学习推理 训练\\n\\n\\n主要面向游戏玩家和普\\n\\n通用户\\n\\n\\n-----\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n|Quadro 系列（P 系列）|专业级GPU产品线，针对商业和专业应用领域进行了优化， 适用于设计、建筑等专业图像处理。|设计、建筑、专业图像 处理等领域。|\\n|Tesla系 列（T系 列）|主要用于高性能计算和机器学习任务，集成深度学习加速 器，提供快速的矩阵运算和神经网络推理。|高性能计算、机器学习 任务等领域。|\\n|Tegra系 列|移动处理器产品线，用于嵌入式系统、智能手机、平板电 脑、汽车电子等领域，具备高性能的图形和计算能力，低功 耗。|嵌入式系统、智能手 机、平板电脑、汽车电 子等领域。|\\n|Jetson系 列|面向边缘计算和人工智能应用的嵌入式开发平台，具备强大 的计算和推理能力，适用于智能摄像头、机器人、自动驾驶 系统等。|边缘计算、人工智能、 机器人等领域。|\\n|DGX系列|面向深度学习和人工智能研究的高性能计算服务器，集成多 个GPU和专用硬件，支持大规模深度学习模型的训练和推 理。|深度学习、人工智能研 究和开发等领域。|\\n\\n\\n按照架构划分：\\n\\n\\n\\n\\n\\n\\n\\n|架构|年份|芯 片 代 号|特点|代表产品|\\n|---|---|---|---|---|\\n|Tesla|2006|GT|第一个通用并行计算架构，主要用于科学计算和 高性能计算。|Tesla C870/GeForce 8800 GTX|\\n|Fermi|2010|GF|引入CUDA架构、ECC内存等，用于科学计算、图 形处理和高性能计算。|Tesla C2050/GeForce GTX 480|\\n|Kepler|2012|GK|功耗效率和性能改进，引入GPU Boost技术，适 用于科学计算、深度学习和游戏。|Tesla K40/GeForce GTX 680|\\n|Maxwell|2014|GM|提高功耗效率，引入新技术如多层次内存系统， 应用于游戏、深度学习和移动设备。|Tesla M40/GeForce GTX 980|\\n|Pascal|2016|GP|16nm FinFET制程技术，加强深度学习和AI计算 支持，引入Tensor Cores，应用于深度学习和高 性能计算。|Tesla P100/GeForce GTX 1080|\\n\\n\\n-----\\n\\n**架构** **年份**\\n\\n\\n**特点** **代表产品**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|Col1|Col2|号|Col4|Col5|\\n|---|---|---|---|---|\\n|Volta|2017|GV|深度学习优化特性，如Tensor Cores，主要用于 深度学习、科学计算和高性能计算。|Tesla V100|\\n|Turing|2018|TU|实时光线追踪技术、深度学习技术，适用于游 戏、深度学习和专业可视化。|Tesla T4/GeForce RTX 2080 Ti|\\n|Ampere|2020|GA|第二代深度学习架构，更多Tensor Cores、改进 的Ray Tracing技术，应用于深度学习、科学计算 和高性能计算。|Telsa A100/GeForce RTX 3090|\\n|Ada Lovelace|2022|AD|专为光线追踪和基于AI的神经图形设计，第四代 Tensor Core，第三代RT Core，提高GPU性能。|GeForce RTX 4090|\\n|Hopper|2022|GH|下一代加速计算平台，支持PCIe 5.0，专用的 Transformer引擎，适用于大型语言模型和对话 AI，提供企业级AI支持。|Telsa H100|\\n\\n\\n按照应用领域划分：\\n\\n\\n\\n\\n\\n\\n\\n|类型|系列|描述|应用领域|代表产品|\\n|---|---|---|---|---|\\n|游戏娱 乐|GeForce RTX™系 列|面向大众消费级游戏和创作者用户的图形 加速卡。在性能、功耗和成本之间达到最 佳平衡点,提供极致的游戏和创作体验。|游戏、娱乐、 内容创作|如RTX 3090、RTX 4090等|\\n|专业设 计和虚 拟化|NVIDIA RTX™系 列|面向专业可视化和创意工作负载的高性能 GPU,提供强大的计算性能、大容量视频 内存等。服务于工业设计、建筑设计、影 视特效渲染等专业用户。|工业设计、建 筑设计、影视 特效渲染|高端专业可视 化工作站级显 卡|\\n|深度学 习、人 工智能 和高性 能计算|A系列、 H系列、 L系列、 V系列、 T系列|不同系列针对不同AI和计算需求。A系列 是AI计算加速器; H系列是AI超算; L系列 是边缘AI推理; V系列是虚拟工作站; T系 列是AI推理优化解决方案。|数据中心AI训 练和推理、边 缘AI、虚拟桌 面、AI推理加 速|A100、 A30、A40、 H100、 L40、 DeepStream 加速器等|\\n\\n\\n像大模型领域这种生成式人工智能，需要强大的算力来生成文本、图像、视频等内容。在这个背景下，\\n\\nNVIDIA 先后推出 V100 、 A100 和 H100 等多款用于 AI 训练的芯片，其中 A100 是 H100 的上一代产品，于 2020\\n\\n年发布，使用 7 纳米工艺，支持 AI 推理和训练。而 H100 ，该显卡是 2022 年 3 月发布，可谓是核弹级性能显\\n\\n卡，采用了台机电 4 纳米工艺，具备 800 亿个晶体管，采用最新 Neda Hopper 架构，同时显存还支持\\n\\nhbm3 ，最高带宽可达 3TB 每秒。第四代 MNLINK 的带宽， 900G 每秒。是 PCIE5.0 的 7 倍，比上一代的 A100 显\\n\\n\\n-----\\n\\n飞跃，各项基础性能是 A100 的三倍之多， H100 的单片显卡售价 24 万元左右。\\n\\n但在 2022 年 10 月，漂亮国政府为了限制我国的人工智能发展，发布禁令：禁止 NVIDIA 向中国出售 A100\\n\\n和 H100 显卡。数据显示， 2022 年中国市场的人工智能芯片规模高达 70 亿美元，而这 70 亿的市场，被 NVIDIA\\n\\n垄断了 90% ，虽然 NVIDIA 的 A100 ， H100 这样的顶级芯片不能卖给中国，但 NVIDIA 作为商业公司，也是要做\\n\\n生意的，于是为了合规， NVIDIA 针对传输速率进行了限制，提出了中国大陆特供版的 A800 和 H800 ，即：\\n\\nH100 、 A100 的阉割版。\\n\\n也就是说，由于漂亮国的禁令，我们现在使用的 GPU 都是中国特供版的，说白了就是阉割版的，像\\n\\nA100 ，到国内就成了 A800 ， H100 到国内就成了 H800 ，那么 A ~ H 的差距在哪里呢？\\n\\n直接用 SXM 版本的 H800 进行对比，只能说这个参数对比，对于不了解的人来说，还是比较出人意料\\n\\n的，除了 FP64 和 NVLink 传输速率上的明显削弱，其他参数和 H100 都是一模一样的。 FP64 上的削弱主要影\\n\\n响是 H800 在科学计算、流体计算、有限元分析等超算领域的应用，受到影响最大的还是 NVLINK 上的削减，\\n\\n但因为架构上的升级，虽然比不上同为 Hoper 架构的 H100 ，但是比 AMPERRE 架构的 A100 还是要强上不少，\\n\\n说白了，老黄想要抓住国内市场，就算是阉割，也不会阉割的特别过分，漂亮国政府想限制国内的超算，那\\n\\n就算把超算性能砍掉，传输速率减小，换个名字， GPU 照卖。只要保证 H800 在大部分场景下的性能不受影\\n\\n响，能满足大部分人的使用需求就足够了。毕竟也不会有人跟钱过不去，所以 其实 H800 和 H 100 的性能差\\n\\n距并没有想象的那么夸张，就算是砍掉了 FP64 和 NVLINK 的传输速率，性能依旧够用。最关键的是，它合法\\n\\n呀。所以如果不是追求极致性能的话，也没必要冒着风险去选择 H100 。\\n\\n而就在今年的 10 月份，漂亮国又玩起了变卦， 10 月份刚升级了芯片禁令，开启了新一轮的出口管制，先\\n\\n预留了 30 天的窗口期，随后又要求立即生效，连 30 天都没了，也就是说，从 10 月份开始，中国将无法再获得\\n\\nNVIDIA5 类的 GPU 显卡 （ A800 、 A800 、 H100 、 A100 ， L40S ），其实早在 8 月份的时候， BAT 的一些大厂不\\n\\n知道是收到风声还是控制风险，就向 NVIDIA 提前订购了 10 万个 A800 芯片，结果这次也是彻底泡汤。其实从\\n\\n\\n-----\\n\\n的尖端 AI 芯片了，漂亮国就是亮牌，高端 AI 芯片，必禁无疑。所以对于目前的 A100 系列和 H100 系列，因为\\n\\n是漂亮国断供之前出的芯片，所以现在国内还有货，只不过市场渠道比较乱，需要甄别。\\n\\n同时需要说明的是， GeForce 系列显卡虽被官方定位为面向消费级市场，适合游戏爱好者。但这类显卡\\n\\n在深度学习领域同样展现出了出色的性能，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡 Tesla\\n\\n系列比起来其实差不太多，但是性价比却高很多。对于大模型来说，同样可以使用 GeForce 系列显卡。\\n\\n那么个人使用或者实验室针对大模型的推理和微调需求配置服务器，高端显卡目前我们可选的就是\\n\\nA100 、 A800 、 H100 和 4090 等，应该如何选呢？\\n\\n## 2.3 单卡 4090 vs A100 系列\\n\\n先说结论： **没有双精度需求，追求性价比，选** **4090** **。有双精度需求，选** **A100** **，没有** **A100** **选** **A800** **。如果**\\n\\n**是做大模型的训练，** **GeForce RTX 4090** **是不行的。但在执行推理（** **inference/serving** **）任务时，使用**\\n\\n**RTX 4090** **不仅可行，而且在性价比方面甚至略优于** **A100** **。同时如果做微调，也勉强是可以的，但建议多**\\n\\n**卡。**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|GPU 型号|Tensor FP16 算力|Tensor FP32 算力|内存 容量|内存 带宽|通信 带宽|通信 时延|售价（美元）|\\n|---|---|---|---|---|---|---|---|\\n|H100|989 Tflops|495 Tflops|80 GB|3.35 TB/s|900 GB/s|~1 us|30000~40000|\\n|A100|312 Tflops|156 Tflops|80 GB|2 TB/s|900 GB/s|~1 us|15000|\\n|4090|330 Tflops|83 Tflops|24 GB|1 TB/s|64 GB/s|~10 us|1600|\\n\\n\\n**推理**\\n\\n从数据对比来看， A100 和 GeForce RTX 4090 显卡在通信能力和内存容量方面存在显著差异，但在算力\\n\\n上差距并不大。在 FP16 算力方面，两者几乎相当， 4090 甚至略有优势。相较于 A100 ，其较高的性价比主\\n\\n要源于推理过程通常涉及单一模型，在这种场景下，显卡的算力才是关键因素，而 4090 在这方面表现出\\n\\n色。虽然内存带宽同样重要，但在推理任务中， 4090 的内存带宽通常足以应对需求，不会成为显著的制约\\n\\n因素。\\n\\nLambdaLabs 有个很好的 GPU 单机训练性能和成本对比： [https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)\\n\\n， 我们来看：\\n\\n\\n-----\\n\\n高的。\\n\\n**微调**\\n\\n反观训练需求下， 4090 在训练的时候表现不佳的原因主要是其有限的通信能力和内存容量。比如训练\\n\\nLLaMA-2 70B 时需要 2400 块 A100 ，同时据说训练 ChatGPT 用了上万块 A100 ，主要还是因为训练过程除了\\n\\n存储模型参数外，还需要处理大量数据以及各层之间的中间数据和参数。因此，大容量内存和高通信带宽会\\n\\n比较关键，以便高效地处理和协调这些信息。首先就是把 n 个 T 的数据，分发到不同的 GPU 上去，然后训练，\\n\\n这叫数据并行。第二个并行就是会把这个模型的数据在一块 GPU 里可能放不下，所以要按照每一层，把某几\\n\\n层放在不同的 GPU 上面，进行串联。这就叫流水线并行。第三个就是 Tensor 张量并行。主要是我们目前训练\\n\\n的 Transform 模型都是多头的，每一个头都是可以按照张量来进行并行训练。所以整个 LLaMA-2 70B 他会通\\n\\n过张量，流水线、数据三种并行方式，从模型内层，到模型层之间，和训练数据三个维度进行计算空间的划\\n\\n分。\\n\\n2400 块 GPU 之间要进行大量的协调和通讯计算，这种复杂的并行结构需要 GPU 之间进行大量的协调和\\n\\n通信。 4090 的通信带宽仅为 64 GB/s ，与 A100 的 900 GB/s 相比差距过大，导致在这类大规模训练任务中\\n\\n通信成为瓶颈，进而影响整体性价比。因此，尽管 4090 在某些方面表现优秀，但在大模型训练中的局限性\\n\\n仍然明显。尽管微调过程对硬件的要求相较于训练相对较低，但这个过程仍然需要足够的内存以存储模型参\\n\\n数，以及有效的通信带宽来处理数据和模型层之间的交互。所以对于需要高通信带宽和大内存容量的大模型\\n\\n微调任务， A100 等高端 GPU 可能是更合适的选择。\\n\\n我们拿 GPT 3 来说， GPT 3 的参数将近 700 亿，假设每个参数使用 4 字节（通常使用 float 32 ）进行存\\n\\n储，训练运算储备需求是 4200 GB ，完成一次 GPT 3 训练的总算力是： 3.15 * 10 ^23 Flops, 仅考虑算力的情\\n\\n况下，单块 A100 需要 45741 天，几乎是 128 年（假设有效算力是 78Tflpos ），单块 4090 需要 91146 天，几\\n\\n乎是 250 年，（假设有效算力是 40 Tflpos ）。任何一张单卡训练一次都需要超过 100 年，对于参数量达到 10\\n\\n亿级别的大模型，参数本身就大，而且大模型通常需要更高的显存来来存储参数、中间计算结果和梯度等，\\n\\n既然要多卡运行，数据的同步效率就会显得非常重要，那么内存带宽、通信带宽、通信延时等性能将非常非\\n\\n常重要。， 4090 24g 的显存小，而且内存带块、通信贷款、和通信延时都相对较弱，这就有点像短板理论。\\n\\n最弱的那一项就决定了显卡的能力。综上， 4090 在较大的大模型没有什么发挥的余地，但随着现在的大模型\\n\\n越来越小，对显存和算力需求相对较小的大模型，再加上推理的算力需求更低，如 LLama 7B 13B 模型，单\\n\\n卡的 4090 都可以运行，至少对于学习和研究大模型的个人或实验室来说， 4090 还真是不错的选择。\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少 4 张 A100 80G 显卡；（ ChatGLM6B 模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.4 单卡 4090 vs 双卡 3090\\n\\n如果预算差不多的情况下，对于两张 3090 与一张 4090 的选择，推荐使用两张 3090 显卡。虽然从算力角\\n\\n度看，两张 3090 与一张 4090 大致持平，但两张 3090 显卡提供的总显存会更多，这对于处理大型模型尤为重\\n\\n要。目前，大多数深度学习计算框架都支持各种并行计算技术，如流水线并行、张量并行和 CPU 卸载。这些\\n\\n技术使得即使是显存较小的显卡也能处理大型模型。在这种情况下，双 3090 配置可以更有效地利用流水线并\\n\\n行，同时，与单 4090 配置相比， CPU 卸载的需求会降低。此外，企业环境一般都是多卡并行，使用双 3090 配\\n\\n置还可以练习如何写多卡代码。现有技术如串行反向传播，进一步增强了多卡系统的效率，使其成为一个经\\n\\n济高效的选择，尤其是在需要处理大量数据和复杂模型的情况下。因此，从目前的技术和应用需求来看，选\\n\\n择两张 3090 显卡无疑是更优的选择。\\n\\n## 2.5 风扇卡与涡轮卡如何选择\\n\\n\\n-----\\n\\n风扇卡与涡轮卡的供电接口位置不同，涡轮卡的供电接口位置在接口尾部，供电线比风扇卡的线\\n\\n更短，这样是方便安装和理线，而风扇卡供电接口一般在显卡顶部，接线后线缆会高于机箱最高\\n\\n面，在服务器中使用风扇卡，服务器盖板盖不上。\\n\\n在散热方向上面，涡轮卡散热方向是朝尾部散热，并于服务器风向是一致的，而风扇卡的散热是朝四面\\n\\n八方来散热的，平常的 PC 机箱放一张是可以适应的，但用作服务器上（很多时候是多卡）就不适合了，\\n\\n很容易因为温度过热出现宕机。\\n\\n风扇卡与涡轮卡的尺寸大小不同\\n\\n涡轮卡与风扇卡的尺寸大小也是不一样的，风扇卡的尺寸一般是 2.5-3 倍宽设计，而涡轮卡的尺寸\\n\\n大小是双宽设计，因为涡轮卡为了方便放入服务器里，所以涡轮卡的尺寸和高度都远远低于风扇\\n\\n卡，从而服务器可以支持 4 卡或者 8 卡，如果用风扇卡代替涡轮卡装在服务器里，那位置够不够还\\n\\n是一回事儿呢。\\n\\n面对市场不同\\n\\n风扇卡无论是公版显卡还是非公版显卡，风扇卡都是面向个人的，是应用在个人游戏行业的，\\n\\n4090 风扇卡的特点就是外观炫酷，而个人游戏行业就是为了风扇卡的外观和玩游戏的性能。而\\n\\n4090 涡轮卡是定制版，是面向 AI 科技产业，因为做工精巧、支持多卡安装、性价比高等一系列优\\n\\n点， 4090 涡轮卡深受广大 AI 深度学习用户的喜爱。\\n\\n## 2.6 整机参考配置\\n\\n确定 GPU 后，根据 GPU 搭配合适的计算机组件，具体来说，计算机八大件： CPU 、散热器、主板、内\\n\\n存、硬盘、显卡、电源、风扇、机箱。个人使用的计算机，典型的配置是单 GPU 或双 GPU ，一般不超过四个\\n\\nGPU ，否则常规的机箱放不下，且运行时噪声很大，而且容易跳闸。\\n\\n目前国内实验室主流的还是 4090 和 3090,10 万 + 的预算配置 4 张 4090 是没问题的， 20~30 万的预算则可以\\n\\n考虑 8 张 4090 ，或者两张 A100 80G ，如果预算不限， A100 8 卡服务器一定是最佳选择。\\n\\n这里给出一个本地部署 ChatGLM-6B ，同时也适用于大多数消费级实验环境的配置：\\n\\nGPU ： 3090 双卡，涡轮版；总共 48G 显存，能够适⽤于⼤多数试验和复现性质深度学习任务；同时双卡\\n\\n也便于模拟多卡运⾏的⼯业级环境；\\n\\nCPU ： AMD 5900X ； 12 核 24 线程，模拟普通服务器多线程设置；\\n\\n存储： 64G 内存 +2T SSD 数据盘；内存主要考虑机器学习任务需求；\\n\\n电源： 1600W 单电源；双卡 GPU 的电源在 1200W-1600W 均可；\\n\\n主板：华硕 ROG X570-E ；服务器级 PCE ，⽀持双卡 PCIE ；\\n\\n机箱： ROG 太阳神 601 ； atx 全塔式⼤机箱，便于⾼功耗下散热；\\n\\nA800 工作站的典型配置信\\n\\n|配置项|规格|\\n|---|---|\\n|CPU|Intel 8358P 2.6G 11.2UFI 48M 32C 240W *2|\\n|内存|DDR4 3200 64G *32|\\n\\n\\n\\n数据盘 960G 2.5 SATA 6Gb R SSD *2\\n\\n\\n-----\\n\\n|配置项|规格|\\n|---|---|\\n|硬盘|3.84T 2.5-E4x4R SSD *2|\\n|网络|双口10G光纤网卡（含模块）*1|\\n||双口25G SFP28无模块光纤网卡（MCX512A-ADAT ）*1|\\n|GPU|HV HGX A800 8-GPU 8OGB *1|\\n|电源|3500W电源模块*4|\\n|其他|25G SFP28多模光模块 *2|\\n||单端口200G HDR HCA卡(型号:MCX653105A-HDAT) *4|\\n||2GB SAS 12Gb 8口 RAID卡 *1|\\n||16A电源线缆国标1.8m *4|\\n||托轨 *1|\\n||主板预留PCIE4.0x16接口 *4|\\n||支持2个M.2 *1|\\n|原厂质保|3年 *1|\\n\\n\\n总的来说：\\n\\n3090 ⽐ 4090 综合性价⽐更⾼，不过 4090 计算速度⼏乎是 3090 的两倍，有需求亦可考虑升级，不过\\n\\n4090 需要的机箱空间更⼤、电源配置也要求更⾼；\\n\\n双卡 GPU 升级路线： 3090—>4090—>A100 40G （ 2.5w 左右） —>A100 80G （ 6~7w 左右）；\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少 4 张 A100 80G 显卡；（ ChatGLM6B 模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.7 显卡博弈的形式分析\\n\\n除此之外，在 2023 年 11 月 13 日老黄又放出来两个核弹。第一个核弹就是它推出了全新的超算 GPU\\n\\nH200 ，直接说是当世最强，听起来很嚣张，但其实一点没有吹牛。在 AI 超算领域，对手只有看 NVIDIA 车尾\\n\\n灯的份。从数据层面看， H200 强在大模型推理上，以 700 亿参数的 Llama2 二代大模型为例， h200 推理速度\\n\\n几乎比前代的 h100 快了一倍。而且能耗还降低了一半。显存从 h100 的 80GB ，直接拉到了 141gb ，带宽也从\\n\\n3.35TB/s ，提升到了 4.8TB/s ，最新的 GPU H200 ，跟前一代 H100 相比，最大的提升就是它的内存，达到了\\n\\n惊人的 1.15TB/s ，相当于在 1s 内传输了 230 步 FHD 的高清电影。如果每一部的容量按 5G 来算的话。这个跟我\\n\\n们以前的计算机里的内存条就不一样了，它采用的最新技术是 HBM3e ， HBM 就是高带宽内存，这个实现是\\n\\n把 DRAM 内存用 3D 封装的技术叠了起来，然后把它和 GPU 芯片放在同一个 GPU 的底板上，它们之间的通信就\\n\\n通过这块晶元直接做了，这样内存的容量就大大提高了，同时他们和 GPU 的通信速度也有显著的增长，。达\\n\\n到了每秒钟 4.8 个 TB 。然后又把所有的软件做了优化，这样就使得 ChatGPT 这样 大模型的推理速度大大的提\\n\\n升，跟 A100 相比提高了 18 倍。第二个核弹就是 CPU 和 GPU 的合体， GH200 ， 就是把 ARM 的 CPU 和它的 GPU\\n\\n封装在了同一块 GPU 晶圆板上，这样 CPU 和 GPU 之间的传输速度就非常快，而且可以共享内存。内存也达到\\n\\n\\n-----\\n\\n有 1/2 。\\n\\n炸一听好像是王炸升级，刚装满 h100 的企业要哭晕在厕所了。但实际上，它可能只是 h100 的一个中期\\n\\n改款，单论峰值算力， H100 和 H200 其实是一模一样的。，真正提升的是显存和带宽，然而对于 AI 芯片的性\\n\\n能，讨论最多的是训练能力。，在 GPT 3 175B 大模型的训练中， H200 相较于 H100 ，只强了 10% ，提升并不\\n\\n明显，这操作，大概率是老黄有意为之，以前为了打造大模型，对 GPU 的首要要求是训练，但是到了现在，\\n\\n随着各种 AI 大预言模型的落地，大家开始卷的是推理速度。于是 H200 的升级，就忽略了算力升级，转向推理\\n\\n方面的发力，老黄的刀法依旧精准。哪怕只是小提升，依然当得起最强的称号。谁让 NVIDIA 的显卡，在 AI 芯\\n\\n片这块，这就是遥遥领先。\\n\\n但这因为是断供后的新卡，国内现在基本买不到。\\n\\n在 H200 没出现以前， H100 是地表最强 GPU ， NVIDIA 每一个层级的性能基本都是翻倍的， H100 ，其中\\n\\n微软采购了 15w 片， mate 采购了 15w 片，谷歌、亚马逊、甲骨文、腾讯都是 5w 片，那么谷歌的 gemini 发布\\n\\n晚，原来是因为缺少 GPU 哈。一共是 48w 片，和外界传的 一年 H100 的产量 50w 基本吻合。在 2024 年预计出\\n\\n货量在 200 万张。中国采购的用户 H800 要比 H100 量大，而且 H800 的售价比 H100 还要高，为什么性能不行\\n\\n价格还是高呢，主要原因还是有一份建议国企采购产品中的文件，里面只有 A800 和 H800 ，没有 A100 和\\n\\nH100 ，这就导致国企采购更愿意采购 A800 的原因，\\n\\n同时需要说的是，在今年的 10 月份，漂亮国再次禁用 H800 、 A800 芯片后， NVIDIA 计算再次推出中国特\\n\\n供 AI 芯片，初步计划是 3 款，分别是 h20 ， L20 和 L2 ，这三款基于 H100 进行阉割。使以性能符合禁令的要\\n\\n求。其中最强的是 H20 ，但与 H100 相比，性能被封印了 80% ，只有 H100 的 20% 左右的性能，对于 NVIDIA 而\\n\\n言，中国这笔 70 亿美元的大市场肯定不能丢，必须推出 AI 芯片来抢占，不过，近日有消息传出，这三款特供\\n\\n版芯片要跳票了，只有 L20 可能会按期推出， H20 和 L2 都可能延期。特别是 H20 这个最强的，什么时候推\\n\\n出，会不会推出都是未知数，最重要的原因，还是目前中国的市场已经发生了变化。没有 NVIDIA 想像的那么\\n\\n美好了。\\n\\n有一些朋友可能还不知道这回事，而知道的朋友有些也不清楚，为什么偏偏是显卡会成为国际博弈的工\\n\\n具，这些卡一张卖 十几 甚至几十万元，这么赚钱的生意，怎么就不让做了。在 1999 年之前的人类文明早\\n\\n期，世界上是没有显卡的，但已经有了电子游戏了，那时候的游戏画面是由 CPU 生成的，游戏玩家说，需要\\n\\n有高画质，于是就有了显卡。 1999 年， NVIDIA 声称自己发明了 GPU ，也就是 GFFORCE 256 ，所谓的 GPU ，\\n\\n就是图形计算单元，他是显卡最最核心的部件，再给他配上其他一系列零件，就成为了一张显卡。 GPU 跟显\\n\\n卡，严格来说不是一个概念，只是大家平时很少特意区分。这玩意为啥能提升游戏画质？因为渲染游戏画面\\n\\n这件事，难就难在计算量太大了，比如游戏中任何一个 3D 物体，它的位置、方向、大小、光源、物体表面等\\n\\n变化，都需要电脑来计算。\\n\\n渲染画面这件事，就像再做 10000 道加减乘除， CPU 的核心很强，但数量少。每个核心就像出于一个智\\n\\n力巅峰的高三学生，他能熟练的解出模拟卷上的最后一道大题，但让他算 1000 道，他得累死。而显卡上面\\n\\n密密麻麻的分布着几千个小核心，每个核心都像是一个小学生，高考题肯定是不会，但他们能同时并排启\\n\\n动，在 10 秒只能，把 10000 道题做完。所以渲染特别快。显卡能提升画质，就是因为他有这种强大的并行计\\n\\n算能力。而显卡从此脱离游戏领域，被别的领域盯上，乃至成为国际博弈的筹码。也是因为这种强大的并行\\n\\n计算能力，。发布了没几年，斯坦福大学实验室就盯上了显卡，它们想要显卡解决其他领域的问题，但是并\\n\\n不简单，显卡算力虽强，但是它们都是小学生，需要合适的软件能驾驭才行。沿用刚才的比方， GPU 就是一\\n\\n万个小学生在同时工作，计算能力很强，但前提是你得能把一道难解的大题，分解成无数个小学生能解决的\\n\\n简单问题才行。否则显卡再强又有什么用呢？转换到现实中，就是得让开发者能方便的写出代码。利用上显\\n\\n卡的并行计算能力才行。所以在 2006 年，带领团队出现了至今仍然在不断更新的 CUDA ， CUDA 就是更方便\\n\\n的让开发人员能够面向 编程 如果绝大多数 模型的训练 背后都离 开 的支持 除了\\n\\n\\n-----\\n\\n并行计算能力，在软件层面，配套的编程平台也成熟了，这就意味着， GPU 可以完全离开游戏领域，走向更\\n\\n大的世界了。\\n\\n第一次感受到 GPU ，就是挖矿，也就是挖比特币，挖矿其实就是用计算机 来解决数学问题，比如任何数\\n\\n据，都可以通过哈希算法生成一串哈希值，原始数据不管发生多小的变化，最后生成的哈希值都完全不同。\\n\\n我们有时候下载大文件时，也会利用哈希算法的这种特性，来做一次校验。。看看下载的文件是否完整。很\\n\\n多挖矿，就是要生成一个符合要求的哈希值，这就需要计算机去反复的尝试，所以挖矿就跟游戏画面一样，\\n\\n属于那种不难，但是计算量非常大的事情。恰好能够利用显卡的算力。于是在加密价格持续攀升的日子里，\\n\\n显卡涨价，缺货。一路推动 NVIDIA 的市值从 140 亿美元暴涨到了 1750 亿美元。但显卡跟加密货币之间只是一\\n\\n段露水情缘，随着专用矿机的出现，虚拟比特币的价格跳水，以太坊等调整了挖矿规则等原因，显卡跟虚拟\\n\\n货币的关系已经大不如前了。但显卡就跟上了更大、更革命的科技浪潮，就是 AI 。现在所有人都知道， AI 是\\n\\n可能引起新一轮科技革命的巨大产业，而几乎所有的 AI 模型训练，都需要显卡。\\n\\n就拿现在正火的 ChatGPT 来说，它的模型训练中涉及到大量的矩阵运算，这些矩阵运算本身不难，但是\\n\\n量很大很大，所以就需要 GPU 来并行处理。 AI 是可能改变世界的，而 AI 的基础是 算法、算力和数据。而提到\\n\\n的 A100 和 H100 ，售价高到十几万、甚至几十万的专业显卡，还供不应求。有报道说，训练 ChatGPT 需要相\\n\\n当于 300 块 A100 显卡的算力，光这一个项目就需要花几十个亿来购买显卡，这也是为什么 从 2022 年 10 月开\\n\\n始， NVIDIA 的市值在半年时间内就飙升了 34 倍。\\n\\n## 2.8 国产 AI 超算芯片期待\\n\\n这么着急赶尽杀绝，不惜拉上自己企业垫背。答案就是我国在半导体自主研发上，已经触碰到了他们的\\n\\n痛处。很多人总以为，我们依赖国外的 AI 芯片，是自身技术上不过关，其实真相是我们的芯片都没有真正的\\n\\n上过牌桌，为什么？ AI 芯片只有在实际应用中才能够发现问题，加快迭代，而我们的国产芯片，起步晚、性\\n\\n能差，所以国内的厂商大多不愿意使用，这也就造成了国产芯片无法获得正向反馈，发展速度只会越来越\\n\\n慢，这是一个矛盾的循环。在还有选择的时候，考虑到性能也好，成本也好，中国企业往往愿意选择像\\n\\nNVIDIA 的芯片，所以在很长一段时间，美国对中国的高端芯片的制裁，也不是彻底封死，因为国产的 AI 芯片\\n\\n不是它的对手，都还不够好用，但现在，局面彻底改变了。出口禁令全面升级，中国企业没有了其他的任何\\n\\n选择，下一步只能规模化的采购国产芯片，并且培育出本土的产业链，逐步实现真正意义上的国产替代。那\\n\\n可能有人说，这件事这么简单，能实现岂不是早就实现了。不太可能。其实还真不一定，放眼全球算力芯片\\n\\n市场，不可否认， NVIDIA 占据了九成多的份额，出于高度垄断的地位。但是，目前国产 AI 芯片的可替代方\\n\\n案，也不少。\\n\\n如果单看并行计算这个领域，有两家国产 GPU 公司值得关注：分别是摩尔线程和壁任科技。\\n\\n摩尔线程 2020 年 10 月成立，在 2023 年 10 月 17 日，第一款产品摩尔芯用了 7 纳米工艺，支持 CUDA 平台和\\n\\n算法模型，性能超过每秒 20 万亿次浮点计算，仅成立三年就上了白宫严选名单。成为老美的制裁对象之一。\\n\\n是现在唯一可能买到的实际产品，并且一直在更新驱动的公司。壁任科技一款产品 壁任一号， 7 纳米工艺，\\n\\n支持 CUDA 平台和算法模型，性能超过每秒 30 万亿次浮点计算。去年发布了一个 GPU 叫 BR100 ，性能就直逼\\n\\n英伟达的 H100 ，但是这个芯片并没有使用，原因还是台积电不给生产，准确的说是美国政府不让台积电给我\\n\\n们生产，这就是一个不公平的竞争，华为的遭遇大家就更熟悉了， 19 年以后 芯片的生产、制造全都被摁的死\\n\\n死的，但是麒麟芯片出来了，说明我们大概率已经突破了先进芯片制造的生产流程了，顶多就是成本高一\\n\\n点。\\n\\n这些当中除了华为之外都面临相同的问题，那就是没有针对芯片专门优化的计算架构，换句话说，这些\\n\\n公司不具备与 CUDA 抗衡的能力，如果能成为芯片供应商，去识别其他的计算架构，理论上也是可以的。但\\n\\n是对于使用者来说这样做效率就太低了。一旦训练十几天，一旦出现 BUG ，不就前功尽弃了吗。所以这事还\\n\\n是得看华为 华为就厉害了 在人工智能领域的布局包括但不限于昇腾系列计算芯片 CANN 异构计算架\\n\\n\\n-----\\n\\n可能布局出全栈式人工智能的公司，也是唯一一个全栈式国产化公司，困扰华为的最大问题还是美国的制\\n\\n裁，芯片没法生产，再怎么布局也是白搭，一旦能解决芯片问题，\\n\\n有句话说的好，天下苦英伟达久矣。有时候看似把我们逼入绝境，但其实是给我了我们绝地求生的机\\n\\n会，就看我们怎么把握了。尽管中国企业会迎来一段痛苦的过渡期，但从长远来看，这是一条不得不走的\\n\\n路。而面对当下的巨大差距，最关键的是终于有了攻坚克难的凝聚力。相信度过这段阵痛期，有的人，有的\\n\\n国家，只会后悔的拍大腿，眼睁睁的看着被逆袭、被超越，毕竟每一次我们都是这样过来的。\\n\\n# 三、组装计算机硬件选型策略\\n\\n计算机八大件： CPU 、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套\\n\\n需要部署大模型的个人计算机，如何搭配。\\n\\n## 3.1 GPU 选型策略\\n\\n1. **选择厂商**\\n\\n目前独立显卡主要有 AMD 和 NVIDIA 两家厂商。其中 NVIDIA 在深度学习布局较早，对深度学习框架支持\\n\\n更好。 **建议选择** **NVIDIA** **的** **GPU** **。**\\n\\n桌面显卡性能天梯图： [https://www.mydrivers.com/zhuanti/tianti/gpu/index.html](https://www.mydrivers.com/zhuanti/tianti/gpu/index.html)\\n\\n2. **选择系列及品牌**\\n\\n**对个人用户来说，就是从** **NVIDIA** **的** **RTX** **系列中，选择出合适的** **GPU** **。** 就部署大模型的需求来说，只需\\n\\n考虑计算能力和显存大小就可以了。显存带宽通常相对固定，选择空间较小。目前各级别显卡的均价如下：\\n\\n\\n-----\\n\\n|品牌|华硕|微星|技嘉|\\n|---|---|---|---|\\n|顶级旗舰||||\\n|旗舰|ROG猛禽|超龙X|大雕|\\n|次旗舰|TUF|魔龙|超级雕/小雕|\\n|中端|巨齿鲨|/|雪鹰/魔鹰|\\n|丐版|DUAL|万图师|猎鹰|\\n\\n\\n华硕显卡品控优做工好，高品牌信仰足，但溢价严重，这个牌子最会宣传，卡不错但性价比不是很高。\\n\\n高端优先推荐华硕 ROG 猛禽，当然缺点就是：贵，另外主流用户个人更推荐 TUF ，更低端的巨齿鲨和\\n\\nDUAL 不太推荐\\n\\n微星显卡 30 系列之前更推荐魔龙， 30 系列更推荐超龙\\n\\n**准一线**\\n\\n|品牌|七彩虹|\\n|---|---|\\n|顶级旗舰|九段|\\n|旗舰|火神/水神|\\n|次旗舰|adoc|\\n|中端|ultra|\\n|丐版|战斧|\\n\\n\\n\\n推荐七彩虹，用料良心，保修出名的好，深受矿老板们的喜爱，支持个人送保， ultra ，三风扇，散热性\\n\\n能极好，噪音小，白色颜值高，不带 rgb 灯效，喜欢 rgb 灯效的可以选择 adoc 。\\n\\n**二线**\\n\\n|品牌|影驰|索泰|映众|耕升|铭瑄|\\n|---|---|---|---|---|---|\\n|顶级旗舰||||||\\n|旗舰|名人堂|PGF/AMP|冰龙寒霜||MGG 大玩家|\\n|此旗舰|GAMER/星耀|天启|超级冰龙||电竞之心|\\n|中端|金属大师|/|冰龙|炫光/星极||\\n|丐版|黑将/大将|X-gaming|黑金至尊|追风|turbo/终结者|\\n\\n\\n\\n二线品牌中，影驰和索泰属于二线中实力比较强的两个，可以说是强二线，索泰重点推荐 PGF （排骨\\n\\n\\n-----\\n\\n级别的产品，颜值高，性能强，次旗舰 GAMER 和星耀一个主打 DIY 一个主打 RGB ，都是非常有特点的产品\\n\\n企业级显卡\\n\\n参考第二部分的 GPU 推荐。\\n\\n服务器推断卡\\n\\n除了用于训练，还有一类卡是用于推断的（只预测，不训练），如：\\n\\n这些卡全部都是不带风扇的，但它们也需要散热，需要借助服务器强大的风扇被动散热，所以只能在专\\n\\n门设计的服务器上运行，性价比首选 Tesla T4 ，但是发挥全部性能需要使用 TensorRT 深度优化，目前仍然\\n\\n存在许多坑，比如当你的网络使用了不支持的运算符时，需要自己实现。\\n\\n**避免踩坑**\\n\\n如果选择配置单机多卡，采购显卡的时候， **一定要注意买涡轮版的** ，不要买两个或者三个风扇的版本，\\n\\n除非只打算买一张卡。因为涡轮风扇的热是往外机箱外部吹的，可以很好地带走热量，如果买三个风扇的版\\n\\n本，插多卡的时候，上面的卡会把热量吹向第二张卡，导致第二张卡温度过高，影响性能。\\n\\n## 3.2 CPU 选型策略\\n\\nCPU 在大模型使用中起到什么作用？当在 GPU 上运行大模型时， CPU 几乎不会进行任何计算。最有用的\\n\\n应用是数据预处理。 CPU 负责将数据从系统内存传输到 GPU 的显存中，同时也处理 GPU 完成计算后的数据。\\n\\n有两种不同的通用数据处理策略，具有不同的 CPU 需求。\\n\\n训练时处理数据：高性能的多核 CPU 能显著提高效率。建议每个 GPU 至少有 4 个线程，即为每个 GPU 分\\n\\n配两个 CPU 核心。每为 GPU 增加一个核心 ，应该获得大约 0-5 ％的额外性能提升。\\n\\n训练前处理数据：不需要非常好的 CPU 。建议每个 GPU 至少有 2 个线程，即为每个 GPU 分配一个 CPU 核\\n\\n心。用这种策略，更多内核也不会让性能显著提升。\\n\\n在这种情况下， GPU 通常承担大部分计算负担， CPU 的作用更多是管理和协调，因此需要高核心数，同\\n\\n时也需要快速的数据预处理，同样需要高频率，所以 **高核心** **+** **高频率，虽然不是必须，但推我们推荐还是能**\\n\\n**高即高，标准是：要与选择的** **GPU** **和** **CPU** **的性能水平相匹配** ，避免将一款高端显卡与低端 CPU 或一款高性能\\n\\nCPU 与低端显卡匹配，因为这可能导致性能瓶颈。比如：\\n\\nNVIDIA GeForce RTX 3090 * 2 搭配 Intel Core i7-13700K/KF 或 AMD 5900X CPU ；\\n\\nNVIDIA GeForce RTX 3080 搭配 Intel Core i9-11900K CPU 。\\n\\n但相对来说，瓶颈没有那么大，一般以一个 GPU 对应 2~4 个 CPU 核数就满足基本需求，比如单卡机器买\\n\\n四核 CPU ，四卡机器买十核 CPU 。在训练的时候，只要数据生成器（ DataLoader ）的产出速度比 GPU 的消\\n\\n耗速度快，那么 CPU 就不会成为瓶颈，也就不会拖慢训练速度。\\n\\n\\n-----\\n\\n去很长的一段时间了里，英特尔一直是绝对霸主的存在，代表最先进的生产力，时至今日， AMD 在产品性能\\n\\n层面已经完全可以和 Intel 正面硬刚了。\\n\\nCPU 性能天梯图： [https://www.mydrivers.com/zhuanti/tianti/cpu/index.html](https://www.mydrivers.com/zhuanti/tianti/cpu/index.html)\\n\\n**Intel** **系列命名规范**\\n\\n可以通过 CPU 名称得到一些信息，如 i7-10700K ，代表产品型号是 i7 ，后面的 10 代表是第 10 代，然后 700\\n\\n代表性能等级高低， K 代表这个 CPU 可以超频，当然后缀字母还有 T 、 X 、 F 等， X 后缀代表高性能处理器，而 T\\n\\n代表超低电压， /F 代表无 CPU 无内置显卡版本。\\n\\n1. 系列：由低到高 Celeron （赛扬） / Pentium （奔腾） / 酷睿系列的 i3 / i5 / i7 / i9\\n\\n2. 世代：第 1 组数字代表是第几代\\n\\n例如这三个 CPU ： I7-8700 、 I7-9700 ， i7-10700 第 1 个是第八代，第 2 个是第九代、第 3 个是第十代，还\\n\\n是比较容易理解的。\\n\\n3. 性能：第 2 组 (3 个数 ) 是表示性能等级\\n\\n例如： I5-12400 、 I5-12500 ，数字越大表示越好。\\n\\n4. 后缀： K → 可超频， F → 没有核显\\n\\n可超频 K 版 CPU 要搭配可超频的 Z 系列主板才行，才可以超频，搭载不能超频的主板也可以用，只是不能\\n\\n超频了而已。\\n\\n没有核显的 F 版 CPU 要搭配独立显卡才能开机点亮屏幕\\n\\n超频简单理解就是可以提升处理器性能，核显就是把一张入门级的显卡塞进处理器里。\\n\\n**i3** **是家用级别，** **i5** **是游戏级别，** **i7** **是生产力和游戏发烧友级别，** **i9** **是最顶级的。后缀带** **K** **可以超频，带** **F**\\n\\n**表示没有核显。**\\n\\n**AMD** **系列命名规范**\\n\\n和 Intel 类似：\\n\\n1. 系列：由低到高 APU / Althlon （速龙） / Ryzen （锐龙）系列 R3 / R5 / R7 / R9\\n\\n2. 世代：第 1 个数字代表第几代\\n\\n3. 例如这两个 CPU ： R7-2700X 、 R7-3700X ，第 1 个是第二代，第 2 个是第三代。\\n\\n4. 性能：第 2 组数字（ 3 个数字）表示性能等级\\n\\n数字越大性能越好，例如 R7 3800X 的性能大于 R7 3700X 。\\n\\n5 后缀：字母 G 表示有核显 字母 X 没有明确意思 一般性能强一点 如 R5 3600X 比 R5 3600 性能高一\\n\\n\\n-----\\n\\n**要选** **Intel** **还是** **AMD** **，其实都可以。** 如果追求性价比， AMD 性价比高一些，如果主要玩游戏，且对价格\\n\\n不敏感，建议选择英特尔 Intel ，英特尔 Intel 一般主频较高，一些游戏主要依赖主频，所以高主频的 Intel 玩游\\n\\n戏更推荐一些。除了品牌维度的分析，目前 **主流的大模型训练硬件通常采用** **Intel + NVIDIA GPU** 。但具体\\n\\n情况具体分析，只能简单说：一分钱一分货，一般来说贵的好。\\n\\n**选购** **CPU** **误区**\\n\\n电子产品有一个说法是， “ 买新不买旧 ” ，一般新产品，会用更新的工艺架构，性能更强，功耗更低，比\\n\\n较值得购买。当然有些时候，老一代的性价比很高，也可以考虑，但如果是好几代前的老产品，就不要考虑\\n\\n了，有些商家会卖几年前的 i7 电脑主机，它的性能可能还不如最新的 i3 ，主要是忽悠小白的，要注意辨别。\\n\\n目前消费级市场，我们最常听到的 i3 i5 i7 等，是英特尔的酷睿系列产品，主要面向一般消费市场，数字\\n\\n大的性能更强，（注意这里只在同代产品中成立）。 AMD 与之对应的是 R3 R5 R7 。这里值得注意是，同代产\\n\\n品 i7 比 i5 强，如果拿老一代的 i7 和新一代的 i5 比，就未必成立，部分商家经常会营销 i5 免费升级 i7 ，其实是把\\n\\n最新一代的 i5 换成立了老一代的 i7 ，性能方面可能还不如没升级呢？比如 i5-8400 的性能就高于 i7-7700.\\n\\n## 3.3 散热选型策略\\n\\nCPU 不断地更新换代和性能提升，其功耗和发热量也越来越大，如果温度过高，就会出现自动关机或者\\n\\n是蓝屏死机等情况，所以需要单独的散热器来压制，目前 **CPU** **散热器分两种：水冷和风冷。**\\n\\n风冷和水冷系统都是用于 GPU 的散热解决方案。它们各有优势和不足：通常， **水冷系统在散热效率方面**\\n\\n**优于风冷系统。** 以 Intel 的 i9-13900KF 为例，这款 CPU 性能目前位于 CPU 性能天梯榜第二位，很多用户认为使\\n\\n用水冷系统是必要的。但如果这款 CPU 没有超频需求，使用高质量的风冷系统其实也能够有效地散热。只有\\n\\n在超频的情况下，水冷系统由于其更出色的冷却效果，才成为更佳的选择。\\n\\n但需要注意， **风冷和水冷与** **GPU** **无关** 。在计算机硬件中， CPU 和 GPU （显卡）的散热策略和要求各有不\\n\\n同。 CPU 通常需要配单独的散热器，我们可以根据需要选择并购买不同类型的散热器，例如水冷或风冷系\\n\\n统，并且可以根据性能要求进行升级。由于 CPU 的 **高主频和较少的核心数（通常是几个到二十几个核心）** ，\\n\\n高性能的 CPU 在运行时会产生较多热量，因此需要更有效的散热解决方案来维持合适的运行温度。与此相\\n\\n对，显卡通常采用一体化的散热设计，其散热器是显卡的重要组成部分。显卡散热器的设计已经由各厂商经\\n\\n过测试和优化，因此用户一般不需要担心显卡的散热问题。显卡采用的是 **多核心、低频率** 的策略，即使是高\\n\\n端显卡如 Nvidia 的 4090 ，其频率也相对较低，通常在 3000MHz 左右，而同代的高端 CPU （如 Intel i9 ）的频\\n\\n率可能是其两倍。显卡的散热器可以直接接触 GPU 核心和显存，从而高效散热。因此，在正常满载情况下，\\n\\n显卡的温度达到 70 多或 80 多度是正常现象，通常不会成为性能瓶颈。\\n\\n对于大模型部署来说，首要原则还是 **CPU** **的等级要和** **GPU** **相匹配** 。对于中低端处理器，如 Intel 的 i5 系\\n\\n列，以及 AMD 的 R5 和 R7 系列，一般推荐使用风冷系统。这些处理器的热设计功耗（ TDP ）通常较低，风冷系\\n\\n统足以提供有效的散热。而对于更高性能的处理器，如 Intel 的 i7 13700KF 及更高级别的 i7 和 i9 系列，建议至\\n\\n少使用 240mm 规格的水冷系统。考虑到这些处理器较高的性能和热输出，水冷系统能提供更为高效和稳定\\n\\n的冷却效果。因此，尽管风冷在某些情况下依然可行，但为了确保最佳性能和稳定性，对于高性能处理器，\\n\\n\\n-----\\n\\n在构建大模型的系统时，低端主板通常不适用。根据所选的 CPU 和 GPU 规格，应从中端或高端主板中选\\n\\n择出合适的。\\n\\n\\n\\n\\n\\n\\n\\n|制造 商|系列|定 位|支持CPU超 频|支持内存超 频|适用用户|\\n|---|---|---|---|---|---|\\n|Intel|Z系 列|高 端|是|是|追求高性能和定制化设置的用户|\\n|Intel|B系 列|中 端|否|是|需要一定性能但预算有限的用户|\\n|Intel|H系 列|低 端|否|否|预算有限或对性能要求不高的基本用 途|\\n|AMD|X系 列|高 端|是|是|追求最高性能和高度定制化的用户|\\n|AMD|B系 列|中 端|否|是|寻求性价比的用户|\\n|AMD|A系 列|低 端|否|否|有限预算或基本计算需求的用户|\\n\\n\\n选择主板时，核心因素是 **确保它与** **CPU** **的性能和超频能力相匹配** 。以 Intel 处理器为例：对于中高端 CPU\\n\\n（如 i5 系列及以上），更适合选择 B660 到 Z690 系列的主板。对于如 13600KF 这样的高性能 CPU ，至少应选择\\n\\nB660 系列的主板作为起点。需要考虑的是 CPU 是否支持超频（如带有 “K” 后缀）。可超频的 CPU 更适合搭配\\n\\n支持超频的高端主板，如 Z 系列\\n\\n其次，需要 **检查** **CPU** **和主板型号是否匹配及合理。**\\n\\n通常情况下，每一种型号的 CPU 都需要搭配对应型号的主板，每代 CPU 和主板都有自己的针角及接口类\\n\\n型， Intel cpu 不能用于 AMD 系列主板，某些主板可能会通用几代 cpu ，但有的主板只能兼容某一代，例如\\n\\nintel 十代 的 i510400f ，不能用于早期的四代 b85 系列主板，而是否匹配，指的是高性能 CPU 搭配低性能主\\n\\n板， h610 是入门主板，虽然可以点亮，但是低端主板因为供电有限，无法发挥出 cpu 的全部性能，以及无法\\n\\n超频，这样就失去了 cpu 本身的性能和意义。\\n\\n最后， **考虑** **PCIe** **通道。**\\n\\nPCIe 通道是一种高速接口，用于将 GPU 连接到计算机的主板。通过这些通道， GPU 可以与 CPU 以及系统\\n\\n内存快速交换数据。每个 PCIe 通道（或称为 “ 通道 ” ）都提供一定的数据传输带宽。更多的通道意味着更高的\\n\\n总体带宽。例如， PCIe 3.0 x16 接口意味着有 16 个通道，每个通道的速度是 PCIe 3.0 标准的速度。\\n\\nGPU 的性能部分取决于它与主板之间的通信速度，这是由 PCIe 通道的数量和版本（如 PCIe 3.0 、 4.0 或\\n\\n5.0 ）决定的。更高版本的 PCIe 提供更高的传输速率，从而可能提高 GPU 的性能。以下是需要考虑的几个关键\\n\\n点：\\n\\n1. **PCIe** **版本** ：\\n\\n\\n-----\\n\\n4.0 和 5.0 ）提供更高的数据传输速率，这对于高性能 GPU 和其他高速设备非常重要。\\n\\n2. **PCIe** **槽数量和布局** ：\\n\\n主板上的 PCIe 槽数量决定了可以安装多少个扩展卡。如果计划安装多个 GPU 或其他 PCIe 设备，需\\n\\n要确保主板有足够的槽位。\\n\\n槽位布局也很重要，尤其是在安装大型 GPU 时，需要确保它们之间有足够的空间，避免过热或物\\n\\n理干扰。\\n\\n3. **PCIe** **通道分配** ：\\n\\n主板上的 PCIe 通道是从 CPU 和芯片组分配的。不同的主板可能有不同的通道分配方式，这可能会\\n\\n影响到扩展卡的性能，特别是在多 GPU 配置中。\\n\\n确认主板是否支持您所需的 PCIe 配置，例如双向或四向 GPU 设置。\\n\\n4. **与** **GPU** **的兼容性** ：\\n\\n虽然大多数现代 GPU 兼容大多数主板的 PCIe 槽，但是为了最佳性能，最好确认 GPU 与主板的 PCIe\\n\\n版本相匹配。\\n\\n综上所述，因为需要通过 PCIe 通道连接和使用 GPU ，因此在选择主板时考虑 PCIe 通道的版本、数量、布\\n\\n局和通道分配非常重要。\\n\\n## 3.5 硬盘选型策略\\n\\n**首先考虑接口类型** 。主流固态硬盘主要有两种接口： SATA 和 M.2 。\\n\\n**SATA** **接口** 的固态硬盘体积较大，形状类似于传统的机械硬盘，主要用于升级老式电脑，因为这些电脑\\n\\n通常不具备 M.2 接口。 SATA 接口硬盘的最高速度为 600MB/s 。\\n\\n**M.2** **接口** 的硬盘则较小，可以直接安装在主板上的专用接口。它们采用新的硬盘协议，速度上可以达到\\n\\n4GB/s 。\\n\\n**推荐选择** **M.2** **接口的硬盘。**\\n\\n**然后考虑协议** 。 M.2 接口的固态硬盘分为 SATA 协议和 NVMe 协议两种。\\n\\nM.2 接口的 **SATA** **协议硬盘** 速度较慢，实际上就是标准 SATA 硬盘的形状变化，速度仍然是最高\\n\\n600MB/s ，这类硬盘多用于旧电脑。\\n\\n**NVMe** **协议硬盘** 则速度更快，适合对速度有较高要求的应用。\\n\\n在面对大量小文件的时候，使用 NVMe 硬盘可以一分钟扫完 1000 万文件，如果使用普通硬盘，那么就\\n\\n需要一天时间。 **推荐选择** **NVME** **协议的** **M.2** **接口的硬盘。**\\n\\n**最后考虑** **PCIe** **等级。** 当前市面上最新的是 PCIe 5.0 ，但更常见的是 PCIe 3.0 和 PCIe 4.0 。 PCIe 等级越\\n\\n高，硬盘的速度潜力越大。但重要的是检查主板是否支持相应的 PCIe 等级。例如，一些主板可能最高只支持\\n\\n到 PCIe 4.0 。一般来说，选择 PCIe 4.0 的即可。\\n\\n硬盘不会限制深度学习任务的运行，但如果小看了硬盘的作用，可能会让你追、悔、莫、及。想象一\\n\\n下，如果你从硬盘中读取的数据的速度只有 100MB/s ，那么加载一个 32 张 ImageNet 图片构成的 mini\\n\\n-----\\n\\n**建议内存容量应大于** **GPU** **的显存。** 例如，对于搭载单卡 GPU 的系统，建议配置至少 16GB 内存。如果是\\n\\n四卡 GPU 系统，则建议至少配置 64GB 内存。由于数据生成器（ DataLoader ）的存在，数据不需要全部加载\\n\\n到内存中，因此内存通常不会成为性能瓶颈。\\n\\n内存不用太纠结，是 GPU 显存的一到两倍。目前， 128G 就可以， 64G 也凑合。而且内存没那么贵，可\\n\\n以配满。\\n\\n内存大小不会影响深度学习性能，但是它可能会影响你执行 GPU 代码的效率。内存容量大一点， CPU 就\\n\\n可以不通过磁盘，直接和 GPU 交换数据。所以应该配备与 GPU 显存匹配的内存容量。\\n\\n在选择的时候， **注意检查主板是否支持内存的数量及型号。** 目前常见的 ddr3 ~ 5 ，每一代内存都需要对\\n\\n应主板的插槽类， ddr 5 代内存 是无法混插在 ddr 4 代内存上的。另外需要确定主板的内存插槽数量，如果只\\n\\n有两个插槽，买了四个，那么根本插不进去。\\n\\n其次检查 cpu 主板是否支持内存频率。内存条的频率上限 受到 cpu 和主板的控制， 例如 i5 的 10400f +\\n\\nb460 主板 = 2666 ，如果你买的内存是 3600 频率的，无疑发挥不出内存本身的优势。\\n\\n## 3.7 电源选型策略\\n\\n在选择电脑电源时，需要 **检查电源的瓦数是否足以支持整机的功耗。** 并非越高瓦数越好，但瓦数过低可\\n\\n能会导致关机或黑屏等问题。在确定合适的电源瓦数之前，应综合评估整机硬件的功耗，尤其要考虑 CPU 和\\n\\n显卡这两个功耗大户。通常，将 CPU 和显卡的 TDP 功耗相加后乘以 2 可以得到一个合适的电源瓦数估计。例\\n\\n如，对于一个 65W 的 CPU 和 125W 的显卡，合适的电源瓦数应该在 400W 或 450W 左右。\\n\\n双卡最好 1000W 以上，四卡最好买 1600W 的电源\\n\\n## 3.8 机箱选型策略\\n\\n最后，选择完所有上述所有配件之后，选择机箱就相对简单，只需要确保这个机箱足够宽敞，能够容纳\\n\\n所选的所有配件。我们需要检查以下几项内容：\\n\\n1. **核对主板与机箱尺寸匹配性** ：\\n\\n确保所选主板的大小与机箱兼容。例如， ITX 主板应与 ITX 机箱相匹配。这就像选择合适大小的鞋子\\n\\n一样重要。\\n\\n2. **确认机箱支持显卡尺寸** ：\\n\\n对比显卡的长度与机箱对显卡长度的限制。建议选择机箱的显卡限长至少比显卡长度长出 30 毫米\\n\\n以上，以确保有足够空间进行安装和通风。\\n\\n3. **检查散热器与机箱的兼容性** ：\\n\\n非常重要的一点是比较散热器的尺寸与机箱的散热限高。如果散热器太高，可能无法正常安装侧\\n\\n盖。\\n\\n考虑到许多内存条配备较高的散热马甲，需要确认散热风扇是否会受到内存条的干扰。\\n\\n如果选择水冷散热系统，确保机箱的水冷位能够容纳水冷冷排的尺寸。例如， 360mm 的水冷冷排\\n\\n无法安装在仅适用于 240mm 的位置上。\\n\\n\\n-----\\n\\n常见的电源类型包括 SFX 、 ATX 和 TFX 。由于不同规格的电源在形状和大小上有所不同，必须确认\\n\\n机箱的电源仓是否适合所选电源的尺寸。\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c06f1d-1632-4e08-ae9b-0bb1509f134c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们定义一个用于从Markdown格式文本中提取目录（Table of Contents，TOC）的Python函数。它使用正则表达式来识别Markdown中的标题并将其分类。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb15ba8-fbab-468f-a2d1-b8a62fc31fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_toc(markdown):\n",
    "    # 初始化一个列表用于存储目录项，每项为标题级别和标题文本的元组\n",
    "    toc = []\n",
    "    \n",
    "    # 正则表达式模式，用于匹配Markdown标题，这包括'#'字符和后面的文本\n",
    "    # '^' 表示行的开始\n",
    "    # '(#+)' 捕获一个或多个'#'字符\n",
    "    # '\\s+' 匹配一个或多个空格\n",
    "    # '(.*)' 捕获标题文本，直到行尾\n",
    "    pattern = r'^(#+)\\s+(.*)$'\n",
    "    \n",
    "    # 将输入的Markdown文本按行分割\n",
    "    lines = markdown.split('\\n')\n",
    "    \n",
    "    # 遍历每一行\n",
    "    for line in lines:\n",
    "        # 使用正则表达式匹配当前行\n",
    "        match = re.match(pattern, line)\n",
    "        \n",
    "        # 如果当前行匹配Markdown标题格式\n",
    "        if match:\n",
    "            # 计算标题的级别，基于'#'的数量减去1（即，'#'对应0级，'##'对应1级，以此类推）\n",
    "            level = len(match.group(1)) - 1\n",
    "            \n",
    "            # 获取标题文本\n",
    "            title = match.group(2)\n",
    "            \n",
    "            # 将当前标题的级别和文本作为元组添加到目录列表中\n",
    "            toc.append((level, title))\n",
    "    \n",
    "    # 返回整理好的目录列表\n",
    "    return toc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd1018-82d1-4496-ab5b-02ab28edac87",
   "metadata": {},
   "source": [
    "&emsp;&emsp;执行调用测试函数功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da88e17a-09ff-466b-8e64-ef26d648ecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '本地部署开源大模型'),\n",
       " (1, 'Ch.1 如何选择合适的硬件配置'),\n",
       " (0, '一、大模型应用需求分析'),\n",
       " (0, '二、硬件配置的选择标准'),\n",
       " (1, '2.1 选择满足显存需求的 GPU'),\n",
       " (1, '2.2 主流显卡性能分析'),\n",
       " (1, '2.3 单卡 4090 vs A100 系列'),\n",
       " (1, '2.4 单卡 4090 vs 双卡 3090'),\n",
       " (1, '2.5 风扇卡与涡轮卡如何选择'),\n",
       " (1, '2.6 整机参考配置'),\n",
       " (1, '2.7 显卡博弈的形式分析'),\n",
       " (1, '2.8 国产 AI 超算芯片期待'),\n",
       " (0, '三、组装计算机硬件选型策略'),\n",
       " (1, '3.1 GPU 选型策略'),\n",
       " (1, '3.2 CPU 选型策略'),\n",
       " (1, '3.3 散热选型策略'),\n",
       " (1, '3.5 硬盘选型策略'),\n",
       " (1, '3.7 电源选型策略'),\n",
       " (1, '3.8 机箱选型策略')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce163d5-9c44-4b23-abe6-6b3b8edaf7bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本地部署开源大模型\n",
      " Ch.1 如何选择合适的硬件配置\n",
      "一、大模型应用需求分析\n",
      "二、硬件配置的选择标准\n",
      " 2.1 选择满足显存需求的 GPU\n",
      " 2.2 主流显卡性能分析\n",
      " 2.3 单卡 4090 vs A100 系列\n",
      " 2.4 单卡 4090 vs 双卡 3090\n",
      " 2.5 风扇卡与涡轮卡如何选择\n",
      " 2.6 整机参考配置\n",
      " 2.7 显卡博弈的形式分析\n",
      " 2.8 国产 AI 超算芯片期待\n",
      "三、组装计算机硬件选型策略\n",
      " 3.1 GPU 选型策略\n",
      " 3.2 CPU 选型策略\n",
      " 3.3 散热选型策略\n",
      " 3.5 硬盘选型策略\n",
      " 3.7 电源选型策略\n",
      " 3.8 机箱选型策略\n"
     ]
    }
   ],
   "source": [
    "toc = extract_toc(doc)\n",
    "\n",
    "for level, title in toc:\n",
    "    print(' ' * level + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13057b6d-6249-4f2a-920c-c0e6301424de",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进一步，从Markdown格式的文本中根据一级标题提取并组织各个部分的内容。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d108abef-ee7c-4301-bd93-bc8aa1b141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_by_sections(markdown):\n",
    "    # 初始化一个字典来存储每个一级标题及其对应的内容\n",
    "    sections = {}\n",
    "    # 用于追踪当前处理的一级标题\n",
    "    current_section = None\n",
    "    # 内容累积器，用于收集当前一级标题下的所有行\n",
    "    content_accumulator = []\n",
    "    \n",
    "    # 将输入的Markdown文本按行分割\n",
    "    lines = markdown.split('\\n')\n",
    "    # 正则表达式模式，用于匹配Markdown标题\n",
    "    pattern = r'^(#+)\\s+(.*)$'\n",
    "    \n",
    "    # 遍历每一行\n",
    "    for line in lines:\n",
    "        # 使用正则表达式匹配当前行\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            # 计算标题的级别，基于'#'的数量减去1\n",
    "            level = len(match.group(1)) - 1\n",
    "            # 获取标题文本\n",
    "            title = match.group(2)\n",
    "            # 如果是一级标题\n",
    "            if level == 0:\n",
    "                # 如果不是处理的第一个一级标题，存储之前标题下的所有累积内容\n",
    "                if current_section is not None:\n",
    "                    sections[current_section] = \"\\n\".join(content_accumulator).strip()\n",
    "                # 更新当前处理的一级标题并重置内容累积器\n",
    "                current_section = title\n",
    "                content_accumulator = []\n",
    "            elif current_section is not None:\n",
    "                # 如果是子标题或内容，并且已经有一级标题被处理，继续累积内容\n",
    "                content_accumulator.append(line)\n",
    "        elif current_section is not None:\n",
    "            # 如果当前行不是标题，但已经存在一级标题，继续累积内容\n",
    "            content_accumulator.append(line)\n",
    "    \n",
    "    # 处理完成所有行后，确保最后一个一级标题下的内容也被存储\n",
    "    if current_section is not None:\n",
    "        sections[current_section] = \"\\n\".join(content_accumulator).strip()\n",
    "    \n",
    "    # 返回存储了各个一级标题和对应内容的字典\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715d3e6-d941-4063-bb55-ac5d32474c48",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个函数通过Markdown中的标题级别来组织内容，仅收集一级标题下的内容。它忽略不属于任何一级标题下的内容，并将每个一级标题下的内容组织为连续的文本块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7d4f61-588b-43c5-b674-260b9545021a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: 本地部署开源大模型\n",
      "## Ch.1 如何选择合适的硬件配置\n",
      "\n",
      "为了在本地有效部署和使用开源大模型， **深入理解硬件与软件的需求至关重要。** 在硬件需求方面，关键\n",
      "\n",
      "是 **配置一台或多台高性能的个人计算机系统或租用配备了先进** **GPU** **的在线服务器** ，确保有足够的内存和存储\n",
      "\n",
      "空间来处理大数据和复杂模型。至于软件需求， **推荐使用** **Ubuntu** **操作系统** ，因其在机器学习领域的支持和\n",
      "\n",
      "兼容性优于 Windows 。编程语言建议以 Python 为主，结合 TensorFlow 或 PyTorch 等流行机器学习框架，并\n",
      "\n",
      "利用 DeepSpeed 等优化工具来提升大模型的运行效率和性能。\n",
      "\n",
      "所以在本系列课程中，我们将从硬件选择入手，逐步引导大家理解并掌握如何为大模型部署选择合适的\n",
      "\n",
      "硬件，以及如何高效地配置和运行这些模型，从零到一实现大模型的本地部署和应用。首先来看硬件方面，\n",
      "\n",
      "提前规划计算资源是必要的。目前，我们主要考虑以下两种途径：\n",
      "\n",
      "1. **配置个人计算机或服务器** ，组建一个适合大模型使用需求的计算机系统。\n",
      "\n",
      "2. **租用在线** **GPU** **服务** ，通过云计算平台获取大模型所需的计算能力。\n",
      "\n",
      "---\n",
      "\n",
      "Section: 一、大模型应用需求分析\n",
      "大模型的本地部署主要应用于三个方面： **训练（** **train** **）、高效微调（** **fine-tune** **）和推理**\n",
      "\n",
      "**（** **inference** **）** 。这些过程在算力消耗上有显著差异：\n",
      "\n",
      "**训练** ：算力最密集，通常消耗的算力是推理过程的至少三个数量级以上。\n",
      "\n",
      "**微调** ：微调是在预训练模型的基础上对其进行进一步调整以适应特定任务的过程，其算力需求低于训\n",
      "\n",
      "练，但高于推理。\n",
      "\n",
      "**推理** ：推理指的是使用训练好的模型来进行预测或分析，是算力消耗最低的阶段。\n",
      "\n",
      "总的来说，在算力消耗上， **训练** **>** **微调** **>** **推理。**\n",
      "\n",
      "从头训练一个大模型并非易事，这不仅对个人用户，对于许多企业而言也同样困难。因此，如果个人使\n",
      "\n",
      "用，关注点应该放在 **推理和微调** 的性能上。在这两种应用需求下，对 **硬件的核心要求体现在** **GPU** **的选择上，**\n",
      "\n",
      "**对** **CPU** **和内存的要求并不高。** 无论是选择租用在线算力还是配置本地计算机，如果想在本地运行大模型，我\n",
      "\n",
      "们可以拆分成两个关注点：\n",
      "\n",
      "模型：选择什么基座模型或微调模型，这可以直接下载至本地。\n",
      "\n",
      "硬件：希望在什么硬件平台上来执行，可以分为 CPU 和 GPU 两大类。\n",
      "\n",
      "⼤部分开源⼤模型⽀持在 CPU 和 Mac M 系列芯片上运⾏，但较为繁琐且占⽤内存⾄少 32G 以上，因此\n",
      "\n",
      "更推荐在 GPU 上运⾏。针对本地部署大模型， **在选择** **GPU** **时，可以遵循的简单策略是：在满足具体的大模**\n",
      "\n",
      "**型的官方配置要求下，选择性价比最高的** **GPU** **。**\n",
      "\n",
      "GPU 的性能主要由以下三个核心参数决定：\n",
      "\n",
      "1. **计算能力** ：这是最关注的指标，尤其是 32 位浮点计算能力。随着技术发展， 16 位浮点训练也日渐普\n",
      "\n",
      "及。对于仅进行预测的任务， INT 8 量化版本也足够；\n",
      "\n",
      "2. **显存大小** ：大模型的规模和训练批量大小直接影响对显存的需求。更大的模型或更大的批量处理需要更\n",
      "\n",
      "多的显存；\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "处理大量数据时的性能通常也越好；\n",
      "\n",
      "注：显存带宽相对固定，选择空间较小。\n",
      "\n",
      "---\n",
      "\n",
      "Section: 二、硬件配置的选择标准\n",
      "无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\n",
      "\n",
      "（如微调或推理）都需要相应的硬件配置方案来支持。所以 **在选择硬件配置时应根据具体的模型需求和预期**\n",
      "\n",
      "**用途来确定。**\n",
      "\n",
      "因此，我们的建议是： **根据部署的大模型配置需求，先选择出最合适的** **GPU** **，然后再根据所选** **GPU** **的**\n",
      "\n",
      "**特性，进一步搭配计算机的其他组件，如** **CPU** **、内存和存储等，以确保整体系统的协调性和高效性能。最简**\n",
      "\n",
      "**单的匹配** **GPU** **的标准是显存大小和性价比。** 因为训练不纯粹看一个显存容量大小，而是和芯片的算力高度相\n",
      "\n",
      "关的。因为实际训练的过程当中，将海量的数据切块成不同的 batch size ，然后送入显卡进行训练。显存\n",
      "\n",
      "大，意味着一次可以送进更大的数据块。但是芯片算力如果不足，单个数据块就需要更长的等待时间显存和\n",
      "\n",
      "算力，必须要相辅相成。\n",
      "\n",
      "简单来说，在深度学习的训练和推理中， GPU 的显存主要用于以下几个方面：\n",
      "\n",
      "1. **权重存储** ：模型的参数，包括权重和偏置，都需要在显存中存储。这些参数是模型进行预测或分类所必\n",
      "\n",
      "需的。\n",
      "\n",
      "2. **中间过程数据存储** ：在模型计算的前向传播和反向传播过程中，会产生并且需要暂时存储大量的中间计\n",
      "\n",
      "算结果。这些数据同样存储在显存中。\n",
      "\n",
      "3. **计算过程** ： GPU 专门为并行处理大量的矩阵和向量运算而设计，这正是深度学习中常见的计算类型。这\n",
      "\n",
      "些计算直接在显存中进行，以利用 GPU 的高速运算能力。\n",
      "\n",
      "显存的大小和速度直接影响到模型的处理速度和能处理的模型大小。显存越大，意味着可以处理更大的\n",
      "\n",
      "模型和更复杂的计算任务，但同时也需要更多的能源和可能导致更高的成本。\n",
      "\n",
      "\" 芯片 \" 通常指的是集成电路，它们被集成到各种电脑硬件组件中，如 CPU 、 GPU 和主板等。 CPU 本身就\n",
      "\n",
      "是一种芯片。它是计算机的大脑，负责执行程序和处理数据。显卡上的核心组件是图形处理器\n",
      "\n",
      "（ GPU ），它也是一种芯片。 GPU 负责处理图形和视频渲染。\n",
      "\n",
      "**所谓的** **\"** **算力** **\"** **大小，通常指的是整个计算系统的处理能力，尽管在特定上下文中，它有时特指** **GPU** **的处**\n",
      "\n",
      "**理能力。**\n",
      "\n",
      "我们以 ChatGLM-6B 模型为例，官方给出的硬件配置说明如下：\n",
      "\n",
      "模型量化是一种用于优化模型的技术，特别是在推理时。它通过减少模型中使用的数值精度来减小模\n",
      "\n",
      "型的大小，加快推理速度，并降低内存和能源消耗。模型量化常用于将模型部署到资源受限的设备\n",
      "\n",
      "上，如手机或嵌入式系统，量化程度越高，对硬件的要求就会越低。 单精度通常指的是 32 位浮点数\n",
      "\n",
      "（ FP32 ），使用 32 位表示，包括 1 位符号位、 8 位指数位和 23 位尾数位。 FP32 是标准的训练和推理格\n",
      "\n",
      "式，但由于半精度（ FP16 ）提供了相似的结果且计算速度更快，更节省内存，因此在资源受限或需要\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "少，它的计算量就会越小，对应的输出结果的精度也就会越差。\n",
      "\n",
      "## 2.1 选择满足显存需求的 GPU\n",
      "\n",
      "关于如何选择 GPU ，当前市场 NVIDIA 和 AMD 是两大主要显卡生产商。但在人工智能、大数据、深度\n",
      "\n",
      "学习领域， NVIDIA （通常被称为 N 卡）几乎独占鳌头。主要原因还是 NVIDIA 在很早期就开始专注于 AI 和深度\n",
      "\n",
      "学习市场，开发了强大的软件工具和库，例如 cuDNN 、 TensorRT ，这些都是专门为深度学习优化的，与流\n",
      "\n",
      "行的深度学习框架（如 TensorFlow 、 PyTorch 等）紧密集成，同时 NVIDIA 的 CUDA （ Compute Unified\n",
      "\n",
      "Device Architecture ）作为独特的平行计算平台和编程模型，它允许开发者利用 NVIDIA 的 GPU 进行高效的通\n",
      "\n",
      "用计算。这一点对于深度学习和大数据分析等需要大量并行处理的应用来说至关重要。\n",
      "\n",
      "**英伟达是一家什么公司？**\n",
      "\n",
      "这时候可能有小伙伴说了，英伟达是一家卖游戏显卡的，这个说法呢，对，但也不对，从财报来看，英\n",
      "\n",
      "伟达目前主要有四块业务，分别是游戏 GPU ，数据中心产品，自动驾驶芯片和其他业务。占比分别为\n",
      "\n",
      "33.6% ， 55.% ， 3.3% 和 7.4% 。游戏 GPU ，数据中心产品，自动驾驶芯片其实都可以归类为计算芯片这个门\n",
      "\n",
      "类下面，换句话说，如果从财报公布的业务情况来分析的话，英伟达确实就是一家卖计算芯片的公司。但如\n",
      "\n",
      "果真的把英伟达当做一家卖芯片的公司，那就大错特错了。英伟达的确是靠着游戏显卡起家，并且在人工智\n",
      "\n",
      "能爆发的现在靠着一手 AI 计算芯片市值突破了万亿美元，但其实它并不是一家卖芯片的公司，我对英伟达的\n",
      "\n",
      "定位是，它是一家卖 **人工智能系统** 的公司。这就有两个核心的概念，一个是英伟达的计算芯片，一个是英伟\n",
      "\n",
      "达针对自家芯片做的计算架构 CUDA ，二者缺一不可。这种定位就像智能手机时代的苹果公司，苹果依靠着 A\n",
      "\n",
      "系列芯片和 ios 操作系统收割了智能手机行业超过 80% 的利润。人工智能大发展的时代，英伟达就依靠着 GPU\n",
      "\n",
      "和计算芯片与 CUDA 计算架构，共同组成的 AI 生态系统赢得了市场青睐，根据相关机构的统计数据，在独立\n",
      "\n",
      "显卡领域，英伟达的市占率高达 85% ，在 AI 算力芯片领域，在未来可能达到 90% ，现在做深度学习，英伟达\n",
      "\n",
      "的卡就是刚需，没有其他的选择。\n",
      "\n",
      "因此，我们建议还是选择 NVIDIA 的显卡。如果对应的 ChatGLM-6B 模型的硬件配置说明，我们就可以\n",
      "\n",
      "这样选择 GPU 。理论上， **在进行少量对话时** **:**\n",
      "\n",
      "在选择显卡时，必须遵循的首要准则是：显卡的显存容量一定要高于大模型官方要求的最低显存配置。\n",
      "\n",
      "这是确保模型能够有效运行的基本要求。显存容量越大，其推理或微调的能力就会越强。当然，随着显存容\n",
      "\n",
      "量的增加，显卡的价格也相应提高。以下是目前最主流的几款大模型的显卡型号及其显存容量：\n",
      "\n",
      "|显卡型号|显存容量|\n",
      "|---|---|\n",
      "|H100|80 GB|\n",
      "|A100|80/40 GB|\n",
      "|H800|80 GB|\n",
      "\n",
      "\n",
      "\n",
      "A800 80 GB\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|显卡型号|显存容量|\n",
      "|---|---|\n",
      "|4090|24 GB|\n",
      "|3090|24 GB|\n",
      "\n",
      "\n",
      "其组合形式可以分为以下四类：\n",
      "\n",
      "1. 纯 CPU ：基于不同架构的 CPU 配置，适用于不需要或不能使用 GPU 加速的场景。 **（不推荐）**\n",
      "\n",
      "x86 ( 如 Intel 或 AMD)\n",
      "\n",
      "ARM ( 如 Apple 、 Qualcomm 、 MTK)\n",
      "\n",
      "2. 单机单卡：使用一块 GPU 进行计算，适用于大多数个人使用和一些中等计算负载的场景。 **（典型配置）**\n",
      "\n",
      "Nvidia 系列 GPU\n",
      "\n",
      "AMD 系列 GPU\n",
      "\n",
      "Apple 系列 GPU\n",
      "\n",
      "Apple Neural Engine （较少见，支持有限）\n",
      "\n",
      "3. 单机多卡：在一台机器上使用多张 GPU 卡，适用于高计算负载的场景，如模型分割处理。 **（典型配置）**\n",
      "\n",
      "4. 多机配置：使用多台计算机进行集群计算，通常超出个人使用范围，主要用于从头预训练基座模型等高\n",
      "\n",
      "负载任务。\n",
      "\n",
      "所以，在单个显卡的显存容量不足以满足需求时，也可以采用多显卡配置来增加整体的显存容量。只要\n",
      "\n",
      "总显存超过官方推荐的配置要求就可以。此外，在选择显卡时，除了考虑整体显存容量，还要根据不同显卡\n",
      "\n",
      "的性能和成本进行权衡。根据具体需求和预算，决定是选择单张高性能显卡，还是部署多张成本效益更高的\n",
      "\n",
      "低版本显卡。实现最优的性价比。比如在理论上，在进行多轮对话时或需要微调时，采用单机多卡：\n",
      "\n",
      "## 2.2 主流显卡性能分析\n",
      "\n",
      "对于 NVIDIA 的显卡（ N 卡）卡来说，我们可以按照以下几个维度来划分：\n",
      "\n",
      "按照产品线划分：\n",
      "\n",
      "|系列|特点|主要应用领域|\n",
      "|---|---|---|\n",
      "\n",
      "\n",
      "GeForce\n",
      "\n",
      "系列（ G\n",
      "\n",
      "\n",
      "消费级 GPU 产品线，注重提供高性能的图形处理能力和游戏\n",
      "\n",
      "特性 性价比高 适合游戏和深度学习推理 训练\n",
      "\n",
      "\n",
      "主要面向游戏玩家和普\n",
      "\n",
      "通用户\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|系列|特点|主要应用领域|\n",
      "|---|---|---|\n",
      "|Quadro 系列（P 系列）|专业级GPU产品线，针对商业和专业应用领域进行了优化， 适用于设计、建筑等专业图像处理。|设计、建筑、专业图像 处理等领域。|\n",
      "|Tesla系 列（T系 列）|主要用于高性能计算和机器学习任务，集成深度学习加速 器，提供快速的矩阵运算和神经网络推理。|高性能计算、机器学习 任务等领域。|\n",
      "|Tegra系 列|移动处理器产品线，用于嵌入式系统、智能手机、平板电 脑、汽车电子等领域，具备高性能的图形和计算能力，低功 耗。|嵌入式系统、智能手 机、平板电脑、汽车电 子等领域。|\n",
      "|Jetson系 列|面向边缘计算和人工智能应用的嵌入式开发平台，具备强大 的计算和推理能力，适用于智能摄像头、机器人、自动驾驶 系统等。|边缘计算、人工智能、 机器人等领域。|\n",
      "|DGX系列|面向深度学习和人工智能研究的高性能计算服务器，集成多 个GPU和专用硬件，支持大规模深度学习模型的训练和推 理。|深度学习、人工智能研 究和开发等领域。|\n",
      "\n",
      "\n",
      "按照架构划分：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|架构|年份|芯 片 代 号|特点|代表产品|\n",
      "|---|---|---|---|---|\n",
      "|Tesla|2006|GT|第一个通用并行计算架构，主要用于科学计算和 高性能计算。|Tesla C870/GeForce 8800 GTX|\n",
      "|Fermi|2010|GF|引入CUDA架构、ECC内存等，用于科学计算、图 形处理和高性能计算。|Tesla C2050/GeForce GTX 480|\n",
      "|Kepler|2012|GK|功耗效率和性能改进，引入GPU Boost技术，适 用于科学计算、深度学习和游戏。|Tesla K40/GeForce GTX 680|\n",
      "|Maxwell|2014|GM|提高功耗效率，引入新技术如多层次内存系统， 应用于游戏、深度学习和移动设备。|Tesla M40/GeForce GTX 980|\n",
      "|Pascal|2016|GP|16nm FinFET制程技术，加强深度学习和AI计算 支持，引入Tensor Cores，应用于深度学习和高 性能计算。|Tesla P100/GeForce GTX 1080|\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**架构** **年份**\n",
      "\n",
      "\n",
      "**特点** **代表产品**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|Col1|Col2|号|Col4|Col5|\n",
      "|---|---|---|---|---|\n",
      "|Volta|2017|GV|深度学习优化特性，如Tensor Cores，主要用于 深度学习、科学计算和高性能计算。|Tesla V100|\n",
      "|Turing|2018|TU|实时光线追踪技术、深度学习技术，适用于游 戏、深度学习和专业可视化。|Tesla T4/GeForce RTX 2080 Ti|\n",
      "|Ampere|2020|GA|第二代深度学习架构，更多Tensor Cores、改进 的Ray Tracing技术，应用于深度学习、科学计算 和高性能计算。|Telsa A100/GeForce RTX 3090|\n",
      "|Ada Lovelace|2022|AD|专为光线追踪和基于AI的神经图形设计，第四代 Tensor Core，第三代RT Core，提高GPU性能。|GeForce RTX 4090|\n",
      "|Hopper|2022|GH|下一代加速计算平台，支持PCIe 5.0，专用的 Transformer引擎，适用于大型语言模型和对话 AI，提供企业级AI支持。|Telsa H100|\n",
      "\n",
      "\n",
      "按照应用领域划分：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|类型|系列|描述|应用领域|代表产品|\n",
      "|---|---|---|---|---|\n",
      "|游戏娱 乐|GeForce RTX™系 列|面向大众消费级游戏和创作者用户的图形 加速卡。在性能、功耗和成本之间达到最 佳平衡点,提供极致的游戏和创作体验。|游戏、娱乐、 内容创作|如RTX 3090、RTX 4090等|\n",
      "|专业设 计和虚 拟化|NVIDIA RTX™系 列|面向专业可视化和创意工作负载的高性能 GPU,提供强大的计算性能、大容量视频 内存等。服务于工业设计、建筑设计、影 视特效渲染等专业用户。|工业设计、建 筑设计、影视 特效渲染|高端专业可视 化工作站级显 卡|\n",
      "|深度学 习、人 工智能 和高性 能计算|A系列、 H系列、 L系列、 V系列、 T系列|不同系列针对不同AI和计算需求。A系列 是AI计算加速器; H系列是AI超算; L系列 是边缘AI推理; V系列是虚拟工作站; T系 列是AI推理优化解决方案。|数据中心AI训 练和推理、边 缘AI、虚拟桌 面、AI推理加 速|A100、 A30、A40、 H100、 L40、 DeepStream 加速器等|\n",
      "\n",
      "\n",
      "像大模型领域这种生成式人工智能，需要强大的算力来生成文本、图像、视频等内容。在这个背景下，\n",
      "\n",
      "NVIDIA 先后推出 V100 、 A100 和 H100 等多款用于 AI 训练的芯片，其中 A100 是 H100 的上一代产品，于 2020\n",
      "\n",
      "年发布，使用 7 纳米工艺，支持 AI 推理和训练。而 H100 ，该显卡是 2022 年 3 月发布，可谓是核弹级性能显\n",
      "\n",
      "卡，采用了台机电 4 纳米工艺，具备 800 亿个晶体管，采用最新 Neda Hopper 架构，同时显存还支持\n",
      "\n",
      "hbm3 ，最高带宽可达 3TB 每秒。第四代 MNLINK 的带宽， 900G 每秒。是 PCIE5.0 的 7 倍，比上一代的 A100 显\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "飞跃，各项基础性能是 A100 的三倍之多， H100 的单片显卡售价 24 万元左右。\n",
      "\n",
      "但在 2022 年 10 月，漂亮国政府为了限制我国的人工智能发展，发布禁令：禁止 NVIDIA 向中国出售 A100\n",
      "\n",
      "和 H100 显卡。数据显示， 2022 年中国市场的人工智能芯片规模高达 70 亿美元，而这 70 亿的市场，被 NVIDIA\n",
      "\n",
      "垄断了 90% ，虽然 NVIDIA 的 A100 ， H100 这样的顶级芯片不能卖给中国，但 NVIDIA 作为商业公司，也是要做\n",
      "\n",
      "生意的，于是为了合规， NVIDIA 针对传输速率进行了限制，提出了中国大陆特供版的 A800 和 H800 ，即：\n",
      "\n",
      "H100 、 A100 的阉割版。\n",
      "\n",
      "也就是说，由于漂亮国的禁令，我们现在使用的 GPU 都是中国特供版的，说白了就是阉割版的，像\n",
      "\n",
      "A100 ，到国内就成了 A800 ， H100 到国内就成了 H800 ，那么 A ~ H 的差距在哪里呢？\n",
      "\n",
      "直接用 SXM 版本的 H800 进行对比，只能说这个参数对比，对于不了解的人来说，还是比较出人意料\n",
      "\n",
      "的，除了 FP64 和 NVLink 传输速率上的明显削弱，其他参数和 H100 都是一模一样的。 FP64 上的削弱主要影\n",
      "\n",
      "响是 H800 在科学计算、流体计算、有限元分析等超算领域的应用，受到影响最大的还是 NVLINK 上的削减，\n",
      "\n",
      "但因为架构上的升级，虽然比不上同为 Hoper 架构的 H100 ，但是比 AMPERRE 架构的 A100 还是要强上不少，\n",
      "\n",
      "说白了，老黄想要抓住国内市场，就算是阉割，也不会阉割的特别过分，漂亮国政府想限制国内的超算，那\n",
      "\n",
      "就算把超算性能砍掉，传输速率减小，换个名字， GPU 照卖。只要保证 H800 在大部分场景下的性能不受影\n",
      "\n",
      "响，能满足大部分人的使用需求就足够了。毕竟也不会有人跟钱过不去，所以 其实 H800 和 H 100 的性能差\n",
      "\n",
      "距并没有想象的那么夸张，就算是砍掉了 FP64 和 NVLINK 的传输速率，性能依旧够用。最关键的是，它合法\n",
      "\n",
      "呀。所以如果不是追求极致性能的话，也没必要冒着风险去选择 H100 。\n",
      "\n",
      "而就在今年的 10 月份，漂亮国又玩起了变卦， 10 月份刚升级了芯片禁令，开启了新一轮的出口管制，先\n",
      "\n",
      "预留了 30 天的窗口期，随后又要求立即生效，连 30 天都没了，也就是说，从 10 月份开始，中国将无法再获得\n",
      "\n",
      "NVIDIA5 类的 GPU 显卡 （ A800 、 A800 、 H100 、 A100 ， L40S ），其实早在 8 月份的时候， BAT 的一些大厂不\n",
      "\n",
      "知道是收到风声还是控制风险，就向 NVIDIA 提前订购了 10 万个 A800 芯片，结果这次也是彻底泡汤。其实从\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "的尖端 AI 芯片了，漂亮国就是亮牌，高端 AI 芯片，必禁无疑。所以对于目前的 A100 系列和 H100 系列，因为\n",
      "\n",
      "是漂亮国断供之前出的芯片，所以现在国内还有货，只不过市场渠道比较乱，需要甄别。\n",
      "\n",
      "同时需要说明的是， GeForce 系列显卡虽被官方定位为面向消费级市场，适合游戏爱好者。但这类显卡\n",
      "\n",
      "在深度学习领域同样展现出了出色的性能，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡 Tesla\n",
      "\n",
      "系列比起来其实差不太多，但是性价比却高很多。对于大模型来说，同样可以使用 GeForce 系列显卡。\n",
      "\n",
      "那么个人使用或者实验室针对大模型的推理和微调需求配置服务器，高端显卡目前我们可选的就是\n",
      "\n",
      "A100 、 A800 、 H100 和 4090 等，应该如何选呢？\n",
      "\n",
      "## 2.3 单卡 4090 vs A100 系列\n",
      "\n",
      "先说结论： **没有双精度需求，追求性价比，选** **4090** **。有双精度需求，选** **A100** **，没有** **A100** **选** **A800** **。如果**\n",
      "\n",
      "**是做大模型的训练，** **GeForce RTX 4090** **是不行的。但在执行推理（** **inference/serving** **）任务时，使用**\n",
      "\n",
      "**RTX 4090** **不仅可行，而且在性价比方面甚至略优于** **A100** **。同时如果做微调，也勉强是可以的，但建议多**\n",
      "\n",
      "**卡。**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|GPU 型号|Tensor FP16 算力|Tensor FP32 算力|内存 容量|内存 带宽|通信 带宽|通信 时延|售价（美元）|\n",
      "|---|---|---|---|---|---|---|---|\n",
      "|H100|989 Tflops|495 Tflops|80 GB|3.35 TB/s|900 GB/s|~1 us|30000~40000|\n",
      "|A100|312 Tflops|156 Tflops|80 GB|2 TB/s|900 GB/s|~1 us|15000|\n",
      "|4090|330 Tflops|83 Tflops|24 GB|1 TB/s|64 GB/s|~10 us|1600|\n",
      "\n",
      "\n",
      "**推理**\n",
      "\n",
      "从数据对比来看， A100 和 GeForce RTX 4090 显卡在通信能力和内存容量方面存在显著差异，但在算力\n",
      "\n",
      "上差距并不大。在 FP16 算力方面，两者几乎相当， 4090 甚至略有优势。相较于 A100 ，其较高的性价比主\n",
      "\n",
      "要源于推理过程通常涉及单一模型，在这种场景下，显卡的算力才是关键因素，而 4090 在这方面表现出\n",
      "\n",
      "色。虽然内存带宽同样重要，但在推理任务中， 4090 的内存带宽通常足以应对需求，不会成为显著的制约\n",
      "\n",
      "因素。\n",
      "\n",
      "LambdaLabs 有个很好的 GPU 单机训练性能和成本对比： [https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)\n",
      "\n",
      "， 我们来看：\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "高的。\n",
      "\n",
      "**微调**\n",
      "\n",
      "反观训练需求下， 4090 在训练的时候表现不佳的原因主要是其有限的通信能力和内存容量。比如训练\n",
      "\n",
      "LLaMA-2 70B 时需要 2400 块 A100 ，同时据说训练 ChatGPT 用了上万块 A100 ，主要还是因为训练过程除了\n",
      "\n",
      "存储模型参数外，还需要处理大量数据以及各层之间的中间数据和参数。因此，大容量内存和高通信带宽会\n",
      "\n",
      "比较关键，以便高效地处理和协调这些信息。首先就是把 n 个 T 的数据，分发到不同的 GPU 上去，然后训练，\n",
      "\n",
      "这叫数据并行。第二个并行就是会把这个模型的数据在一块 GPU 里可能放不下，所以要按照每一层，把某几\n",
      "\n",
      "层放在不同的 GPU 上面，进行串联。这就叫流水线并行。第三个就是 Tensor 张量并行。主要是我们目前训练\n",
      "\n",
      "的 Transform 模型都是多头的，每一个头都是可以按照张量来进行并行训练。所以整个 LLaMA-2 70B 他会通\n",
      "\n",
      "过张量，流水线、数据三种并行方式，从模型内层，到模型层之间，和训练数据三个维度进行计算空间的划\n",
      "\n",
      "分。\n",
      "\n",
      "2400 块 GPU 之间要进行大量的协调和通讯计算，这种复杂的并行结构需要 GPU 之间进行大量的协调和\n",
      "\n",
      "通信。 4090 的通信带宽仅为 64 GB/s ，与 A100 的 900 GB/s 相比差距过大，导致在这类大规模训练任务中\n",
      "\n",
      "通信成为瓶颈，进而影响整体性价比。因此，尽管 4090 在某些方面表现优秀，但在大模型训练中的局限性\n",
      "\n",
      "仍然明显。尽管微调过程对硬件的要求相较于训练相对较低，但这个过程仍然需要足够的内存以存储模型参\n",
      "\n",
      "数，以及有效的通信带宽来处理数据和模型层之间的交互。所以对于需要高通信带宽和大内存容量的大模型\n",
      "\n",
      "微调任务， A100 等高端 GPU 可能是更合适的选择。\n",
      "\n",
      "我们拿 GPT 3 来说， GPT 3 的参数将近 700 亿，假设每个参数使用 4 字节（通常使用 float 32 ）进行存\n",
      "\n",
      "储，训练运算储备需求是 4200 GB ，完成一次 GPT 3 训练的总算力是： 3.15 * 10 ^23 Flops, 仅考虑算力的情\n",
      "\n",
      "况下，单块 A100 需要 45741 天，几乎是 128 年（假设有效算力是 78Tflpos ），单块 4090 需要 91146 天，几\n",
      "\n",
      "乎是 250 年，（假设有效算力是 40 Tflpos ）。任何一张单卡训练一次都需要超过 100 年，对于参数量达到 10\n",
      "\n",
      "亿级别的大模型，参数本身就大，而且大模型通常需要更高的显存来来存储参数、中间计算结果和梯度等，\n",
      "\n",
      "既然要多卡运行，数据的同步效率就会显得非常重要，那么内存带宽、通信带宽、通信延时等性能将非常非\n",
      "\n",
      "常重要。， 4090 24g 的显存小，而且内存带块、通信贷款、和通信延时都相对较弱，这就有点像短板理论。\n",
      "\n",
      "最弱的那一项就决定了显卡的能力。综上， 4090 在较大的大模型没有什么发挥的余地，但随着现在的大模型\n",
      "\n",
      "越来越小，对显存和算力需求相对较小的大模型，再加上推理的算力需求更低，如 LLama 7B 13B 模型，单\n",
      "\n",
      "卡的 4090 都可以运行，至少对于学习和研究大模型的个人或实验室来说， 4090 还真是不错的选择。\n",
      "\n",
      "⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少 4 张 A100 80G 显卡；（ ChatGLM6B 模型 全量微调\n",
      "\n",
      "差不多也是需要这个配置）\n",
      "\n",
      "## 2.4 单卡 4090 vs 双卡 3090\n",
      "\n",
      "如果预算差不多的情况下，对于两张 3090 与一张 4090 的选择，推荐使用两张 3090 显卡。虽然从算力角\n",
      "\n",
      "度看，两张 3090 与一张 4090 大致持平，但两张 3090 显卡提供的总显存会更多，这对于处理大型模型尤为重\n",
      "\n",
      "要。目前，大多数深度学习计算框架都支持各种并行计算技术，如流水线并行、张量并行和 CPU 卸载。这些\n",
      "\n",
      "技术使得即使是显存较小的显卡也能处理大型模型。在这种情况下，双 3090 配置可以更有效地利用流水线并\n",
      "\n",
      "行，同时，与单 4090 配置相比， CPU 卸载的需求会降低。此外，企业环境一般都是多卡并行，使用双 3090 配\n",
      "\n",
      "置还可以练习如何写多卡代码。现有技术如串行反向传播，进一步增强了多卡系统的效率，使其成为一个经\n",
      "\n",
      "济高效的选择，尤其是在需要处理大量数据和复杂模型的情况下。因此，从目前的技术和应用需求来看，选\n",
      "\n",
      "择两张 3090 显卡无疑是更优的选择。\n",
      "\n",
      "## 2.5 风扇卡与涡轮卡如何选择\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "风扇卡与涡轮卡的供电接口位置不同，涡轮卡的供电接口位置在接口尾部，供电线比风扇卡的线\n",
      "\n",
      "更短，这样是方便安装和理线，而风扇卡供电接口一般在显卡顶部，接线后线缆会高于机箱最高\n",
      "\n",
      "面，在服务器中使用风扇卡，服务器盖板盖不上。\n",
      "\n",
      "在散热方向上面，涡轮卡散热方向是朝尾部散热，并于服务器风向是一致的，而风扇卡的散热是朝四面\n",
      "\n",
      "八方来散热的，平常的 PC 机箱放一张是可以适应的，但用作服务器上（很多时候是多卡）就不适合了，\n",
      "\n",
      "很容易因为温度过热出现宕机。\n",
      "\n",
      "风扇卡与涡轮卡的尺寸大小不同\n",
      "\n",
      "涡轮卡与风扇卡的尺寸大小也是不一样的，风扇卡的尺寸一般是 2.5-3 倍宽设计，而涡轮卡的尺寸\n",
      "\n",
      "大小是双宽设计，因为涡轮卡为了方便放入服务器里，所以涡轮卡的尺寸和高度都远远低于风扇\n",
      "\n",
      "卡，从而服务器可以支持 4 卡或者 8 卡，如果用风扇卡代替涡轮卡装在服务器里，那位置够不够还\n",
      "\n",
      "是一回事儿呢。\n",
      "\n",
      "面对市场不同\n",
      "\n",
      "风扇卡无论是公版显卡还是非公版显卡，风扇卡都是面向个人的，是应用在个人游戏行业的，\n",
      "\n",
      "4090 风扇卡的特点就是外观炫酷，而个人游戏行业就是为了风扇卡的外观和玩游戏的性能。而\n",
      "\n",
      "4090 涡轮卡是定制版，是面向 AI 科技产业，因为做工精巧、支持多卡安装、性价比高等一系列优\n",
      "\n",
      "点， 4090 涡轮卡深受广大 AI 深度学习用户的喜爱。\n",
      "\n",
      "## 2.6 整机参考配置\n",
      "\n",
      "确定 GPU 后，根据 GPU 搭配合适的计算机组件，具体来说，计算机八大件： CPU 、散热器、主板、内\n",
      "\n",
      "存、硬盘、显卡、电源、风扇、机箱。个人使用的计算机，典型的配置是单 GPU 或双 GPU ，一般不超过四个\n",
      "\n",
      "GPU ，否则常规的机箱放不下，且运行时噪声很大，而且容易跳闸。\n",
      "\n",
      "目前国内实验室主流的还是 4090 和 3090,10 万 + 的预算配置 4 张 4090 是没问题的， 20~30 万的预算则可以\n",
      "\n",
      "考虑 8 张 4090 ，或者两张 A100 80G ，如果预算不限， A100 8 卡服务器一定是最佳选择。\n",
      "\n",
      "这里给出一个本地部署 ChatGLM-6B ，同时也适用于大多数消费级实验环境的配置：\n",
      "\n",
      "GPU ： 3090 双卡，涡轮版；总共 48G 显存，能够适⽤于⼤多数试验和复现性质深度学习任务；同时双卡\n",
      "\n",
      "也便于模拟多卡运⾏的⼯业级环境；\n",
      "\n",
      "CPU ： AMD 5900X ； 12 核 24 线程，模拟普通服务器多线程设置；\n",
      "\n",
      "存储： 64G 内存 +2T SSD 数据盘；内存主要考虑机器学习任务需求；\n",
      "\n",
      "电源： 1600W 单电源；双卡 GPU 的电源在 1200W-1600W 均可；\n",
      "\n",
      "主板：华硕 ROG X570-E ；服务器级 PCE ，⽀持双卡 PCIE ；\n",
      "\n",
      "机箱： ROG 太阳神 601 ； atx 全塔式⼤机箱，便于⾼功耗下散热；\n",
      "\n",
      "A800 工作站的典型配置信\n",
      "\n",
      "|配置项|规格|\n",
      "|---|---|\n",
      "|CPU|Intel 8358P 2.6G 11.2UFI 48M 32C 240W *2|\n",
      "|内存|DDR4 3200 64G *32|\n",
      "\n",
      "\n",
      "\n",
      "数据盘 960G 2.5 SATA 6Gb R SSD *2\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|配置项|规格|\n",
      "|---|---|\n",
      "|硬盘|3.84T 2.5-E4x4R SSD *2|\n",
      "|网络|双口10G光纤网卡（含模块）*1|\n",
      "||双口25G SFP28无模块光纤网卡（MCX512A-ADAT ）*1|\n",
      "|GPU|HV HGX A800 8-GPU 8OGB *1|\n",
      "|电源|3500W电源模块*4|\n",
      "|其他|25G SFP28多模光模块 *2|\n",
      "||单端口200G HDR HCA卡(型号:MCX653105A-HDAT) *4|\n",
      "||2GB SAS 12Gb 8口 RAID卡 *1|\n",
      "||16A电源线缆国标1.8m *4|\n",
      "||托轨 *1|\n",
      "||主板预留PCIE4.0x16接口 *4|\n",
      "||支持2个M.2 *1|\n",
      "|原厂质保|3年 *1|\n",
      "\n",
      "\n",
      "总的来说：\n",
      "\n",
      "3090 ⽐ 4090 综合性价⽐更⾼，不过 4090 计算速度⼏乎是 3090 的两倍，有需求亦可考虑升级，不过\n",
      "\n",
      "4090 需要的机箱空间更⼤、电源配置也要求更⾼；\n",
      "\n",
      "双卡 GPU 升级路线： 3090—>4090—>A100 40G （ 2.5w 左右） —>A100 80G （ 6~7w 左右）；\n",
      "\n",
      "⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少 4 张 A100 80G 显卡；（ ChatGLM6B 模型 全量微调\n",
      "\n",
      "差不多也是需要这个配置）\n",
      "\n",
      "## 2.7 显卡博弈的形式分析\n",
      "\n",
      "除此之外，在 2023 年 11 月 13 日老黄又放出来两个核弹。第一个核弹就是它推出了全新的超算 GPU\n",
      "\n",
      "H200 ，直接说是当世最强，听起来很嚣张，但其实一点没有吹牛。在 AI 超算领域，对手只有看 NVIDIA 车尾\n",
      "\n",
      "灯的份。从数据层面看， H200 强在大模型推理上，以 700 亿参数的 Llama2 二代大模型为例， h200 推理速度\n",
      "\n",
      "几乎比前代的 h100 快了一倍。而且能耗还降低了一半。显存从 h100 的 80GB ，直接拉到了 141gb ，带宽也从\n",
      "\n",
      "3.35TB/s ，提升到了 4.8TB/s ，最新的 GPU H200 ，跟前一代 H100 相比，最大的提升就是它的内存，达到了\n",
      "\n",
      "惊人的 1.15TB/s ，相当于在 1s 内传输了 230 步 FHD 的高清电影。如果每一部的容量按 5G 来算的话。这个跟我\n",
      "\n",
      "们以前的计算机里的内存条就不一样了，它采用的最新技术是 HBM3e ， HBM 就是高带宽内存，这个实现是\n",
      "\n",
      "把 DRAM 内存用 3D 封装的技术叠了起来，然后把它和 GPU 芯片放在同一个 GPU 的底板上，它们之间的通信就\n",
      "\n",
      "通过这块晶元直接做了，这样内存的容量就大大提高了，同时他们和 GPU 的通信速度也有显著的增长，。达\n",
      "\n",
      "到了每秒钟 4.8 个 TB 。然后又把所有的软件做了优化，这样就使得 ChatGPT 这样 大模型的推理速度大大的提\n",
      "\n",
      "升，跟 A100 相比提高了 18 倍。第二个核弹就是 CPU 和 GPU 的合体， GH200 ， 就是把 ARM 的 CPU 和它的 GPU\n",
      "\n",
      "封装在了同一块 GPU 晶圆板上，这样 CPU 和 GPU 之间的传输速度就非常快，而且可以共享内存。内存也达到\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "有 1/2 。\n",
      "\n",
      "炸一听好像是王炸升级，刚装满 h100 的企业要哭晕在厕所了。但实际上，它可能只是 h100 的一个中期\n",
      "\n",
      "改款，单论峰值算力， H100 和 H200 其实是一模一样的。，真正提升的是显存和带宽，然而对于 AI 芯片的性\n",
      "\n",
      "能，讨论最多的是训练能力。，在 GPT 3 175B 大模型的训练中， H200 相较于 H100 ，只强了 10% ，提升并不\n",
      "\n",
      "明显，这操作，大概率是老黄有意为之，以前为了打造大模型，对 GPU 的首要要求是训练，但是到了现在，\n",
      "\n",
      "随着各种 AI 大预言模型的落地，大家开始卷的是推理速度。于是 H200 的升级，就忽略了算力升级，转向推理\n",
      "\n",
      "方面的发力，老黄的刀法依旧精准。哪怕只是小提升，依然当得起最强的称号。谁让 NVIDIA 的显卡，在 AI 芯\n",
      "\n",
      "片这块，这就是遥遥领先。\n",
      "\n",
      "但这因为是断供后的新卡，国内现在基本买不到。\n",
      "\n",
      "在 H200 没出现以前， H100 是地表最强 GPU ， NVIDIA 每一个层级的性能基本都是翻倍的， H100 ，其中\n",
      "\n",
      "微软采购了 15w 片， mate 采购了 15w 片，谷歌、亚马逊、甲骨文、腾讯都是 5w 片，那么谷歌的 gemini 发布\n",
      "\n",
      "晚，原来是因为缺少 GPU 哈。一共是 48w 片，和外界传的 一年 H100 的产量 50w 基本吻合。在 2024 年预计出\n",
      "\n",
      "货量在 200 万张。中国采购的用户 H800 要比 H100 量大，而且 H800 的售价比 H100 还要高，为什么性能不行\n",
      "\n",
      "价格还是高呢，主要原因还是有一份建议国企采购产品中的文件，里面只有 A800 和 H800 ，没有 A100 和\n",
      "\n",
      "H100 ，这就导致国企采购更愿意采购 A800 的原因，\n",
      "\n",
      "同时需要说的是，在今年的 10 月份，漂亮国再次禁用 H800 、 A800 芯片后， NVIDIA 计算再次推出中国特\n",
      "\n",
      "供 AI 芯片，初步计划是 3 款，分别是 h20 ， L20 和 L2 ，这三款基于 H100 进行阉割。使以性能符合禁令的要\n",
      "\n",
      "求。其中最强的是 H20 ，但与 H100 相比，性能被封印了 80% ，只有 H100 的 20% 左右的性能，对于 NVIDIA 而\n",
      "\n",
      "言，中国这笔 70 亿美元的大市场肯定不能丢，必须推出 AI 芯片来抢占，不过，近日有消息传出，这三款特供\n",
      "\n",
      "版芯片要跳票了，只有 L20 可能会按期推出， H20 和 L2 都可能延期。特别是 H20 这个最强的，什么时候推\n",
      "\n",
      "出，会不会推出都是未知数，最重要的原因，还是目前中国的市场已经发生了变化。没有 NVIDIA 想像的那么\n",
      "\n",
      "美好了。\n",
      "\n",
      "有一些朋友可能还不知道这回事，而知道的朋友有些也不清楚，为什么偏偏是显卡会成为国际博弈的工\n",
      "\n",
      "具，这些卡一张卖 十几 甚至几十万元，这么赚钱的生意，怎么就不让做了。在 1999 年之前的人类文明早\n",
      "\n",
      "期，世界上是没有显卡的，但已经有了电子游戏了，那时候的游戏画面是由 CPU 生成的，游戏玩家说，需要\n",
      "\n",
      "有高画质，于是就有了显卡。 1999 年， NVIDIA 声称自己发明了 GPU ，也就是 GFFORCE 256 ，所谓的 GPU ，\n",
      "\n",
      "就是图形计算单元，他是显卡最最核心的部件，再给他配上其他一系列零件，就成为了一张显卡。 GPU 跟显\n",
      "\n",
      "卡，严格来说不是一个概念，只是大家平时很少特意区分。这玩意为啥能提升游戏画质？因为渲染游戏画面\n",
      "\n",
      "这件事，难就难在计算量太大了，比如游戏中任何一个 3D 物体，它的位置、方向、大小、光源、物体表面等\n",
      "\n",
      "变化，都需要电脑来计算。\n",
      "\n",
      "渲染画面这件事，就像再做 10000 道加减乘除， CPU 的核心很强，但数量少。每个核心就像出于一个智\n",
      "\n",
      "力巅峰的高三学生，他能熟练的解出模拟卷上的最后一道大题，但让他算 1000 道，他得累死。而显卡上面\n",
      "\n",
      "密密麻麻的分布着几千个小核心，每个核心都像是一个小学生，高考题肯定是不会，但他们能同时并排启\n",
      "\n",
      "动，在 10 秒只能，把 10000 道题做完。所以渲染特别快。显卡能提升画质，就是因为他有这种强大的并行计\n",
      "\n",
      "算能力。而显卡从此脱离游戏领域，被别的领域盯上，乃至成为国际博弈的筹码。也是因为这种强大的并行\n",
      "\n",
      "计算能力，。发布了没几年，斯坦福大学实验室就盯上了显卡，它们想要显卡解决其他领域的问题，但是并\n",
      "\n",
      "不简单，显卡算力虽强，但是它们都是小学生，需要合适的软件能驾驭才行。沿用刚才的比方， GPU 就是一\n",
      "\n",
      "万个小学生在同时工作，计算能力很强，但前提是你得能把一道难解的大题，分解成无数个小学生能解决的\n",
      "\n",
      "简单问题才行。否则显卡再强又有什么用呢？转换到现实中，就是得让开发者能方便的写出代码。利用上显\n",
      "\n",
      "卡的并行计算能力才行。所以在 2006 年，带领团队出现了至今仍然在不断更新的 CUDA ， CUDA 就是更方便\n",
      "\n",
      "的让开发人员能够面向 编程 如果绝大多数 模型的训练 背后都离 开 的支持 除了\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "并行计算能力，在软件层面，配套的编程平台也成熟了，这就意味着， GPU 可以完全离开游戏领域，走向更\n",
      "\n",
      "大的世界了。\n",
      "\n",
      "第一次感受到 GPU ，就是挖矿，也就是挖比特币，挖矿其实就是用计算机 来解决数学问题，比如任何数\n",
      "\n",
      "据，都可以通过哈希算法生成一串哈希值，原始数据不管发生多小的变化，最后生成的哈希值都完全不同。\n",
      "\n",
      "我们有时候下载大文件时，也会利用哈希算法的这种特性，来做一次校验。。看看下载的文件是否完整。很\n",
      "\n",
      "多挖矿，就是要生成一个符合要求的哈希值，这就需要计算机去反复的尝试，所以挖矿就跟游戏画面一样，\n",
      "\n",
      "属于那种不难，但是计算量非常大的事情。恰好能够利用显卡的算力。于是在加密价格持续攀升的日子里，\n",
      "\n",
      "显卡涨价，缺货。一路推动 NVIDIA 的市值从 140 亿美元暴涨到了 1750 亿美元。但显卡跟加密货币之间只是一\n",
      "\n",
      "段露水情缘，随着专用矿机的出现，虚拟比特币的价格跳水，以太坊等调整了挖矿规则等原因，显卡跟虚拟\n",
      "\n",
      "货币的关系已经大不如前了。但显卡就跟上了更大、更革命的科技浪潮，就是 AI 。现在所有人都知道， AI 是\n",
      "\n",
      "可能引起新一轮科技革命的巨大产业，而几乎所有的 AI 模型训练，都需要显卡。\n",
      "\n",
      "就拿现在正火的 ChatGPT 来说，它的模型训练中涉及到大量的矩阵运算，这些矩阵运算本身不难，但是\n",
      "\n",
      "量很大很大，所以就需要 GPU 来并行处理。 AI 是可能改变世界的，而 AI 的基础是 算法、算力和数据。而提到\n",
      "\n",
      "的 A100 和 H100 ，售价高到十几万、甚至几十万的专业显卡，还供不应求。有报道说，训练 ChatGPT 需要相\n",
      "\n",
      "当于 300 块 A100 显卡的算力，光这一个项目就需要花几十个亿来购买显卡，这也是为什么 从 2022 年 10 月开\n",
      "\n",
      "始， NVIDIA 的市值在半年时间内就飙升了 34 倍。\n",
      "\n",
      "## 2.8 国产 AI 超算芯片期待\n",
      "\n",
      "这么着急赶尽杀绝，不惜拉上自己企业垫背。答案就是我国在半导体自主研发上，已经触碰到了他们的\n",
      "\n",
      "痛处。很多人总以为，我们依赖国外的 AI 芯片，是自身技术上不过关，其实真相是我们的芯片都没有真正的\n",
      "\n",
      "上过牌桌，为什么？ AI 芯片只有在实际应用中才能够发现问题，加快迭代，而我们的国产芯片，起步晚、性\n",
      "\n",
      "能差，所以国内的厂商大多不愿意使用，这也就造成了国产芯片无法获得正向反馈，发展速度只会越来越\n",
      "\n",
      "慢，这是一个矛盾的循环。在还有选择的时候，考虑到性能也好，成本也好，中国企业往往愿意选择像\n",
      "\n",
      "NVIDIA 的芯片，所以在很长一段时间，美国对中国的高端芯片的制裁，也不是彻底封死，因为国产的 AI 芯片\n",
      "\n",
      "不是它的对手，都还不够好用，但现在，局面彻底改变了。出口禁令全面升级，中国企业没有了其他的任何\n",
      "\n",
      "选择，下一步只能规模化的采购国产芯片，并且培育出本土的产业链，逐步实现真正意义上的国产替代。那\n",
      "\n",
      "可能有人说，这件事这么简单，能实现岂不是早就实现了。不太可能。其实还真不一定，放眼全球算力芯片\n",
      "\n",
      "市场，不可否认， NVIDIA 占据了九成多的份额，出于高度垄断的地位。但是，目前国产 AI 芯片的可替代方\n",
      "\n",
      "案，也不少。\n",
      "\n",
      "如果单看并行计算这个领域，有两家国产 GPU 公司值得关注：分别是摩尔线程和壁任科技。\n",
      "\n",
      "摩尔线程 2020 年 10 月成立，在 2023 年 10 月 17 日，第一款产品摩尔芯用了 7 纳米工艺，支持 CUDA 平台和\n",
      "\n",
      "算法模型，性能超过每秒 20 万亿次浮点计算，仅成立三年就上了白宫严选名单。成为老美的制裁对象之一。\n",
      "\n",
      "是现在唯一可能买到的实际产品，并且一直在更新驱动的公司。壁任科技一款产品 壁任一号， 7 纳米工艺，\n",
      "\n",
      "支持 CUDA 平台和算法模型，性能超过每秒 30 万亿次浮点计算。去年发布了一个 GPU 叫 BR100 ，性能就直逼\n",
      "\n",
      "英伟达的 H100 ，但是这个芯片并没有使用，原因还是台积电不给生产，准确的说是美国政府不让台积电给我\n",
      "\n",
      "们生产，这就是一个不公平的竞争，华为的遭遇大家就更熟悉了， 19 年以后 芯片的生产、制造全都被摁的死\n",
      "\n",
      "死的，但是麒麟芯片出来了，说明我们大概率已经突破了先进芯片制造的生产流程了，顶多就是成本高一\n",
      "\n",
      "点。\n",
      "\n",
      "这些当中除了华为之外都面临相同的问题，那就是没有针对芯片专门优化的计算架构，换句话说，这些\n",
      "\n",
      "公司不具备与 CUDA 抗衡的能力，如果能成为芯片供应商，去识别其他的计算架构，理论上也是可以的。但\n",
      "\n",
      "是对于使用者来说这样做效率就太低了。一旦训练十几天，一旦出现 BUG ，不就前功尽弃了吗。所以这事还\n",
      "\n",
      "是得看华为 华为就厉害了 在人工智能领域的布局包括但不限于昇腾系列计算芯片 CANN 异构计算架\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "可能布局出全栈式人工智能的公司，也是唯一一个全栈式国产化公司，困扰华为的最大问题还是美国的制\n",
      "\n",
      "裁，芯片没法生产，再怎么布局也是白搭，一旦能解决芯片问题，\n",
      "\n",
      "有句话说的好，天下苦英伟达久矣。有时候看似把我们逼入绝境，但其实是给我了我们绝地求生的机\n",
      "\n",
      "会，就看我们怎么把握了。尽管中国企业会迎来一段痛苦的过渡期，但从长远来看，这是一条不得不走的\n",
      "\n",
      "路。而面对当下的巨大差距，最关键的是终于有了攻坚克难的凝聚力。相信度过这段阵痛期，有的人，有的\n",
      "\n",
      "国家，只会后悔的拍大腿，眼睁睁的看着被逆袭、被超越，毕竟每一次我们都是这样过来的。\n",
      "\n",
      "---\n",
      "\n",
      "Section: 三、组装计算机硬件选型策略\n",
      "计算机八大件： CPU 、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套\n",
      "\n",
      "需要部署大模型的个人计算机，如何搭配。\n",
      "\n",
      "## 3.1 GPU 选型策略\n",
      "\n",
      "1. **选择厂商**\n",
      "\n",
      "目前独立显卡主要有 AMD 和 NVIDIA 两家厂商。其中 NVIDIA 在深度学习布局较早，对深度学习框架支持\n",
      "\n",
      "更好。 **建议选择** **NVIDIA** **的** **GPU** **。**\n",
      "\n",
      "桌面显卡性能天梯图： [https://www.mydrivers.com/zhuanti/tianti/gpu/index.html](https://www.mydrivers.com/zhuanti/tianti/gpu/index.html)\n",
      "\n",
      "2. **选择系列及品牌**\n",
      "\n",
      "**对个人用户来说，就是从** **NVIDIA** **的** **RTX** **系列中，选择出合适的** **GPU** **。** 就部署大模型的需求来说，只需\n",
      "\n",
      "考虑计算能力和显存大小就可以了。显存带宽通常相对固定，选择空间较小。目前各级别显卡的均价如下：\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|品牌|华硕|微星|技嘉|\n",
      "|---|---|---|---|\n",
      "|顶级旗舰||||\n",
      "|旗舰|ROG猛禽|超龙X|大雕|\n",
      "|次旗舰|TUF|魔龙|超级雕/小雕|\n",
      "|中端|巨齿鲨|/|雪鹰/魔鹰|\n",
      "|丐版|DUAL|万图师|猎鹰|\n",
      "\n",
      "\n",
      "华硕显卡品控优做工好，高品牌信仰足，但溢价严重，这个牌子最会宣传，卡不错但性价比不是很高。\n",
      "\n",
      "高端优先推荐华硕 ROG 猛禽，当然缺点就是：贵，另外主流用户个人更推荐 TUF ，更低端的巨齿鲨和\n",
      "\n",
      "DUAL 不太推荐\n",
      "\n",
      "微星显卡 30 系列之前更推荐魔龙， 30 系列更推荐超龙\n",
      "\n",
      "**准一线**\n",
      "\n",
      "|品牌|七彩虹|\n",
      "|---|---|\n",
      "|顶级旗舰|九段|\n",
      "|旗舰|火神/水神|\n",
      "|次旗舰|adoc|\n",
      "|中端|ultra|\n",
      "|丐版|战斧|\n",
      "\n",
      "\n",
      "\n",
      "推荐七彩虹，用料良心，保修出名的好，深受矿老板们的喜爱，支持个人送保， ultra ，三风扇，散热性\n",
      "\n",
      "能极好，噪音小，白色颜值高，不带 rgb 灯效，喜欢 rgb 灯效的可以选择 adoc 。\n",
      "\n",
      "**二线**\n",
      "\n",
      "|品牌|影驰|索泰|映众|耕升|铭瑄|\n",
      "|---|---|---|---|---|---|\n",
      "|顶级旗舰||||||\n",
      "|旗舰|名人堂|PGF/AMP|冰龙寒霜||MGG 大玩家|\n",
      "|此旗舰|GAMER/星耀|天启|超级冰龙||电竞之心|\n",
      "|中端|金属大师|/|冰龙|炫光/星极||\n",
      "|丐版|黑将/大将|X-gaming|黑金至尊|追风|turbo/终结者|\n",
      "\n",
      "\n",
      "\n",
      "二线品牌中，影驰和索泰属于二线中实力比较强的两个，可以说是强二线，索泰重点推荐 PGF （排骨\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "级别的产品，颜值高，性能强，次旗舰 GAMER 和星耀一个主打 DIY 一个主打 RGB ，都是非常有特点的产品\n",
      "\n",
      "企业级显卡\n",
      "\n",
      "参考第二部分的 GPU 推荐。\n",
      "\n",
      "服务器推断卡\n",
      "\n",
      "除了用于训练，还有一类卡是用于推断的（只预测，不训练），如：\n",
      "\n",
      "这些卡全部都是不带风扇的，但它们也需要散热，需要借助服务器强大的风扇被动散热，所以只能在专\n",
      "\n",
      "门设计的服务器上运行，性价比首选 Tesla T4 ，但是发挥全部性能需要使用 TensorRT 深度优化，目前仍然\n",
      "\n",
      "存在许多坑，比如当你的网络使用了不支持的运算符时，需要自己实现。\n",
      "\n",
      "**避免踩坑**\n",
      "\n",
      "如果选择配置单机多卡，采购显卡的时候， **一定要注意买涡轮版的** ，不要买两个或者三个风扇的版本，\n",
      "\n",
      "除非只打算买一张卡。因为涡轮风扇的热是往外机箱外部吹的，可以很好地带走热量，如果买三个风扇的版\n",
      "\n",
      "本，插多卡的时候，上面的卡会把热量吹向第二张卡，导致第二张卡温度过高，影响性能。\n",
      "\n",
      "## 3.2 CPU 选型策略\n",
      "\n",
      "CPU 在大模型使用中起到什么作用？当在 GPU 上运行大模型时， CPU 几乎不会进行任何计算。最有用的\n",
      "\n",
      "应用是数据预处理。 CPU 负责将数据从系统内存传输到 GPU 的显存中，同时也处理 GPU 完成计算后的数据。\n",
      "\n",
      "有两种不同的通用数据处理策略，具有不同的 CPU 需求。\n",
      "\n",
      "训练时处理数据：高性能的多核 CPU 能显著提高效率。建议每个 GPU 至少有 4 个线程，即为每个 GPU 分\n",
      "\n",
      "配两个 CPU 核心。每为 GPU 增加一个核心 ，应该获得大约 0-5 ％的额外性能提升。\n",
      "\n",
      "训练前处理数据：不需要非常好的 CPU 。建议每个 GPU 至少有 2 个线程，即为每个 GPU 分配一个 CPU 核\n",
      "\n",
      "心。用这种策略，更多内核也不会让性能显著提升。\n",
      "\n",
      "在这种情况下， GPU 通常承担大部分计算负担， CPU 的作用更多是管理和协调，因此需要高核心数，同\n",
      "\n",
      "时也需要快速的数据预处理，同样需要高频率，所以 **高核心** **+** **高频率，虽然不是必须，但推我们推荐还是能**\n",
      "\n",
      "**高即高，标准是：要与选择的** **GPU** **和** **CPU** **的性能水平相匹配** ，避免将一款高端显卡与低端 CPU 或一款高性能\n",
      "\n",
      "CPU 与低端显卡匹配，因为这可能导致性能瓶颈。比如：\n",
      "\n",
      "NVIDIA GeForce RTX 3090 * 2 搭配 Intel Core i7-13700K/KF 或 AMD 5900X CPU ；\n",
      "\n",
      "NVIDIA GeForce RTX 3080 搭配 Intel Core i9-11900K CPU 。\n",
      "\n",
      "但相对来说，瓶颈没有那么大，一般以一个 GPU 对应 2~4 个 CPU 核数就满足基本需求，比如单卡机器买\n",
      "\n",
      "四核 CPU ，四卡机器买十核 CPU 。在训练的时候，只要数据生成器（ DataLoader ）的产出速度比 GPU 的消\n",
      "\n",
      "耗速度快，那么 CPU 就不会成为瓶颈，也就不会拖慢训练速度。\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "去很长的一段时间了里，英特尔一直是绝对霸主的存在，代表最先进的生产力，时至今日， AMD 在产品性能\n",
      "\n",
      "层面已经完全可以和 Intel 正面硬刚了。\n",
      "\n",
      "CPU 性能天梯图： [https://www.mydrivers.com/zhuanti/tianti/cpu/index.html](https://www.mydrivers.com/zhuanti/tianti/cpu/index.html)\n",
      "\n",
      "**Intel** **系列命名规范**\n",
      "\n",
      "可以通过 CPU 名称得到一些信息，如 i7-10700K ，代表产品型号是 i7 ，后面的 10 代表是第 10 代，然后 700\n",
      "\n",
      "代表性能等级高低， K 代表这个 CPU 可以超频，当然后缀字母还有 T 、 X 、 F 等， X 后缀代表高性能处理器，而 T\n",
      "\n",
      "代表超低电压， /F 代表无 CPU 无内置显卡版本。\n",
      "\n",
      "1. 系列：由低到高 Celeron （赛扬） / Pentium （奔腾） / 酷睿系列的 i3 / i5 / i7 / i9\n",
      "\n",
      "2. 世代：第 1 组数字代表是第几代\n",
      "\n",
      "例如这三个 CPU ： I7-8700 、 I7-9700 ， i7-10700 第 1 个是第八代，第 2 个是第九代、第 3 个是第十代，还\n",
      "\n",
      "是比较容易理解的。\n",
      "\n",
      "3. 性能：第 2 组 (3 个数 ) 是表示性能等级\n",
      "\n",
      "例如： I5-12400 、 I5-12500 ，数字越大表示越好。\n",
      "\n",
      "4. 后缀： K → 可超频， F → 没有核显\n",
      "\n",
      "可超频 K 版 CPU 要搭配可超频的 Z 系列主板才行，才可以超频，搭载不能超频的主板也可以用，只是不能\n",
      "\n",
      "超频了而已。\n",
      "\n",
      "没有核显的 F 版 CPU 要搭配独立显卡才能开机点亮屏幕\n",
      "\n",
      "超频简单理解就是可以提升处理器性能，核显就是把一张入门级的显卡塞进处理器里。\n",
      "\n",
      "**i3** **是家用级别，** **i5** **是游戏级别，** **i7** **是生产力和游戏发烧友级别，** **i9** **是最顶级的。后缀带** **K** **可以超频，带** **F**\n",
      "\n",
      "**表示没有核显。**\n",
      "\n",
      "**AMD** **系列命名规范**\n",
      "\n",
      "和 Intel 类似：\n",
      "\n",
      "1. 系列：由低到高 APU / Althlon （速龙） / Ryzen （锐龙）系列 R3 / R5 / R7 / R9\n",
      "\n",
      "2. 世代：第 1 个数字代表第几代\n",
      "\n",
      "3. 例如这两个 CPU ： R7-2700X 、 R7-3700X ，第 1 个是第二代，第 2 个是第三代。\n",
      "\n",
      "4. 性能：第 2 组数字（ 3 个数字）表示性能等级\n",
      "\n",
      "数字越大性能越好，例如 R7 3800X 的性能大于 R7 3700X 。\n",
      "\n",
      "5 后缀：字母 G 表示有核显 字母 X 没有明确意思 一般性能强一点 如 R5 3600X 比 R5 3600 性能高一\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**要选** **Intel** **还是** **AMD** **，其实都可以。** 如果追求性价比， AMD 性价比高一些，如果主要玩游戏，且对价格\n",
      "\n",
      "不敏感，建议选择英特尔 Intel ，英特尔 Intel 一般主频较高，一些游戏主要依赖主频，所以高主频的 Intel 玩游\n",
      "\n",
      "戏更推荐一些。除了品牌维度的分析，目前 **主流的大模型训练硬件通常采用** **Intel + NVIDIA GPU** 。但具体\n",
      "\n",
      "情况具体分析，只能简单说：一分钱一分货，一般来说贵的好。\n",
      "\n",
      "**选购** **CPU** **误区**\n",
      "\n",
      "电子产品有一个说法是， “ 买新不买旧 ” ，一般新产品，会用更新的工艺架构，性能更强，功耗更低，比\n",
      "\n",
      "较值得购买。当然有些时候，老一代的性价比很高，也可以考虑，但如果是好几代前的老产品，就不要考虑\n",
      "\n",
      "了，有些商家会卖几年前的 i7 电脑主机，它的性能可能还不如最新的 i3 ，主要是忽悠小白的，要注意辨别。\n",
      "\n",
      "目前消费级市场，我们最常听到的 i3 i5 i7 等，是英特尔的酷睿系列产品，主要面向一般消费市场，数字\n",
      "\n",
      "大的性能更强，（注意这里只在同代产品中成立）。 AMD 与之对应的是 R3 R5 R7 。这里值得注意是，同代产\n",
      "\n",
      "品 i7 比 i5 强，如果拿老一代的 i7 和新一代的 i5 比，就未必成立，部分商家经常会营销 i5 免费升级 i7 ，其实是把\n",
      "\n",
      "最新一代的 i5 换成立了老一代的 i7 ，性能方面可能还不如没升级呢？比如 i5-8400 的性能就高于 i7-7700.\n",
      "\n",
      "## 3.3 散热选型策略\n",
      "\n",
      "CPU 不断地更新换代和性能提升，其功耗和发热量也越来越大，如果温度过高，就会出现自动关机或者\n",
      "\n",
      "是蓝屏死机等情况，所以需要单独的散热器来压制，目前 **CPU** **散热器分两种：水冷和风冷。**\n",
      "\n",
      "风冷和水冷系统都是用于 GPU 的散热解决方案。它们各有优势和不足：通常， **水冷系统在散热效率方面**\n",
      "\n",
      "**优于风冷系统。** 以 Intel 的 i9-13900KF 为例，这款 CPU 性能目前位于 CPU 性能天梯榜第二位，很多用户认为使\n",
      "\n",
      "用水冷系统是必要的。但如果这款 CPU 没有超频需求，使用高质量的风冷系统其实也能够有效地散热。只有\n",
      "\n",
      "在超频的情况下，水冷系统由于其更出色的冷却效果，才成为更佳的选择。\n",
      "\n",
      "但需要注意， **风冷和水冷与** **GPU** **无关** 。在计算机硬件中， CPU 和 GPU （显卡）的散热策略和要求各有不\n",
      "\n",
      "同。 CPU 通常需要配单独的散热器，我们可以根据需要选择并购买不同类型的散热器，例如水冷或风冷系\n",
      "\n",
      "统，并且可以根据性能要求进行升级。由于 CPU 的 **高主频和较少的核心数（通常是几个到二十几个核心）** ，\n",
      "\n",
      "高性能的 CPU 在运行时会产生较多热量，因此需要更有效的散热解决方案来维持合适的运行温度。与此相\n",
      "\n",
      "对，显卡通常采用一体化的散热设计，其散热器是显卡的重要组成部分。显卡散热器的设计已经由各厂商经\n",
      "\n",
      "过测试和优化，因此用户一般不需要担心显卡的散热问题。显卡采用的是 **多核心、低频率** 的策略，即使是高\n",
      "\n",
      "端显卡如 Nvidia 的 4090 ，其频率也相对较低，通常在 3000MHz 左右，而同代的高端 CPU （如 Intel i9 ）的频\n",
      "\n",
      "率可能是其两倍。显卡的散热器可以直接接触 GPU 核心和显存，从而高效散热。因此，在正常满载情况下，\n",
      "\n",
      "显卡的温度达到 70 多或 80 多度是正常现象，通常不会成为性能瓶颈。\n",
      "\n",
      "对于大模型部署来说，首要原则还是 **CPU** **的等级要和** **GPU** **相匹配** 。对于中低端处理器，如 Intel 的 i5 系\n",
      "\n",
      "列，以及 AMD 的 R5 和 R7 系列，一般推荐使用风冷系统。这些处理器的热设计功耗（ TDP ）通常较低，风冷系\n",
      "\n",
      "统足以提供有效的散热。而对于更高性能的处理器，如 Intel 的 i7 13700KF 及更高级别的 i7 和 i9 系列，建议至\n",
      "\n",
      "少使用 240mm 规格的水冷系统。考虑到这些处理器较高的性能和热输出，水冷系统能提供更为高效和稳定\n",
      "\n",
      "的冷却效果。因此，尽管风冷在某些情况下依然可行，但为了确保最佳性能和稳定性，对于高性能处理器，\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "在构建大模型的系统时，低端主板通常不适用。根据所选的 CPU 和 GPU 规格，应从中端或高端主板中选\n",
      "\n",
      "择出合适的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|制造 商|系列|定 位|支持CPU超 频|支持内存超 频|适用用户|\n",
      "|---|---|---|---|---|---|\n",
      "|Intel|Z系 列|高 端|是|是|追求高性能和定制化设置的用户|\n",
      "|Intel|B系 列|中 端|否|是|需要一定性能但预算有限的用户|\n",
      "|Intel|H系 列|低 端|否|否|预算有限或对性能要求不高的基本用 途|\n",
      "|AMD|X系 列|高 端|是|是|追求最高性能和高度定制化的用户|\n",
      "|AMD|B系 列|中 端|否|是|寻求性价比的用户|\n",
      "|AMD|A系 列|低 端|否|否|有限预算或基本计算需求的用户|\n",
      "\n",
      "\n",
      "选择主板时，核心因素是 **确保它与** **CPU** **的性能和超频能力相匹配** 。以 Intel 处理器为例：对于中高端 CPU\n",
      "\n",
      "（如 i5 系列及以上），更适合选择 B660 到 Z690 系列的主板。对于如 13600KF 这样的高性能 CPU ，至少应选择\n",
      "\n",
      "B660 系列的主板作为起点。需要考虑的是 CPU 是否支持超频（如带有 “K” 后缀）。可超频的 CPU 更适合搭配\n",
      "\n",
      "支持超频的高端主板，如 Z 系列\n",
      "\n",
      "其次，需要 **检查** **CPU** **和主板型号是否匹配及合理。**\n",
      "\n",
      "通常情况下，每一种型号的 CPU 都需要搭配对应型号的主板，每代 CPU 和主板都有自己的针角及接口类\n",
      "\n",
      "型， Intel cpu 不能用于 AMD 系列主板，某些主板可能会通用几代 cpu ，但有的主板只能兼容某一代，例如\n",
      "\n",
      "intel 十代 的 i510400f ，不能用于早期的四代 b85 系列主板，而是否匹配，指的是高性能 CPU 搭配低性能主\n",
      "\n",
      "板， h610 是入门主板，虽然可以点亮，但是低端主板因为供电有限，无法发挥出 cpu 的全部性能，以及无法\n",
      "\n",
      "超频，这样就失去了 cpu 本身的性能和意义。\n",
      "\n",
      "最后， **考虑** **PCIe** **通道。**\n",
      "\n",
      "PCIe 通道是一种高速接口，用于将 GPU 连接到计算机的主板。通过这些通道， GPU 可以与 CPU 以及系统\n",
      "\n",
      "内存快速交换数据。每个 PCIe 通道（或称为 “ 通道 ” ）都提供一定的数据传输带宽。更多的通道意味着更高的\n",
      "\n",
      "总体带宽。例如， PCIe 3.0 x16 接口意味着有 16 个通道，每个通道的速度是 PCIe 3.0 标准的速度。\n",
      "\n",
      "GPU 的性能部分取决于它与主板之间的通信速度，这是由 PCIe 通道的数量和版本（如 PCIe 3.0 、 4.0 或\n",
      "\n",
      "5.0 ）决定的。更高版本的 PCIe 提供更高的传输速率，从而可能提高 GPU 的性能。以下是需要考虑的几个关键\n",
      "\n",
      "点：\n",
      "\n",
      "1. **PCIe** **版本** ：\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "4.0 和 5.0 ）提供更高的数据传输速率，这对于高性能 GPU 和其他高速设备非常重要。\n",
      "\n",
      "2. **PCIe** **槽数量和布局** ：\n",
      "\n",
      "主板上的 PCIe 槽数量决定了可以安装多少个扩展卡。如果计划安装多个 GPU 或其他 PCIe 设备，需\n",
      "\n",
      "要确保主板有足够的槽位。\n",
      "\n",
      "槽位布局也很重要，尤其是在安装大型 GPU 时，需要确保它们之间有足够的空间，避免过热或物\n",
      "\n",
      "理干扰。\n",
      "\n",
      "3. **PCIe** **通道分配** ：\n",
      "\n",
      "主板上的 PCIe 通道是从 CPU 和芯片组分配的。不同的主板可能有不同的通道分配方式，这可能会\n",
      "\n",
      "影响到扩展卡的性能，特别是在多 GPU 配置中。\n",
      "\n",
      "确认主板是否支持您所需的 PCIe 配置，例如双向或四向 GPU 设置。\n",
      "\n",
      "4. **与** **GPU** **的兼容性** ：\n",
      "\n",
      "虽然大多数现代 GPU 兼容大多数主板的 PCIe 槽，但是为了最佳性能，最好确认 GPU 与主板的 PCIe\n",
      "\n",
      "版本相匹配。\n",
      "\n",
      "综上所述，因为需要通过 PCIe 通道连接和使用 GPU ，因此在选择主板时考虑 PCIe 通道的版本、数量、布\n",
      "\n",
      "局和通道分配非常重要。\n",
      "\n",
      "## 3.5 硬盘选型策略\n",
      "\n",
      "**首先考虑接口类型** 。主流固态硬盘主要有两种接口： SATA 和 M.2 。\n",
      "\n",
      "**SATA** **接口** 的固态硬盘体积较大，形状类似于传统的机械硬盘，主要用于升级老式电脑，因为这些电脑\n",
      "\n",
      "通常不具备 M.2 接口。 SATA 接口硬盘的最高速度为 600MB/s 。\n",
      "\n",
      "**M.2** **接口** 的硬盘则较小，可以直接安装在主板上的专用接口。它们采用新的硬盘协议，速度上可以达到\n",
      "\n",
      "4GB/s 。\n",
      "\n",
      "**推荐选择** **M.2** **接口的硬盘。**\n",
      "\n",
      "**然后考虑协议** 。 M.2 接口的固态硬盘分为 SATA 协议和 NVMe 协议两种。\n",
      "\n",
      "M.2 接口的 **SATA** **协议硬盘** 速度较慢，实际上就是标准 SATA 硬盘的形状变化，速度仍然是最高\n",
      "\n",
      "600MB/s ，这类硬盘多用于旧电脑。\n",
      "\n",
      "**NVMe** **协议硬盘** 则速度更快，适合对速度有较高要求的应用。\n",
      "\n",
      "在面对大量小文件的时候，使用 NVMe 硬盘可以一分钟扫完 1000 万文件，如果使用普通硬盘，那么就\n",
      "\n",
      "需要一天时间。 **推荐选择** **NVME** **协议的** **M.2** **接口的硬盘。**\n",
      "\n",
      "**最后考虑** **PCIe** **等级。** 当前市面上最新的是 PCIe 5.0 ，但更常见的是 PCIe 3.0 和 PCIe 4.0 。 PCIe 等级越\n",
      "\n",
      "高，硬盘的速度潜力越大。但重要的是检查主板是否支持相应的 PCIe 等级。例如，一些主板可能最高只支持\n",
      "\n",
      "到 PCIe 4.0 。一般来说，选择 PCIe 4.0 的即可。\n",
      "\n",
      "硬盘不会限制深度学习任务的运行，但如果小看了硬盘的作用，可能会让你追、悔、莫、及。想象一\n",
      "\n",
      "下，如果你从硬盘中读取的数据的速度只有 100MB/s ，那么加载一个 32 张 ImageNet 图片构成的 mini\n",
      "\n",
      "-----\n",
      "\n",
      "**建议内存容量应大于** **GPU** **的显存。** 例如，对于搭载单卡 GPU 的系统，建议配置至少 16GB 内存。如果是\n",
      "\n",
      "四卡 GPU 系统，则建议至少配置 64GB 内存。由于数据生成器（ DataLoader ）的存在，数据不需要全部加载\n",
      "\n",
      "到内存中，因此内存通常不会成为性能瓶颈。\n",
      "\n",
      "内存不用太纠结，是 GPU 显存的一到两倍。目前， 128G 就可以， 64G 也凑合。而且内存没那么贵，可\n",
      "\n",
      "以配满。\n",
      "\n",
      "内存大小不会影响深度学习性能，但是它可能会影响你执行 GPU 代码的效率。内存容量大一点， CPU 就\n",
      "\n",
      "可以不通过磁盘，直接和 GPU 交换数据。所以应该配备与 GPU 显存匹配的内存容量。\n",
      "\n",
      "在选择的时候， **注意检查主板是否支持内存的数量及型号。** 目前常见的 ddr3 ~ 5 ，每一代内存都需要对\n",
      "\n",
      "应主板的插槽类， ddr 5 代内存 是无法混插在 ddr 4 代内存上的。另外需要确定主板的内存插槽数量，如果只\n",
      "\n",
      "有两个插槽，买了四个，那么根本插不进去。\n",
      "\n",
      "其次检查 cpu 主板是否支持内存频率。内存条的频率上限 受到 cpu 和主板的控制， 例如 i5 的 10400f +\n",
      "\n",
      "b460 主板 = 2666 ，如果你买的内存是 3600 频率的，无疑发挥不出内存本身的优势。\n",
      "\n",
      "## 3.7 电源选型策略\n",
      "\n",
      "在选择电脑电源时，需要 **检查电源的瓦数是否足以支持整机的功耗。** 并非越高瓦数越好，但瓦数过低可\n",
      "\n",
      "能会导致关机或黑屏等问题。在确定合适的电源瓦数之前，应综合评估整机硬件的功耗，尤其要考虑 CPU 和\n",
      "\n",
      "显卡这两个功耗大户。通常，将 CPU 和显卡的 TDP 功耗相加后乘以 2 可以得到一个合适的电源瓦数估计。例\n",
      "\n",
      "如，对于一个 65W 的 CPU 和 125W 的显卡，合适的电源瓦数应该在 400W 或 450W 左右。\n",
      "\n",
      "双卡最好 1000W 以上，四卡最好买 1600W 的电源\n",
      "\n",
      "## 3.8 机箱选型策略\n",
      "\n",
      "最后，选择完所有上述所有配件之后，选择机箱就相对简单，只需要确保这个机箱足够宽敞，能够容纳\n",
      "\n",
      "所选的所有配件。我们需要检查以下几项内容：\n",
      "\n",
      "1. **核对主板与机箱尺寸匹配性** ：\n",
      "\n",
      "确保所选主板的大小与机箱兼容。例如， ITX 主板应与 ITX 机箱相匹配。这就像选择合适大小的鞋子\n",
      "\n",
      "一样重要。\n",
      "\n",
      "2. **确认机箱支持显卡尺寸** ：\n",
      "\n",
      "对比显卡的长度与机箱对显卡长度的限制。建议选择机箱的显卡限长至少比显卡长度长出 30 毫米\n",
      "\n",
      "以上，以确保有足够空间进行安装和通风。\n",
      "\n",
      "3. **检查散热器与机箱的兼容性** ：\n",
      "\n",
      "非常重要的一点是比较散热器的尺寸与机箱的散热限高。如果散热器太高，可能无法正常安装侧\n",
      "\n",
      "盖。\n",
      "\n",
      "考虑到许多内存条配备较高的散热马甲，需要确认散热风扇是否会受到内存条的干扰。\n",
      "\n",
      "如果选择水冷散热系统，确保机箱的水冷位能够容纳水冷冷排的尺寸。例如， 360mm 的水冷冷排\n",
      "\n",
      "无法安装在仅适用于 240mm 的位置上。\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "常见的电源类型包括 SFX 、 ATX 和 TFX 。由于不同规格的电源在形状和大小上有所不同，必须确认\n",
      "\n",
      "机箱的电源仓是否适合所选电源的尺寸。\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用函数并提取内容\n",
    "sections = extract_content_by_sections(doc)\n",
    "\n",
    "# 打印每个一级标题下的内容\n",
    "for section, content in sections.items():\n",
    "    print(f\"Section: {section}\")\n",
    "    print(content)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc805b4-0e9b-4fff-b1ba-2e84ead8e7a8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来构建推荐系统的项目表示，针对我们的当前的设计，可以考虑将每个大课程分解为多个模块或课件，每个模块代表课程中的一个知识点或一组知识点。这样，每个模块都可以作为推荐系统中的一个独立单元。\n",
    "\n",
    "&emsp;&emsp;数据字段设计\n",
    "- ModuleID: 每个模块的唯一标识符。\n",
    "- Course: 对应的大课程名，如“机器学习”、“自然语言处理”等。\n",
    "- ModuleName: 模块名称，如“监督学习基础”、“卷积神经网络介绍”等。\n",
    "- Description: 模块的描述，简要概述模块内容。\n",
    "- URL: 指向模块资源的链接，如视频讲解、课件下载等。\n",
    "- Tags: 模块涉及的关键词或技术点，如“线性回归”，“BERT”，“图像分类”等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a03ce963-7d1b-4697-ac25-28d3a026f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch 1 开源大模型本地部署硬件指南.pdf']\n"
     ]
    }
   ],
   "source": [
    "def list_pdf_files(directory):\n",
    "    # 列出指定目录下的所有文件和文件夹\n",
    "    items = os.listdir(directory)\n",
    "    # 过滤出以 .pdf 结尾的文件\n",
    "    pdf_files = [item for item in items if item.endswith('.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "# 使用当前目录作为参数调用函数\n",
    "current_directory = '开源大模型课件'\n",
    "pdf_files = list_pdf_files(current_directory)\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5959f363-3d5c-4616-8b4c-9516b5f4b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_pdf_files_full_path(directory):\n",
    "    # 列出指定目录下的所有文件和文件夹\n",
    "    items = os.listdir(directory)\n",
    "    # 使用列表推导式，过滤出以 .pdf 结尾的文件，并为每个文件构造完整路径\n",
    "    pdf_files = [os.path.join(directory, item) for item in items if item.endswith('.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "# 使用当前目录作为参数调用函数\n",
    "current_directory = '开源大模型课件'\n",
    "pdf_files_full_paths = list_pdf_files_full_path(current_directory)\n",
    "print(pdf_files_full_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45f31a03-d502-456d-a3dd-3e27fc407719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os\n",
    "\n",
    "data = []\n",
    "\n",
    "for module_name, content in sections.items():\n",
    "    tags = [line.strip() for line in content.split('\\n') if line.startswith('##')]\n",
    "    tags = [re.sub(r'^##\\s+', '', tag) for tag in tags]  # 清理 '##'\n",
    "\n",
    "    data.append({\n",
    "        'ModuleID': str(uuid.uuid4()),\n",
    "        'Course':pdf_files,\n",
    "        'URL': pdf_files_full_paths,\n",
    "        'ModuleName': module_name,\n",
    "        'Tags': \", \".join(tags),\n",
    "        'Content': content\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c6131b8-3a84-4b09-ae0e-98d010164be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ModuleID': '461b2584-6a57-4324-9d72-c0d2f0630f15',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '本地部署开源大模型',\n",
       "  'Tags': 'Ch.1 如何选择合适的硬件配置',\n",
       "  'Content': '## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型， **深入理解硬件与软件的需求至关重要。** 在硬件需求方面，关键\\n\\n是 **配置一台或多台高性能的个人计算机系统或租用配备了先进** **GPU** **的在线服务器** ，确保有足够的内存和存储\\n\\n空间来处理大数据和复杂模型。至于软件需求， **推荐使用** **Ubuntu** **操作系统** ，因其在机器学习领域的支持和\\n\\n兼容性优于 Windows 。编程语言建议以 Python 为主，结合 TensorFlow 或 PyTorch 等流行机器学习框架，并\\n\\n利用 DeepSpeed 等优化工具来提升大模型的运行效率和性能。\\n\\n所以在本系列课程中，我们将从硬件选择入手，逐步引导大家理解并掌握如何为大模型部署选择合适的\\n\\n硬件，以及如何高效地配置和运行这些模型，从零到一实现大模型的本地部署和应用。首先来看硬件方面，\\n\\n提前规划计算资源是必要的。目前，我们主要考虑以下两种途径：\\n\\n1. **配置个人计算机或服务器** ，组建一个适合大模型使用需求的计算机系统。\\n\\n2. **租用在线** **GPU** **服务** ，通过云计算平台获取大模型所需的计算能力。'},\n",
       " {'ModuleID': 'ce06c456-8d0d-497c-82b5-3affdb70cf2b',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '一、大模型应用需求分析',\n",
       "  'Tags': '',\n",
       "  'Content': '大模型的本地部署主要应用于三个方面： **训练（** **train** **）、高效微调（** **fine-tune** **）和推理**\\n\\n**（** **inference** **）** 。这些过程在算力消耗上有显著差异：\\n\\n**训练** ：算力最密集，通常消耗的算力是推理过程的至少三个数量级以上。\\n\\n**微调** ：微调是在预训练模型的基础上对其进行进一步调整以适应特定任务的过程，其算力需求低于训\\n\\n练，但高于推理。\\n\\n**推理** ：推理指的是使用训练好的模型来进行预测或分析，是算力消耗最低的阶段。\\n\\n总的来说，在算力消耗上， **训练** **>** **微调** **>** **推理。**\\n\\n从头训练一个大模型并非易事，这不仅对个人用户，对于许多企业而言也同样困难。因此，如果个人使\\n\\n用，关注点应该放在 **推理和微调** 的性能上。在这两种应用需求下，对 **硬件的核心要求体现在** **GPU** **的选择上，**\\n\\n**对** **CPU** **和内存的要求并不高。** 无论是选择租用在线算力还是配置本地计算机，如果想在本地运行大模型，我\\n\\n们可以拆分成两个关注点：\\n\\n模型：选择什么基座模型或微调模型，这可以直接下载至本地。\\n\\n硬件：希望在什么硬件平台上来执行，可以分为 CPU 和 GPU 两大类。\\n\\n⼤部分开源⼤模型⽀持在 CPU 和 Mac M 系列芯片上运⾏，但较为繁琐且占⽤内存⾄少 32G 以上，因此\\n\\n更推荐在 GPU 上运⾏。针对本地部署大模型， **在选择** **GPU** **时，可以遵循的简单策略是：在满足具体的大模**\\n\\n**型的官方配置要求下，选择性价比最高的** **GPU** **。**\\n\\nGPU 的性能主要由以下三个核心参数决定：\\n\\n1. **计算能力** ：这是最关注的指标，尤其是 32 位浮点计算能力。随着技术发展， 16 位浮点训练也日渐普\\n\\n及。对于仅进行预测的任务， INT 8 量化版本也足够；\\n\\n2. **显存大小** ：大模型的规模和训练批量大小直接影响对显存的需求。更大的模型或更大的批量处理需要更\\n\\n多的显存；\\n\\n\\n-----\\n\\n处理大量数据时的性能通常也越好；\\n\\n注：显存带宽相对固定，选择空间较小。'},\n",
       " {'ModuleID': 'e2b731c2-f1c7-42a5-b261-5518ad403068',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '二、硬件配置的选择标准',\n",
       "  'Tags': '2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡 4090 vs A100 系列, 2.4 单卡 4090 vs 双卡 3090, 2.5 风扇卡与涡轮卡如何选择, 2.6 整机参考配置, 2.7 显卡博弈的形式分析, 2.8 国产 AI 超算芯片期待',\n",
       "  'Content': '无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\n\\n（如微调或推理）都需要相应的硬件配置方案来支持。所以 **在选择硬件配置时应根据具体的模型需求和预期**\\n\\n**用途来确定。**\\n\\n因此，我们的建议是： **根据部署的大模型配置需求，先选择出最合适的** **GPU** **，然后再根据所选** **GPU** **的**\\n\\n**特性，进一步搭配计算机的其他组件，如** **CPU** **、内存和存储等，以确保整体系统的协调性和高效性能。最简**\\n\\n**单的匹配** **GPU** **的标准是显存大小和性价比。** 因为训练不纯粹看一个显存容量大小，而是和芯片的算力高度相\\n\\n关的。因为实际训练的过程当中，将海量的数据切块成不同的 batch size ，然后送入显卡进行训练。显存\\n\\n大，意味着一次可以送进更大的数据块。但是芯片算力如果不足，单个数据块就需要更长的等待时间显存和\\n\\n算力，必须要相辅相成。\\n\\n简单来说，在深度学习的训练和推理中， GPU 的显存主要用于以下几个方面：\\n\\n1. **权重存储** ：模型的参数，包括权重和偏置，都需要在显存中存储。这些参数是模型进行预测或分类所必\\n\\n需的。\\n\\n2. **中间过程数据存储** ：在模型计算的前向传播和反向传播过程中，会产生并且需要暂时存储大量的中间计\\n\\n算结果。这些数据同样存储在显存中。\\n\\n3. **计算过程** ： GPU 专门为并行处理大量的矩阵和向量运算而设计，这正是深度学习中常见的计算类型。这\\n\\n些计算直接在显存中进行，以利用 GPU 的高速运算能力。\\n\\n显存的大小和速度直接影响到模型的处理速度和能处理的模型大小。显存越大，意味着可以处理更大的\\n\\n模型和更复杂的计算任务，但同时也需要更多的能源和可能导致更高的成本。\\n\\n\" 芯片 \" 通常指的是集成电路，它们被集成到各种电脑硬件组件中，如 CPU 、 GPU 和主板等。 CPU 本身就\\n\\n是一种芯片。它是计算机的大脑，负责执行程序和处理数据。显卡上的核心组件是图形处理器\\n\\n（ GPU ），它也是一种芯片。 GPU 负责处理图形和视频渲染。\\n\\n**所谓的** **\"** **算力** **\"** **大小，通常指的是整个计算系统的处理能力，尽管在特定上下文中，它有时特指** **GPU** **的处**\\n\\n**理能力。**\\n\\n我们以 ChatGLM-6B 模型为例，官方给出的硬件配置说明如下：\\n\\n模型量化是一种用于优化模型的技术，特别是在推理时。它通过减少模型中使用的数值精度来减小模\\n\\n型的大小，加快推理速度，并降低内存和能源消耗。模型量化常用于将模型部署到资源受限的设备\\n\\n上，如手机或嵌入式系统，量化程度越高，对硬件的要求就会越低。 单精度通常指的是 32 位浮点数\\n\\n（ FP32 ），使用 32 位表示，包括 1 位符号位、 8 位指数位和 23 位尾数位。 FP32 是标准的训练和推理格\\n\\n式，但由于半精度（ FP16 ）提供了相似的结果且计算速度更快，更节省内存，因此在资源受限或需要\\n\\n\\n-----\\n\\n少，它的计算量就会越小，对应的输出结果的精度也就会越差。\\n\\n## 2.1 选择满足显存需求的 GPU\\n\\n关于如何选择 GPU ，当前市场 NVIDIA 和 AMD 是两大主要显卡生产商。但在人工智能、大数据、深度\\n\\n学习领域， NVIDIA （通常被称为 N 卡）几乎独占鳌头。主要原因还是 NVIDIA 在很早期就开始专注于 AI 和深度\\n\\n学习市场，开发了强大的软件工具和库，例如 cuDNN 、 TensorRT ，这些都是专门为深度学习优化的，与流\\n\\n行的深度学习框架（如 TensorFlow 、 PyTorch 等）紧密集成，同时 NVIDIA 的 CUDA （ Compute Unified\\n\\nDevice Architecture ）作为独特的平行计算平台和编程模型，它允许开发者利用 NVIDIA 的 GPU 进行高效的通\\n\\n用计算。这一点对于深度学习和大数据分析等需要大量并行处理的应用来说至关重要。\\n\\n**英伟达是一家什么公司？**\\n\\n这时候可能有小伙伴说了，英伟达是一家卖游戏显卡的，这个说法呢，对，但也不对，从财报来看，英\\n\\n伟达目前主要有四块业务，分别是游戏 GPU ，数据中心产品，自动驾驶芯片和其他业务。占比分别为\\n\\n33.6% ， 55.% ， 3.3% 和 7.4% 。游戏 GPU ，数据中心产品，自动驾驶芯片其实都可以归类为计算芯片这个门\\n\\n类下面，换句话说，如果从财报公布的业务情况来分析的话，英伟达确实就是一家卖计算芯片的公司。但如\\n\\n果真的把英伟达当做一家卖芯片的公司，那就大错特错了。英伟达的确是靠着游戏显卡起家，并且在人工智\\n\\n能爆发的现在靠着一手 AI 计算芯片市值突破了万亿美元，但其实它并不是一家卖芯片的公司，我对英伟达的\\n\\n定位是，它是一家卖 **人工智能系统** 的公司。这就有两个核心的概念，一个是英伟达的计算芯片，一个是英伟\\n\\n达针对自家芯片做的计算架构 CUDA ，二者缺一不可。这种定位就像智能手机时代的苹果公司，苹果依靠着 A\\n\\n系列芯片和 ios 操作系统收割了智能手机行业超过 80% 的利润。人工智能大发展的时代，英伟达就依靠着 GPU\\n\\n和计算芯片与 CUDA 计算架构，共同组成的 AI 生态系统赢得了市场青睐，根据相关机构的统计数据，在独立\\n\\n显卡领域，英伟达的市占率高达 85% ，在 AI 算力芯片领域，在未来可能达到 90% ，现在做深度学习，英伟达\\n\\n的卡就是刚需，没有其他的选择。\\n\\n因此，我们建议还是选择 NVIDIA 的显卡。如果对应的 ChatGLM-6B 模型的硬件配置说明，我们就可以\\n\\n这样选择 GPU 。理论上， **在进行少量对话时** **:**\\n\\n在选择显卡时，必须遵循的首要准则是：显卡的显存容量一定要高于大模型官方要求的最低显存配置。\\n\\n这是确保模型能够有效运行的基本要求。显存容量越大，其推理或微调的能力就会越强。当然，随着显存容\\n\\n量的增加，显卡的价格也相应提高。以下是目前最主流的几款大模型的显卡型号及其显存容量：\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|H100|80 GB|\\n|A100|80/40 GB|\\n|H800|80 GB|\\n\\n\\n\\nA800 80 GB\\n\\n\\n-----\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|4090|24 GB|\\n|3090|24 GB|\\n\\n\\n其组合形式可以分为以下四类：\\n\\n1. 纯 CPU ：基于不同架构的 CPU 配置，适用于不需要或不能使用 GPU 加速的场景。 **（不推荐）**\\n\\nx86 ( 如 Intel 或 AMD)\\n\\nARM ( 如 Apple 、 Qualcomm 、 MTK)\\n\\n2. 单机单卡：使用一块 GPU 进行计算，适用于大多数个人使用和一些中等计算负载的场景。 **（典型配置）**\\n\\nNvidia 系列 GPU\\n\\nAMD 系列 GPU\\n\\nApple 系列 GPU\\n\\nApple Neural Engine （较少见，支持有限）\\n\\n3. 单机多卡：在一台机器上使用多张 GPU 卡，适用于高计算负载的场景，如模型分割处理。 **（典型配置）**\\n\\n4. 多机配置：使用多台计算机进行集群计算，通常超出个人使用范围，主要用于从头预训练基座模型等高\\n\\n负载任务。\\n\\n所以，在单个显卡的显存容量不足以满足需求时，也可以采用多显卡配置来增加整体的显存容量。只要\\n\\n总显存超过官方推荐的配置要求就可以。此外，在选择显卡时，除了考虑整体显存容量，还要根据不同显卡\\n\\n的性能和成本进行权衡。根据具体需求和预算，决定是选择单张高性能显卡，还是部署多张成本效益更高的\\n\\n低版本显卡。实现最优的性价比。比如在理论上，在进行多轮对话时或需要微调时，采用单机多卡：\\n\\n## 2.2 主流显卡性能分析\\n\\n对于 NVIDIA 的显卡（ N 卡）卡来说，我们可以按照以下几个维度来划分：\\n\\n按照产品线划分：\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n\\n\\nGeForce\\n\\n系列（ G\\n\\n\\n消费级 GPU 产品线，注重提供高性能的图形处理能力和游戏\\n\\n特性 性价比高 适合游戏和深度学习推理 训练\\n\\n\\n主要面向游戏玩家和普\\n\\n通用户\\n\\n\\n-----\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n|Quadro 系列（P 系列）|专业级GPU产品线，针对商业和专业应用领域进行了优化， 适用于设计、建筑等专业图像处理。|设计、建筑、专业图像 处理等领域。|\\n|Tesla系 列（T系 列）|主要用于高性能计算和机器学习任务，集成深度学习加速 器，提供快速的矩阵运算和神经网络推理。|高性能计算、机器学习 任务等领域。|\\n|Tegra系 列|移动处理器产品线，用于嵌入式系统、智能手机、平板电 脑、汽车电子等领域，具备高性能的图形和计算能力，低功 耗。|嵌入式系统、智能手 机、平板电脑、汽车电 子等领域。|\\n|Jetson系 列|面向边缘计算和人工智能应用的嵌入式开发平台，具备强大 的计算和推理能力，适用于智能摄像头、机器人、自动驾驶 系统等。|边缘计算、人工智能、 机器人等领域。|\\n|DGX系列|面向深度学习和人工智能研究的高性能计算服务器，集成多 个GPU和专用硬件，支持大规模深度学习模型的训练和推 理。|深度学习、人工智能研 究和开发等领域。|\\n\\n\\n按照架构划分：\\n\\n\\n\\n\\n\\n\\n\\n|架构|年份|芯 片 代 号|特点|代表产品|\\n|---|---|---|---|---|\\n|Tesla|2006|GT|第一个通用并行计算架构，主要用于科学计算和 高性能计算。|Tesla C870/GeForce 8800 GTX|\\n|Fermi|2010|GF|引入CUDA架构、ECC内存等，用于科学计算、图 形处理和高性能计算。|Tesla C2050/GeForce GTX 480|\\n|Kepler|2012|GK|功耗效率和性能改进，引入GPU Boost技术，适 用于科学计算、深度学习和游戏。|Tesla K40/GeForce GTX 680|\\n|Maxwell|2014|GM|提高功耗效率，引入新技术如多层次内存系统， 应用于游戏、深度学习和移动设备。|Tesla M40/GeForce GTX 980|\\n|Pascal|2016|GP|16nm FinFET制程技术，加强深度学习和AI计算 支持，引入Tensor Cores，应用于深度学习和高 性能计算。|Tesla P100/GeForce GTX 1080|\\n\\n\\n-----\\n\\n**架构** **年份**\\n\\n\\n**特点** **代表产品**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|Col1|Col2|号|Col4|Col5|\\n|---|---|---|---|---|\\n|Volta|2017|GV|深度学习优化特性，如Tensor Cores，主要用于 深度学习、科学计算和高性能计算。|Tesla V100|\\n|Turing|2018|TU|实时光线追踪技术、深度学习技术，适用于游 戏、深度学习和专业可视化。|Tesla T4/GeForce RTX 2080 Ti|\\n|Ampere|2020|GA|第二代深度学习架构，更多Tensor Cores、改进 的Ray Tracing技术，应用于深度学习、科学计算 和高性能计算。|Telsa A100/GeForce RTX 3090|\\n|Ada Lovelace|2022|AD|专为光线追踪和基于AI的神经图形设计，第四代 Tensor Core，第三代RT Core，提高GPU性能。|GeForce RTX 4090|\\n|Hopper|2022|GH|下一代加速计算平台，支持PCIe 5.0，专用的 Transformer引擎，适用于大型语言模型和对话 AI，提供企业级AI支持。|Telsa H100|\\n\\n\\n按照应用领域划分：\\n\\n\\n\\n\\n\\n\\n\\n|类型|系列|描述|应用领域|代表产品|\\n|---|---|---|---|---|\\n|游戏娱 乐|GeForce RTX™系 列|面向大众消费级游戏和创作者用户的图形 加速卡。在性能、功耗和成本之间达到最 佳平衡点,提供极致的游戏和创作体验。|游戏、娱乐、 内容创作|如RTX 3090、RTX 4090等|\\n|专业设 计和虚 拟化|NVIDIA RTX™系 列|面向专业可视化和创意工作负载的高性能 GPU,提供强大的计算性能、大容量视频 内存等。服务于工业设计、建筑设计、影 视特效渲染等专业用户。|工业设计、建 筑设计、影视 特效渲染|高端专业可视 化工作站级显 卡|\\n|深度学 习、人 工智能 和高性 能计算|A系列、 H系列、 L系列、 V系列、 T系列|不同系列针对不同AI和计算需求。A系列 是AI计算加速器; H系列是AI超算; L系列 是边缘AI推理; V系列是虚拟工作站; T系 列是AI推理优化解决方案。|数据中心AI训 练和推理、边 缘AI、虚拟桌 面、AI推理加 速|A100、 A30、A40、 H100、 L40、 DeepStream 加速器等|\\n\\n\\n像大模型领域这种生成式人工智能，需要强大的算力来生成文本、图像、视频等内容。在这个背景下，\\n\\nNVIDIA 先后推出 V100 、 A100 和 H100 等多款用于 AI 训练的芯片，其中 A100 是 H100 的上一代产品，于 2020\\n\\n年发布，使用 7 纳米工艺，支持 AI 推理和训练。而 H100 ，该显卡是 2022 年 3 月发布，可谓是核弹级性能显\\n\\n卡，采用了台机电 4 纳米工艺，具备 800 亿个晶体管，采用最新 Neda Hopper 架构，同时显存还支持\\n\\nhbm3 ，最高带宽可达 3TB 每秒。第四代 MNLINK 的带宽， 900G 每秒。是 PCIE5.0 的 7 倍，比上一代的 A100 显\\n\\n\\n-----\\n\\n飞跃，各项基础性能是 A100 的三倍之多， H100 的单片显卡售价 24 万元左右。\\n\\n但在 2022 年 10 月，漂亮国政府为了限制我国的人工智能发展，发布禁令：禁止 NVIDIA 向中国出售 A100\\n\\n和 H100 显卡。数据显示， 2022 年中国市场的人工智能芯片规模高达 70 亿美元，而这 70 亿的市场，被 NVIDIA\\n\\n垄断了 90% ，虽然 NVIDIA 的 A100 ， H100 这样的顶级芯片不能卖给中国，但 NVIDIA 作为商业公司，也是要做\\n\\n生意的，于是为了合规， NVIDIA 针对传输速率进行了限制，提出了中国大陆特供版的 A800 和 H800 ，即：\\n\\nH100 、 A100 的阉割版。\\n\\n也就是说，由于漂亮国的禁令，我们现在使用的 GPU 都是中国特供版的，说白了就是阉割版的，像\\n\\nA100 ，到国内就成了 A800 ， H100 到国内就成了 H800 ，那么 A ~ H 的差距在哪里呢？\\n\\n直接用 SXM 版本的 H800 进行对比，只能说这个参数对比，对于不了解的人来说，还是比较出人意料\\n\\n的，除了 FP64 和 NVLink 传输速率上的明显削弱，其他参数和 H100 都是一模一样的。 FP64 上的削弱主要影\\n\\n响是 H800 在科学计算、流体计算、有限元分析等超算领域的应用，受到影响最大的还是 NVLINK 上的削减，\\n\\n但因为架构上的升级，虽然比不上同为 Hoper 架构的 H100 ，但是比 AMPERRE 架构的 A100 还是要强上不少，\\n\\n说白了，老黄想要抓住国内市场，就算是阉割，也不会阉割的特别过分，漂亮国政府想限制国内的超算，那\\n\\n就算把超算性能砍掉，传输速率减小，换个名字， GPU 照卖。只要保证 H800 在大部分场景下的性能不受影\\n\\n响，能满足大部分人的使用需求就足够了。毕竟也不会有人跟钱过不去，所以 其实 H800 和 H 100 的性能差\\n\\n距并没有想象的那么夸张，就算是砍掉了 FP64 和 NVLINK 的传输速率，性能依旧够用。最关键的是，它合法\\n\\n呀。所以如果不是追求极致性能的话，也没必要冒着风险去选择 H100 。\\n\\n而就在今年的 10 月份，漂亮国又玩起了变卦， 10 月份刚升级了芯片禁令，开启了新一轮的出口管制，先\\n\\n预留了 30 天的窗口期，随后又要求立即生效，连 30 天都没了，也就是说，从 10 月份开始，中国将无法再获得\\n\\nNVIDIA5 类的 GPU 显卡 （ A800 、 A800 、 H100 、 A100 ， L40S ），其实早在 8 月份的时候， BAT 的一些大厂不\\n\\n知道是收到风声还是控制风险，就向 NVIDIA 提前订购了 10 万个 A800 芯片，结果这次也是彻底泡汤。其实从\\n\\n\\n-----\\n\\n的尖端 AI 芯片了，漂亮国就是亮牌，高端 AI 芯片，必禁无疑。所以对于目前的 A100 系列和 H100 系列，因为\\n\\n是漂亮国断供之前出的芯片，所以现在国内还有货，只不过市场渠道比较乱，需要甄别。\\n\\n同时需要说明的是， GeForce 系列显卡虽被官方定位为面向消费级市场，适合游戏爱好者。但这类显卡\\n\\n在深度学习领域同样展现出了出色的性能，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡 Tesla\\n\\n系列比起来其实差不太多，但是性价比却高很多。对于大模型来说，同样可以使用 GeForce 系列显卡。\\n\\n那么个人使用或者实验室针对大模型的推理和微调需求配置服务器，高端显卡目前我们可选的就是\\n\\nA100 、 A800 、 H100 和 4090 等，应该如何选呢？\\n\\n## 2.3 单卡 4090 vs A100 系列\\n\\n先说结论： **没有双精度需求，追求性价比，选** **4090** **。有双精度需求，选** **A100** **，没有** **A100** **选** **A800** **。如果**\\n\\n**是做大模型的训练，** **GeForce RTX 4090** **是不行的。但在执行推理（** **inference/serving** **）任务时，使用**\\n\\n**RTX 4090** **不仅可行，而且在性价比方面甚至略优于** **A100** **。同时如果做微调，也勉强是可以的，但建议多**\\n\\n**卡。**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|GPU 型号|Tensor FP16 算力|Tensor FP32 算力|内存 容量|内存 带宽|通信 带宽|通信 时延|售价（美元）|\\n|---|---|---|---|---|---|---|---|\\n|H100|989 Tflops|495 Tflops|80 GB|3.35 TB/s|900 GB/s|~1 us|30000~40000|\\n|A100|312 Tflops|156 Tflops|80 GB|2 TB/s|900 GB/s|~1 us|15000|\\n|4090|330 Tflops|83 Tflops|24 GB|1 TB/s|64 GB/s|~10 us|1600|\\n\\n\\n**推理**\\n\\n从数据对比来看， A100 和 GeForce RTX 4090 显卡在通信能力和内存容量方面存在显著差异，但在算力\\n\\n上差距并不大。在 FP16 算力方面，两者几乎相当， 4090 甚至略有优势。相较于 A100 ，其较高的性价比主\\n\\n要源于推理过程通常涉及单一模型，在这种场景下，显卡的算力才是关键因素，而 4090 在这方面表现出\\n\\n色。虽然内存带宽同样重要，但在推理任务中， 4090 的内存带宽通常足以应对需求，不会成为显著的制约\\n\\n因素。\\n\\nLambdaLabs 有个很好的 GPU 单机训练性能和成本对比： [https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)\\n\\n， 我们来看：\\n\\n\\n-----\\n\\n高的。\\n\\n**微调**\\n\\n反观训练需求下， 4090 在训练的时候表现不佳的原因主要是其有限的通信能力和内存容量。比如训练\\n\\nLLaMA-2 70B 时需要 2400 块 A100 ，同时据说训练 ChatGPT 用了上万块 A100 ，主要还是因为训练过程除了\\n\\n存储模型参数外，还需要处理大量数据以及各层之间的中间数据和参数。因此，大容量内存和高通信带宽会\\n\\n比较关键，以便高效地处理和协调这些信息。首先就是把 n 个 T 的数据，分发到不同的 GPU 上去，然后训练，\\n\\n这叫数据并行。第二个并行就是会把这个模型的数据在一块 GPU 里可能放不下，所以要按照每一层，把某几\\n\\n层放在不同的 GPU 上面，进行串联。这就叫流水线并行。第三个就是 Tensor 张量并行。主要是我们目前训练\\n\\n的 Transform 模型都是多头的，每一个头都是可以按照张量来进行并行训练。所以整个 LLaMA-2 70B 他会通\\n\\n过张量，流水线、数据三种并行方式，从模型内层，到模型层之间，和训练数据三个维度进行计算空间的划\\n\\n分。\\n\\n2400 块 GPU 之间要进行大量的协调和通讯计算，这种复杂的并行结构需要 GPU 之间进行大量的协调和\\n\\n通信。 4090 的通信带宽仅为 64 GB/s ，与 A100 的 900 GB/s 相比差距过大，导致在这类大规模训练任务中\\n\\n通信成为瓶颈，进而影响整体性价比。因此，尽管 4090 在某些方面表现优秀，但在大模型训练中的局限性\\n\\n仍然明显。尽管微调过程对硬件的要求相较于训练相对较低，但这个过程仍然需要足够的内存以存储模型参\\n\\n数，以及有效的通信带宽来处理数据和模型层之间的交互。所以对于需要高通信带宽和大内存容量的大模型\\n\\n微调任务， A100 等高端 GPU 可能是更合适的选择。\\n\\n我们拿 GPT 3 来说， GPT 3 的参数将近 700 亿，假设每个参数使用 4 字节（通常使用 float 32 ）进行存\\n\\n储，训练运算储备需求是 4200 GB ，完成一次 GPT 3 训练的总算力是： 3.15 * 10 ^23 Flops, 仅考虑算力的情\\n\\n况下，单块 A100 需要 45741 天，几乎是 128 年（假设有效算力是 78Tflpos ），单块 4090 需要 91146 天，几\\n\\n乎是 250 年，（假设有效算力是 40 Tflpos ）。任何一张单卡训练一次都需要超过 100 年，对于参数量达到 10\\n\\n亿级别的大模型，参数本身就大，而且大模型通常需要更高的显存来来存储参数、中间计算结果和梯度等，\\n\\n既然要多卡运行，数据的同步效率就会显得非常重要，那么内存带宽、通信带宽、通信延时等性能将非常非\\n\\n常重要。， 4090 24g 的显存小，而且内存带块、通信贷款、和通信延时都相对较弱，这就有点像短板理论。\\n\\n最弱的那一项就决定了显卡的能力。综上， 4090 在较大的大模型没有什么发挥的余地，但随着现在的大模型\\n\\n越来越小，对显存和算力需求相对较小的大模型，再加上推理的算力需求更低，如 LLama 7B 13B 模型，单\\n\\n卡的 4090 都可以运行，至少对于学习和研究大模型的个人或实验室来说， 4090 还真是不错的选择。\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少 4 张 A100 80G 显卡；（ ChatGLM6B 模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.4 单卡 4090 vs 双卡 3090\\n\\n如果预算差不多的情况下，对于两张 3090 与一张 4090 的选择，推荐使用两张 3090 显卡。虽然从算力角\\n\\n度看，两张 3090 与一张 4090 大致持平，但两张 3090 显卡提供的总显存会更多，这对于处理大型模型尤为重\\n\\n要。目前，大多数深度学习计算框架都支持各种并行计算技术，如流水线并行、张量并行和 CPU 卸载。这些\\n\\n技术使得即使是显存较小的显卡也能处理大型模型。在这种情况下，双 3090 配置可以更有效地利用流水线并\\n\\n行，同时，与单 4090 配置相比， CPU 卸载的需求会降低。此外，企业环境一般都是多卡并行，使用双 3090 配\\n\\n置还可以练习如何写多卡代码。现有技术如串行反向传播，进一步增强了多卡系统的效率，使其成为一个经\\n\\n济高效的选择，尤其是在需要处理大量数据和复杂模型的情况下。因此，从目前的技术和应用需求来看，选\\n\\n择两张 3090 显卡无疑是更优的选择。\\n\\n## 2.5 风扇卡与涡轮卡如何选择\\n\\n\\n-----\\n\\n风扇卡与涡轮卡的供电接口位置不同，涡轮卡的供电接口位置在接口尾部，供电线比风扇卡的线\\n\\n更短，这样是方便安装和理线，而风扇卡供电接口一般在显卡顶部，接线后线缆会高于机箱最高\\n\\n面，在服务器中使用风扇卡，服务器盖板盖不上。\\n\\n在散热方向上面，涡轮卡散热方向是朝尾部散热，并于服务器风向是一致的，而风扇卡的散热是朝四面\\n\\n八方来散热的，平常的 PC 机箱放一张是可以适应的，但用作服务器上（很多时候是多卡）就不适合了，\\n\\n很容易因为温度过热出现宕机。\\n\\n风扇卡与涡轮卡的尺寸大小不同\\n\\n涡轮卡与风扇卡的尺寸大小也是不一样的，风扇卡的尺寸一般是 2.5-3 倍宽设计，而涡轮卡的尺寸\\n\\n大小是双宽设计，因为涡轮卡为了方便放入服务器里，所以涡轮卡的尺寸和高度都远远低于风扇\\n\\n卡，从而服务器可以支持 4 卡或者 8 卡，如果用风扇卡代替涡轮卡装在服务器里，那位置够不够还\\n\\n是一回事儿呢。\\n\\n面对市场不同\\n\\n风扇卡无论是公版显卡还是非公版显卡，风扇卡都是面向个人的，是应用在个人游戏行业的，\\n\\n4090 风扇卡的特点就是外观炫酷，而个人游戏行业就是为了风扇卡的外观和玩游戏的性能。而\\n\\n4090 涡轮卡是定制版，是面向 AI 科技产业，因为做工精巧、支持多卡安装、性价比高等一系列优\\n\\n点， 4090 涡轮卡深受广大 AI 深度学习用户的喜爱。\\n\\n## 2.6 整机参考配置\\n\\n确定 GPU 后，根据 GPU 搭配合适的计算机组件，具体来说，计算机八大件： CPU 、散热器、主板、内\\n\\n存、硬盘、显卡、电源、风扇、机箱。个人使用的计算机，典型的配置是单 GPU 或双 GPU ，一般不超过四个\\n\\nGPU ，否则常规的机箱放不下，且运行时噪声很大，而且容易跳闸。\\n\\n目前国内实验室主流的还是 4090 和 3090,10 万 + 的预算配置 4 张 4090 是没问题的， 20~30 万的预算则可以\\n\\n考虑 8 张 4090 ，或者两张 A100 80G ，如果预算不限， A100 8 卡服务器一定是最佳选择。\\n\\n这里给出一个本地部署 ChatGLM-6B ，同时也适用于大多数消费级实验环境的配置：\\n\\nGPU ： 3090 双卡，涡轮版；总共 48G 显存，能够适⽤于⼤多数试验和复现性质深度学习任务；同时双卡\\n\\n也便于模拟多卡运⾏的⼯业级环境；\\n\\nCPU ： AMD 5900X ； 12 核 24 线程，模拟普通服务器多线程设置；\\n\\n存储： 64G 内存 +2T SSD 数据盘；内存主要考虑机器学习任务需求；\\n\\n电源： 1600W 单电源；双卡 GPU 的电源在 1200W-1600W 均可；\\n\\n主板：华硕 ROG X570-E ；服务器级 PCE ，⽀持双卡 PCIE ；\\n\\n机箱： ROG 太阳神 601 ； atx 全塔式⼤机箱，便于⾼功耗下散热；\\n\\nA800 工作站的典型配置信\\n\\n|配置项|规格|\\n|---|---|\\n|CPU|Intel 8358P 2.6G 11.2UFI 48M 32C 240W *2|\\n|内存|DDR4 3200 64G *32|\\n\\n\\n\\n数据盘 960G 2.5 SATA 6Gb R SSD *2\\n\\n\\n-----\\n\\n|配置项|规格|\\n|---|---|\\n|硬盘|3.84T 2.5-E4x4R SSD *2|\\n|网络|双口10G光纤网卡（含模块）*1|\\n||双口25G SFP28无模块光纤网卡（MCX512A-ADAT ）*1|\\n|GPU|HV HGX A800 8-GPU 8OGB *1|\\n|电源|3500W电源模块*4|\\n|其他|25G SFP28多模光模块 *2|\\n||单端口200G HDR HCA卡(型号:MCX653105A-HDAT) *4|\\n||2GB SAS 12Gb 8口 RAID卡 *1|\\n||16A电源线缆国标1.8m *4|\\n||托轨 *1|\\n||主板预留PCIE4.0x16接口 *4|\\n||支持2个M.2 *1|\\n|原厂质保|3年 *1|\\n\\n\\n总的来说：\\n\\n3090 ⽐ 4090 综合性价⽐更⾼，不过 4090 计算速度⼏乎是 3090 的两倍，有需求亦可考虑升级，不过\\n\\n4090 需要的机箱空间更⼤、电源配置也要求更⾼；\\n\\n双卡 GPU 升级路线： 3090—>4090—>A100 40G （ 2.5w 左右） —>A100 80G （ 6~7w 左右）；\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少 4 张 A100 80G 显卡；（ ChatGLM6B 模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.7 显卡博弈的形式分析\\n\\n除此之外，在 2023 年 11 月 13 日老黄又放出来两个核弹。第一个核弹就是它推出了全新的超算 GPU\\n\\nH200 ，直接说是当世最强，听起来很嚣张，但其实一点没有吹牛。在 AI 超算领域，对手只有看 NVIDIA 车尾\\n\\n灯的份。从数据层面看， H200 强在大模型推理上，以 700 亿参数的 Llama2 二代大模型为例， h200 推理速度\\n\\n几乎比前代的 h100 快了一倍。而且能耗还降低了一半。显存从 h100 的 80GB ，直接拉到了 141gb ，带宽也从\\n\\n3.35TB/s ，提升到了 4.8TB/s ，最新的 GPU H200 ，跟前一代 H100 相比，最大的提升就是它的内存，达到了\\n\\n惊人的 1.15TB/s ，相当于在 1s 内传输了 230 步 FHD 的高清电影。如果每一部的容量按 5G 来算的话。这个跟我\\n\\n们以前的计算机里的内存条就不一样了，它采用的最新技术是 HBM3e ， HBM 就是高带宽内存，这个实现是\\n\\n把 DRAM 内存用 3D 封装的技术叠了起来，然后把它和 GPU 芯片放在同一个 GPU 的底板上，它们之间的通信就\\n\\n通过这块晶元直接做了，这样内存的容量就大大提高了，同时他们和 GPU 的通信速度也有显著的增长，。达\\n\\n到了每秒钟 4.8 个 TB 。然后又把所有的软件做了优化，这样就使得 ChatGPT 这样 大模型的推理速度大大的提\\n\\n升，跟 A100 相比提高了 18 倍。第二个核弹就是 CPU 和 GPU 的合体， GH200 ， 就是把 ARM 的 CPU 和它的 GPU\\n\\n封装在了同一块 GPU 晶圆板上，这样 CPU 和 GPU 之间的传输速度就非常快，而且可以共享内存。内存也达到\\n\\n\\n-----\\n\\n有 1/2 。\\n\\n炸一听好像是王炸升级，刚装满 h100 的企业要哭晕在厕所了。但实际上，它可能只是 h100 的一个中期\\n\\n改款，单论峰值算力， H100 和 H200 其实是一模一样的。，真正提升的是显存和带宽，然而对于 AI 芯片的性\\n\\n能，讨论最多的是训练能力。，在 GPT 3 175B 大模型的训练中， H200 相较于 H100 ，只强了 10% ，提升并不\\n\\n明显，这操作，大概率是老黄有意为之，以前为了打造大模型，对 GPU 的首要要求是训练，但是到了现在，\\n\\n随着各种 AI 大预言模型的落地，大家开始卷的是推理速度。于是 H200 的升级，就忽略了算力升级，转向推理\\n\\n方面的发力，老黄的刀法依旧精准。哪怕只是小提升，依然当得起最强的称号。谁让 NVIDIA 的显卡，在 AI 芯\\n\\n片这块，这就是遥遥领先。\\n\\n但这因为是断供后的新卡，国内现在基本买不到。\\n\\n在 H200 没出现以前， H100 是地表最强 GPU ， NVIDIA 每一个层级的性能基本都是翻倍的， H100 ，其中\\n\\n微软采购了 15w 片， mate 采购了 15w 片，谷歌、亚马逊、甲骨文、腾讯都是 5w 片，那么谷歌的 gemini 发布\\n\\n晚，原来是因为缺少 GPU 哈。一共是 48w 片，和外界传的 一年 H100 的产量 50w 基本吻合。在 2024 年预计出\\n\\n货量在 200 万张。中国采购的用户 H800 要比 H100 量大，而且 H800 的售价比 H100 还要高，为什么性能不行\\n\\n价格还是高呢，主要原因还是有一份建议国企采购产品中的文件，里面只有 A800 和 H800 ，没有 A100 和\\n\\nH100 ，这就导致国企采购更愿意采购 A800 的原因，\\n\\n同时需要说的是，在今年的 10 月份，漂亮国再次禁用 H800 、 A800 芯片后， NVIDIA 计算再次推出中国特\\n\\n供 AI 芯片，初步计划是 3 款，分别是 h20 ， L20 和 L2 ，这三款基于 H100 进行阉割。使以性能符合禁令的要\\n\\n求。其中最强的是 H20 ，但与 H100 相比，性能被封印了 80% ，只有 H100 的 20% 左右的性能，对于 NVIDIA 而\\n\\n言，中国这笔 70 亿美元的大市场肯定不能丢，必须推出 AI 芯片来抢占，不过，近日有消息传出，这三款特供\\n\\n版芯片要跳票了，只有 L20 可能会按期推出， H20 和 L2 都可能延期。特别是 H20 这个最强的，什么时候推\\n\\n出，会不会推出都是未知数，最重要的原因，还是目前中国的市场已经发生了变化。没有 NVIDIA 想像的那么\\n\\n美好了。\\n\\n有一些朋友可能还不知道这回事，而知道的朋友有些也不清楚，为什么偏偏是显卡会成为国际博弈的工\\n\\n具，这些卡一张卖 十几 甚至几十万元，这么赚钱的生意，怎么就不让做了。在 1999 年之前的人类文明早\\n\\n期，世界上是没有显卡的，但已经有了电子游戏了，那时候的游戏画面是由 CPU 生成的，游戏玩家说，需要\\n\\n有高画质，于是就有了显卡。 1999 年， NVIDIA 声称自己发明了 GPU ，也就是 GFFORCE 256 ，所谓的 GPU ，\\n\\n就是图形计算单元，他是显卡最最核心的部件，再给他配上其他一系列零件，就成为了一张显卡。 GPU 跟显\\n\\n卡，严格来说不是一个概念，只是大家平时很少特意区分。这玩意为啥能提升游戏画质？因为渲染游戏画面\\n\\n这件事，难就难在计算量太大了，比如游戏中任何一个 3D 物体，它的位置、方向、大小、光源、物体表面等\\n\\n变化，都需要电脑来计算。\\n\\n渲染画面这件事，就像再做 10000 道加减乘除， CPU 的核心很强，但数量少。每个核心就像出于一个智\\n\\n力巅峰的高三学生，他能熟练的解出模拟卷上的最后一道大题，但让他算 1000 道，他得累死。而显卡上面\\n\\n密密麻麻的分布着几千个小核心，每个核心都像是一个小学生，高考题肯定是不会，但他们能同时并排启\\n\\n动，在 10 秒只能，把 10000 道题做完。所以渲染特别快。显卡能提升画质，就是因为他有这种强大的并行计\\n\\n算能力。而显卡从此脱离游戏领域，被别的领域盯上，乃至成为国际博弈的筹码。也是因为这种强大的并行\\n\\n计算能力，。发布了没几年，斯坦福大学实验室就盯上了显卡，它们想要显卡解决其他领域的问题，但是并\\n\\n不简单，显卡算力虽强，但是它们都是小学生，需要合适的软件能驾驭才行。沿用刚才的比方， GPU 就是一\\n\\n万个小学生在同时工作，计算能力很强，但前提是你得能把一道难解的大题，分解成无数个小学生能解决的\\n\\n简单问题才行。否则显卡再强又有什么用呢？转换到现实中，就是得让开发者能方便的写出代码。利用上显\\n\\n卡的并行计算能力才行。所以在 2006 年，带领团队出现了至今仍然在不断更新的 CUDA ， CUDA 就是更方便\\n\\n的让开发人员能够面向 编程 如果绝大多数 模型的训练 背后都离 开 的支持 除了\\n\\n\\n-----\\n\\n并行计算能力，在软件层面，配套的编程平台也成熟了，这就意味着， GPU 可以完全离开游戏领域，走向更\\n\\n大的世界了。\\n\\n第一次感受到 GPU ，就是挖矿，也就是挖比特币，挖矿其实就是用计算机 来解决数学问题，比如任何数\\n\\n据，都可以通过哈希算法生成一串哈希值，原始数据不管发生多小的变化，最后生成的哈希值都完全不同。\\n\\n我们有时候下载大文件时，也会利用哈希算法的这种特性，来做一次校验。。看看下载的文件是否完整。很\\n\\n多挖矿，就是要生成一个符合要求的哈希值，这就需要计算机去反复的尝试，所以挖矿就跟游戏画面一样，\\n\\n属于那种不难，但是计算量非常大的事情。恰好能够利用显卡的算力。于是在加密价格持续攀升的日子里，\\n\\n显卡涨价，缺货。一路推动 NVIDIA 的市值从 140 亿美元暴涨到了 1750 亿美元。但显卡跟加密货币之间只是一\\n\\n段露水情缘，随着专用矿机的出现，虚拟比特币的价格跳水，以太坊等调整了挖矿规则等原因，显卡跟虚拟\\n\\n货币的关系已经大不如前了。但显卡就跟上了更大、更革命的科技浪潮，就是 AI 。现在所有人都知道， AI 是\\n\\n可能引起新一轮科技革命的巨大产业，而几乎所有的 AI 模型训练，都需要显卡。\\n\\n就拿现在正火的 ChatGPT 来说，它的模型训练中涉及到大量的矩阵运算，这些矩阵运算本身不难，但是\\n\\n量很大很大，所以就需要 GPU 来并行处理。 AI 是可能改变世界的，而 AI 的基础是 算法、算力和数据。而提到\\n\\n的 A100 和 H100 ，售价高到十几万、甚至几十万的专业显卡，还供不应求。有报道说，训练 ChatGPT 需要相\\n\\n当于 300 块 A100 显卡的算力，光这一个项目就需要花几十个亿来购买显卡，这也是为什么 从 2022 年 10 月开\\n\\n始， NVIDIA 的市值在半年时间内就飙升了 34 倍。\\n\\n## 2.8 国产 AI 超算芯片期待\\n\\n这么着急赶尽杀绝，不惜拉上自己企业垫背。答案就是我国在半导体自主研发上，已经触碰到了他们的\\n\\n痛处。很多人总以为，我们依赖国外的 AI 芯片，是自身技术上不过关，其实真相是我们的芯片都没有真正的\\n\\n上过牌桌，为什么？ AI 芯片只有在实际应用中才能够发现问题，加快迭代，而我们的国产芯片，起步晚、性\\n\\n能差，所以国内的厂商大多不愿意使用，这也就造成了国产芯片无法获得正向反馈，发展速度只会越来越\\n\\n慢，这是一个矛盾的循环。在还有选择的时候，考虑到性能也好，成本也好，中国企业往往愿意选择像\\n\\nNVIDIA 的芯片，所以在很长一段时间，美国对中国的高端芯片的制裁，也不是彻底封死，因为国产的 AI 芯片\\n\\n不是它的对手，都还不够好用，但现在，局面彻底改变了。出口禁令全面升级，中国企业没有了其他的任何\\n\\n选择，下一步只能规模化的采购国产芯片，并且培育出本土的产业链，逐步实现真正意义上的国产替代。那\\n\\n可能有人说，这件事这么简单，能实现岂不是早就实现了。不太可能。其实还真不一定，放眼全球算力芯片\\n\\n市场，不可否认， NVIDIA 占据了九成多的份额，出于高度垄断的地位。但是，目前国产 AI 芯片的可替代方\\n\\n案，也不少。\\n\\n如果单看并行计算这个领域，有两家国产 GPU 公司值得关注：分别是摩尔线程和壁任科技。\\n\\n摩尔线程 2020 年 10 月成立，在 2023 年 10 月 17 日，第一款产品摩尔芯用了 7 纳米工艺，支持 CUDA 平台和\\n\\n算法模型，性能超过每秒 20 万亿次浮点计算，仅成立三年就上了白宫严选名单。成为老美的制裁对象之一。\\n\\n是现在唯一可能买到的实际产品，并且一直在更新驱动的公司。壁任科技一款产品 壁任一号， 7 纳米工艺，\\n\\n支持 CUDA 平台和算法模型，性能超过每秒 30 万亿次浮点计算。去年发布了一个 GPU 叫 BR100 ，性能就直逼\\n\\n英伟达的 H100 ，但是这个芯片并没有使用，原因还是台积电不给生产，准确的说是美国政府不让台积电给我\\n\\n们生产，这就是一个不公平的竞争，华为的遭遇大家就更熟悉了， 19 年以后 芯片的生产、制造全都被摁的死\\n\\n死的，但是麒麟芯片出来了，说明我们大概率已经突破了先进芯片制造的生产流程了，顶多就是成本高一\\n\\n点。\\n\\n这些当中除了华为之外都面临相同的问题，那就是没有针对芯片专门优化的计算架构，换句话说，这些\\n\\n公司不具备与 CUDA 抗衡的能力，如果能成为芯片供应商，去识别其他的计算架构，理论上也是可以的。但\\n\\n是对于使用者来说这样做效率就太低了。一旦训练十几天，一旦出现 BUG ，不就前功尽弃了吗。所以这事还\\n\\n是得看华为 华为就厉害了 在人工智能领域的布局包括但不限于昇腾系列计算芯片 CANN 异构计算架\\n\\n\\n-----\\n\\n可能布局出全栈式人工智能的公司，也是唯一一个全栈式国产化公司，困扰华为的最大问题还是美国的制\\n\\n裁，芯片没法生产，再怎么布局也是白搭，一旦能解决芯片问题，\\n\\n有句话说的好，天下苦英伟达久矣。有时候看似把我们逼入绝境，但其实是给我了我们绝地求生的机\\n\\n会，就看我们怎么把握了。尽管中国企业会迎来一段痛苦的过渡期，但从长远来看，这是一条不得不走的\\n\\n路。而面对当下的巨大差距，最关键的是终于有了攻坚克难的凝聚力。相信度过这段阵痛期，有的人，有的\\n\\n国家，只会后悔的拍大腿，眼睁睁的看着被逆袭、被超越，毕竟每一次我们都是这样过来的。'},\n",
       " {'ModuleID': '7ea70fff-2919-412b-90a4-9382acc186f0',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '三、组装计算机硬件选型策略',\n",
       "  'Tags': '3.1 GPU 选型策略, 3.2 CPU 选型策略, 3.3 散热选型策略, 3.5 硬盘选型策略, 3.7 电源选型策略, 3.8 机箱选型策略',\n",
       "  'Content': '计算机八大件： CPU 、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套\\n\\n需要部署大模型的个人计算机，如何搭配。\\n\\n## 3.1 GPU 选型策略\\n\\n1. **选择厂商**\\n\\n目前独立显卡主要有 AMD 和 NVIDIA 两家厂商。其中 NVIDIA 在深度学习布局较早，对深度学习框架支持\\n\\n更好。 **建议选择** **NVIDIA** **的** **GPU** **。**\\n\\n桌面显卡性能天梯图： [https://www.mydrivers.com/zhuanti/tianti/gpu/index.html](https://www.mydrivers.com/zhuanti/tianti/gpu/index.html)\\n\\n2. **选择系列及品牌**\\n\\n**对个人用户来说，就是从** **NVIDIA** **的** **RTX** **系列中，选择出合适的** **GPU** **。** 就部署大模型的需求来说，只需\\n\\n考虑计算能力和显存大小就可以了。显存带宽通常相对固定，选择空间较小。目前各级别显卡的均价如下：\\n\\n\\n-----\\n\\n|品牌|华硕|微星|技嘉|\\n|---|---|---|---|\\n|顶级旗舰||||\\n|旗舰|ROG猛禽|超龙X|大雕|\\n|次旗舰|TUF|魔龙|超级雕/小雕|\\n|中端|巨齿鲨|/|雪鹰/魔鹰|\\n|丐版|DUAL|万图师|猎鹰|\\n\\n\\n华硕显卡品控优做工好，高品牌信仰足，但溢价严重，这个牌子最会宣传，卡不错但性价比不是很高。\\n\\n高端优先推荐华硕 ROG 猛禽，当然缺点就是：贵，另外主流用户个人更推荐 TUF ，更低端的巨齿鲨和\\n\\nDUAL 不太推荐\\n\\n微星显卡 30 系列之前更推荐魔龙， 30 系列更推荐超龙\\n\\n**准一线**\\n\\n|品牌|七彩虹|\\n|---|---|\\n|顶级旗舰|九段|\\n|旗舰|火神/水神|\\n|次旗舰|adoc|\\n|中端|ultra|\\n|丐版|战斧|\\n\\n\\n\\n推荐七彩虹，用料良心，保修出名的好，深受矿老板们的喜爱，支持个人送保， ultra ，三风扇，散热性\\n\\n能极好，噪音小，白色颜值高，不带 rgb 灯效，喜欢 rgb 灯效的可以选择 adoc 。\\n\\n**二线**\\n\\n|品牌|影驰|索泰|映众|耕升|铭瑄|\\n|---|---|---|---|---|---|\\n|顶级旗舰||||||\\n|旗舰|名人堂|PGF/AMP|冰龙寒霜||MGG 大玩家|\\n|此旗舰|GAMER/星耀|天启|超级冰龙||电竞之心|\\n|中端|金属大师|/|冰龙|炫光/星极||\\n|丐版|黑将/大将|X-gaming|黑金至尊|追风|turbo/终结者|\\n\\n\\n\\n二线品牌中，影驰和索泰属于二线中实力比较强的两个，可以说是强二线，索泰重点推荐 PGF （排骨\\n\\n\\n-----\\n\\n级别的产品，颜值高，性能强，次旗舰 GAMER 和星耀一个主打 DIY 一个主打 RGB ，都是非常有特点的产品\\n\\n企业级显卡\\n\\n参考第二部分的 GPU 推荐。\\n\\n服务器推断卡\\n\\n除了用于训练，还有一类卡是用于推断的（只预测，不训练），如：\\n\\n这些卡全部都是不带风扇的，但它们也需要散热，需要借助服务器强大的风扇被动散热，所以只能在专\\n\\n门设计的服务器上运行，性价比首选 Tesla T4 ，但是发挥全部性能需要使用 TensorRT 深度优化，目前仍然\\n\\n存在许多坑，比如当你的网络使用了不支持的运算符时，需要自己实现。\\n\\n**避免踩坑**\\n\\n如果选择配置单机多卡，采购显卡的时候， **一定要注意买涡轮版的** ，不要买两个或者三个风扇的版本，\\n\\n除非只打算买一张卡。因为涡轮风扇的热是往外机箱外部吹的，可以很好地带走热量，如果买三个风扇的版\\n\\n本，插多卡的时候，上面的卡会把热量吹向第二张卡，导致第二张卡温度过高，影响性能。\\n\\n## 3.2 CPU 选型策略\\n\\nCPU 在大模型使用中起到什么作用？当在 GPU 上运行大模型时， CPU 几乎不会进行任何计算。最有用的\\n\\n应用是数据预处理。 CPU 负责将数据从系统内存传输到 GPU 的显存中，同时也处理 GPU 完成计算后的数据。\\n\\n有两种不同的通用数据处理策略，具有不同的 CPU 需求。\\n\\n训练时处理数据：高性能的多核 CPU 能显著提高效率。建议每个 GPU 至少有 4 个线程，即为每个 GPU 分\\n\\n配两个 CPU 核心。每为 GPU 增加一个核心 ，应该获得大约 0-5 ％的额外性能提升。\\n\\n训练前处理数据：不需要非常好的 CPU 。建议每个 GPU 至少有 2 个线程，即为每个 GPU 分配一个 CPU 核\\n\\n心。用这种策略，更多内核也不会让性能显著提升。\\n\\n在这种情况下， GPU 通常承担大部分计算负担， CPU 的作用更多是管理和协调，因此需要高核心数，同\\n\\n时也需要快速的数据预处理，同样需要高频率，所以 **高核心** **+** **高频率，虽然不是必须，但推我们推荐还是能**\\n\\n**高即高，标准是：要与选择的** **GPU** **和** **CPU** **的性能水平相匹配** ，避免将一款高端显卡与低端 CPU 或一款高性能\\n\\nCPU 与低端显卡匹配，因为这可能导致性能瓶颈。比如：\\n\\nNVIDIA GeForce RTX 3090 * 2 搭配 Intel Core i7-13700K/KF 或 AMD 5900X CPU ；\\n\\nNVIDIA GeForce RTX 3080 搭配 Intel Core i9-11900K CPU 。\\n\\n但相对来说，瓶颈没有那么大，一般以一个 GPU 对应 2~4 个 CPU 核数就满足基本需求，比如单卡机器买\\n\\n四核 CPU ，四卡机器买十核 CPU 。在训练的时候，只要数据生成器（ DataLoader ）的产出速度比 GPU 的消\\n\\n耗速度快，那么 CPU 就不会成为瓶颈，也就不会拖慢训练速度。\\n\\n\\n-----\\n\\n去很长的一段时间了里，英特尔一直是绝对霸主的存在，代表最先进的生产力，时至今日， AMD 在产品性能\\n\\n层面已经完全可以和 Intel 正面硬刚了。\\n\\nCPU 性能天梯图： [https://www.mydrivers.com/zhuanti/tianti/cpu/index.html](https://www.mydrivers.com/zhuanti/tianti/cpu/index.html)\\n\\n**Intel** **系列命名规范**\\n\\n可以通过 CPU 名称得到一些信息，如 i7-10700K ，代表产品型号是 i7 ，后面的 10 代表是第 10 代，然后 700\\n\\n代表性能等级高低， K 代表这个 CPU 可以超频，当然后缀字母还有 T 、 X 、 F 等， X 后缀代表高性能处理器，而 T\\n\\n代表超低电压， /F 代表无 CPU 无内置显卡版本。\\n\\n1. 系列：由低到高 Celeron （赛扬） / Pentium （奔腾） / 酷睿系列的 i3 / i5 / i7 / i9\\n\\n2. 世代：第 1 组数字代表是第几代\\n\\n例如这三个 CPU ： I7-8700 、 I7-9700 ， i7-10700 第 1 个是第八代，第 2 个是第九代、第 3 个是第十代，还\\n\\n是比较容易理解的。\\n\\n3. 性能：第 2 组 (3 个数 ) 是表示性能等级\\n\\n例如： I5-12400 、 I5-12500 ，数字越大表示越好。\\n\\n4. 后缀： K → 可超频， F → 没有核显\\n\\n可超频 K 版 CPU 要搭配可超频的 Z 系列主板才行，才可以超频，搭载不能超频的主板也可以用，只是不能\\n\\n超频了而已。\\n\\n没有核显的 F 版 CPU 要搭配独立显卡才能开机点亮屏幕\\n\\n超频简单理解就是可以提升处理器性能，核显就是把一张入门级的显卡塞进处理器里。\\n\\n**i3** **是家用级别，** **i5** **是游戏级别，** **i7** **是生产力和游戏发烧友级别，** **i9** **是最顶级的。后缀带** **K** **可以超频，带** **F**\\n\\n**表示没有核显。**\\n\\n**AMD** **系列命名规范**\\n\\n和 Intel 类似：\\n\\n1. 系列：由低到高 APU / Althlon （速龙） / Ryzen （锐龙）系列 R3 / R5 / R7 / R9\\n\\n2. 世代：第 1 个数字代表第几代\\n\\n3. 例如这两个 CPU ： R7-2700X 、 R7-3700X ，第 1 个是第二代，第 2 个是第三代。\\n\\n4. 性能：第 2 组数字（ 3 个数字）表示性能等级\\n\\n数字越大性能越好，例如 R7 3800X 的性能大于 R7 3700X 。\\n\\n5 后缀：字母 G 表示有核显 字母 X 没有明确意思 一般性能强一点 如 R5 3600X 比 R5 3600 性能高一\\n\\n\\n-----\\n\\n**要选** **Intel** **还是** **AMD** **，其实都可以。** 如果追求性价比， AMD 性价比高一些，如果主要玩游戏，且对价格\\n\\n不敏感，建议选择英特尔 Intel ，英特尔 Intel 一般主频较高，一些游戏主要依赖主频，所以高主频的 Intel 玩游\\n\\n戏更推荐一些。除了品牌维度的分析，目前 **主流的大模型训练硬件通常采用** **Intel + NVIDIA GPU** 。但具体\\n\\n情况具体分析，只能简单说：一分钱一分货，一般来说贵的好。\\n\\n**选购** **CPU** **误区**\\n\\n电子产品有一个说法是， “ 买新不买旧 ” ，一般新产品，会用更新的工艺架构，性能更强，功耗更低，比\\n\\n较值得购买。当然有些时候，老一代的性价比很高，也可以考虑，但如果是好几代前的老产品，就不要考虑\\n\\n了，有些商家会卖几年前的 i7 电脑主机，它的性能可能还不如最新的 i3 ，主要是忽悠小白的，要注意辨别。\\n\\n目前消费级市场，我们最常听到的 i3 i5 i7 等，是英特尔的酷睿系列产品，主要面向一般消费市场，数字\\n\\n大的性能更强，（注意这里只在同代产品中成立）。 AMD 与之对应的是 R3 R5 R7 。这里值得注意是，同代产\\n\\n品 i7 比 i5 强，如果拿老一代的 i7 和新一代的 i5 比，就未必成立，部分商家经常会营销 i5 免费升级 i7 ，其实是把\\n\\n最新一代的 i5 换成立了老一代的 i7 ，性能方面可能还不如没升级呢？比如 i5-8400 的性能就高于 i7-7700.\\n\\n## 3.3 散热选型策略\\n\\nCPU 不断地更新换代和性能提升，其功耗和发热量也越来越大，如果温度过高，就会出现自动关机或者\\n\\n是蓝屏死机等情况，所以需要单独的散热器来压制，目前 **CPU** **散热器分两种：水冷和风冷。**\\n\\n风冷和水冷系统都是用于 GPU 的散热解决方案。它们各有优势和不足：通常， **水冷系统在散热效率方面**\\n\\n**优于风冷系统。** 以 Intel 的 i9-13900KF 为例，这款 CPU 性能目前位于 CPU 性能天梯榜第二位，很多用户认为使\\n\\n用水冷系统是必要的。但如果这款 CPU 没有超频需求，使用高质量的风冷系统其实也能够有效地散热。只有\\n\\n在超频的情况下，水冷系统由于其更出色的冷却效果，才成为更佳的选择。\\n\\n但需要注意， **风冷和水冷与** **GPU** **无关** 。在计算机硬件中， CPU 和 GPU （显卡）的散热策略和要求各有不\\n\\n同。 CPU 通常需要配单独的散热器，我们可以根据需要选择并购买不同类型的散热器，例如水冷或风冷系\\n\\n统，并且可以根据性能要求进行升级。由于 CPU 的 **高主频和较少的核心数（通常是几个到二十几个核心）** ，\\n\\n高性能的 CPU 在运行时会产生较多热量，因此需要更有效的散热解决方案来维持合适的运行温度。与此相\\n\\n对，显卡通常采用一体化的散热设计，其散热器是显卡的重要组成部分。显卡散热器的设计已经由各厂商经\\n\\n过测试和优化，因此用户一般不需要担心显卡的散热问题。显卡采用的是 **多核心、低频率** 的策略，即使是高\\n\\n端显卡如 Nvidia 的 4090 ，其频率也相对较低，通常在 3000MHz 左右，而同代的高端 CPU （如 Intel i9 ）的频\\n\\n率可能是其两倍。显卡的散热器可以直接接触 GPU 核心和显存，从而高效散热。因此，在正常满载情况下，\\n\\n显卡的温度达到 70 多或 80 多度是正常现象，通常不会成为性能瓶颈。\\n\\n对于大模型部署来说，首要原则还是 **CPU** **的等级要和** **GPU** **相匹配** 。对于中低端处理器，如 Intel 的 i5 系\\n\\n列，以及 AMD 的 R5 和 R7 系列，一般推荐使用风冷系统。这些处理器的热设计功耗（ TDP ）通常较低，风冷系\\n\\n统足以提供有效的散热。而对于更高性能的处理器，如 Intel 的 i7 13700KF 及更高级别的 i7 和 i9 系列，建议至\\n\\n少使用 240mm 规格的水冷系统。考虑到这些处理器较高的性能和热输出，水冷系统能提供更为高效和稳定\\n\\n的冷却效果。因此，尽管风冷在某些情况下依然可行，但为了确保最佳性能和稳定性，对于高性能处理器，\\n\\n\\n-----\\n\\n在构建大模型的系统时，低端主板通常不适用。根据所选的 CPU 和 GPU 规格，应从中端或高端主板中选\\n\\n择出合适的。\\n\\n\\n\\n\\n\\n\\n\\n|制造 商|系列|定 位|支持CPU超 频|支持内存超 频|适用用户|\\n|---|---|---|---|---|---|\\n|Intel|Z系 列|高 端|是|是|追求高性能和定制化设置的用户|\\n|Intel|B系 列|中 端|否|是|需要一定性能但预算有限的用户|\\n|Intel|H系 列|低 端|否|否|预算有限或对性能要求不高的基本用 途|\\n|AMD|X系 列|高 端|是|是|追求最高性能和高度定制化的用户|\\n|AMD|B系 列|中 端|否|是|寻求性价比的用户|\\n|AMD|A系 列|低 端|否|否|有限预算或基本计算需求的用户|\\n\\n\\n选择主板时，核心因素是 **确保它与** **CPU** **的性能和超频能力相匹配** 。以 Intel 处理器为例：对于中高端 CPU\\n\\n（如 i5 系列及以上），更适合选择 B660 到 Z690 系列的主板。对于如 13600KF 这样的高性能 CPU ，至少应选择\\n\\nB660 系列的主板作为起点。需要考虑的是 CPU 是否支持超频（如带有 “K” 后缀）。可超频的 CPU 更适合搭配\\n\\n支持超频的高端主板，如 Z 系列\\n\\n其次，需要 **检查** **CPU** **和主板型号是否匹配及合理。**\\n\\n通常情况下，每一种型号的 CPU 都需要搭配对应型号的主板，每代 CPU 和主板都有自己的针角及接口类\\n\\n型， Intel cpu 不能用于 AMD 系列主板，某些主板可能会通用几代 cpu ，但有的主板只能兼容某一代，例如\\n\\nintel 十代 的 i510400f ，不能用于早期的四代 b85 系列主板，而是否匹配，指的是高性能 CPU 搭配低性能主\\n\\n板， h610 是入门主板，虽然可以点亮，但是低端主板因为供电有限，无法发挥出 cpu 的全部性能，以及无法\\n\\n超频，这样就失去了 cpu 本身的性能和意义。\\n\\n最后， **考虑** **PCIe** **通道。**\\n\\nPCIe 通道是一种高速接口，用于将 GPU 连接到计算机的主板。通过这些通道， GPU 可以与 CPU 以及系统\\n\\n内存快速交换数据。每个 PCIe 通道（或称为 “ 通道 ” ）都提供一定的数据传输带宽。更多的通道意味着更高的\\n\\n总体带宽。例如， PCIe 3.0 x16 接口意味着有 16 个通道，每个通道的速度是 PCIe 3.0 标准的速度。\\n\\nGPU 的性能部分取决于它与主板之间的通信速度，这是由 PCIe 通道的数量和版本（如 PCIe 3.0 、 4.0 或\\n\\n5.0 ）决定的。更高版本的 PCIe 提供更高的传输速率，从而可能提高 GPU 的性能。以下是需要考虑的几个关键\\n\\n点：\\n\\n1. **PCIe** **版本** ：\\n\\n\\n-----\\n\\n4.0 和 5.0 ）提供更高的数据传输速率，这对于高性能 GPU 和其他高速设备非常重要。\\n\\n2. **PCIe** **槽数量和布局** ：\\n\\n主板上的 PCIe 槽数量决定了可以安装多少个扩展卡。如果计划安装多个 GPU 或其他 PCIe 设备，需\\n\\n要确保主板有足够的槽位。\\n\\n槽位布局也很重要，尤其是在安装大型 GPU 时，需要确保它们之间有足够的空间，避免过热或物\\n\\n理干扰。\\n\\n3. **PCIe** **通道分配** ：\\n\\n主板上的 PCIe 通道是从 CPU 和芯片组分配的。不同的主板可能有不同的通道分配方式，这可能会\\n\\n影响到扩展卡的性能，特别是在多 GPU 配置中。\\n\\n确认主板是否支持您所需的 PCIe 配置，例如双向或四向 GPU 设置。\\n\\n4. **与** **GPU** **的兼容性** ：\\n\\n虽然大多数现代 GPU 兼容大多数主板的 PCIe 槽，但是为了最佳性能，最好确认 GPU 与主板的 PCIe\\n\\n版本相匹配。\\n\\n综上所述，因为需要通过 PCIe 通道连接和使用 GPU ，因此在选择主板时考虑 PCIe 通道的版本、数量、布\\n\\n局和通道分配非常重要。\\n\\n## 3.5 硬盘选型策略\\n\\n**首先考虑接口类型** 。主流固态硬盘主要有两种接口： SATA 和 M.2 。\\n\\n**SATA** **接口** 的固态硬盘体积较大，形状类似于传统的机械硬盘，主要用于升级老式电脑，因为这些电脑\\n\\n通常不具备 M.2 接口。 SATA 接口硬盘的最高速度为 600MB/s 。\\n\\n**M.2** **接口** 的硬盘则较小，可以直接安装在主板上的专用接口。它们采用新的硬盘协议，速度上可以达到\\n\\n4GB/s 。\\n\\n**推荐选择** **M.2** **接口的硬盘。**\\n\\n**然后考虑协议** 。 M.2 接口的固态硬盘分为 SATA 协议和 NVMe 协议两种。\\n\\nM.2 接口的 **SATA** **协议硬盘** 速度较慢，实际上就是标准 SATA 硬盘的形状变化，速度仍然是最高\\n\\n600MB/s ，这类硬盘多用于旧电脑。\\n\\n**NVMe** **协议硬盘** 则速度更快，适合对速度有较高要求的应用。\\n\\n在面对大量小文件的时候，使用 NVMe 硬盘可以一分钟扫完 1000 万文件，如果使用普通硬盘，那么就\\n\\n需要一天时间。 **推荐选择** **NVME** **协议的** **M.2** **接口的硬盘。**\\n\\n**最后考虑** **PCIe** **等级。** 当前市面上最新的是 PCIe 5.0 ，但更常见的是 PCIe 3.0 和 PCIe 4.0 。 PCIe 等级越\\n\\n高，硬盘的速度潜力越大。但重要的是检查主板是否支持相应的 PCIe 等级。例如，一些主板可能最高只支持\\n\\n到 PCIe 4.0 。一般来说，选择 PCIe 4.0 的即可。\\n\\n硬盘不会限制深度学习任务的运行，但如果小看了硬盘的作用，可能会让你追、悔、莫、及。想象一\\n\\n下，如果你从硬盘中读取的数据的速度只有 100MB/s ，那么加载一个 32 张 ImageNet 图片构成的 mini\\n\\n-----\\n\\n**建议内存容量应大于** **GPU** **的显存。** 例如，对于搭载单卡 GPU 的系统，建议配置至少 16GB 内存。如果是\\n\\n四卡 GPU 系统，则建议至少配置 64GB 内存。由于数据生成器（ DataLoader ）的存在，数据不需要全部加载\\n\\n到内存中，因此内存通常不会成为性能瓶颈。\\n\\n内存不用太纠结，是 GPU 显存的一到两倍。目前， 128G 就可以， 64G 也凑合。而且内存没那么贵，可\\n\\n以配满。\\n\\n内存大小不会影响深度学习性能，但是它可能会影响你执行 GPU 代码的效率。内存容量大一点， CPU 就\\n\\n可以不通过磁盘，直接和 GPU 交换数据。所以应该配备与 GPU 显存匹配的内存容量。\\n\\n在选择的时候， **注意检查主板是否支持内存的数量及型号。** 目前常见的 ddr3 ~ 5 ，每一代内存都需要对\\n\\n应主板的插槽类， ddr 5 代内存 是无法混插在 ddr 4 代内存上的。另外需要确定主板的内存插槽数量，如果只\\n\\n有两个插槽，买了四个，那么根本插不进去。\\n\\n其次检查 cpu 主板是否支持内存频率。内存条的频率上限 受到 cpu 和主板的控制， 例如 i5 的 10400f +\\n\\nb460 主板 = 2666 ，如果你买的内存是 3600 频率的，无疑发挥不出内存本身的优势。\\n\\n## 3.7 电源选型策略\\n\\n在选择电脑电源时，需要 **检查电源的瓦数是否足以支持整机的功耗。** 并非越高瓦数越好，但瓦数过低可\\n\\n能会导致关机或黑屏等问题。在确定合适的电源瓦数之前，应综合评估整机硬件的功耗，尤其要考虑 CPU 和\\n\\n显卡这两个功耗大户。通常，将 CPU 和显卡的 TDP 功耗相加后乘以 2 可以得到一个合适的电源瓦数估计。例\\n\\n如，对于一个 65W 的 CPU 和 125W 的显卡，合适的电源瓦数应该在 400W 或 450W 左右。\\n\\n双卡最好 1000W 以上，四卡最好买 1600W 的电源\\n\\n## 3.8 机箱选型策略\\n\\n最后，选择完所有上述所有配件之后，选择机箱就相对简单，只需要确保这个机箱足够宽敞，能够容纳\\n\\n所选的所有配件。我们需要检查以下几项内容：\\n\\n1. **核对主板与机箱尺寸匹配性** ：\\n\\n确保所选主板的大小与机箱兼容。例如， ITX 主板应与 ITX 机箱相匹配。这就像选择合适大小的鞋子\\n\\n一样重要。\\n\\n2. **确认机箱支持显卡尺寸** ：\\n\\n对比显卡的长度与机箱对显卡长度的限制。建议选择机箱的显卡限长至少比显卡长度长出 30 毫米\\n\\n以上，以确保有足够空间进行安装和通风。\\n\\n3. **检查散热器与机箱的兼容性** ：\\n\\n非常重要的一点是比较散热器的尺寸与机箱的散热限高。如果散热器太高，可能无法正常安装侧\\n\\n盖。\\n\\n考虑到许多内存条配备较高的散热马甲，需要确认散热风扇是否会受到内存条的干扰。\\n\\n如果选择水冷散热系统，确保机箱的水冷位能够容纳水冷冷排的尺寸。例如， 360mm 的水冷冷排\\n\\n无法安装在仅适用于 240mm 的位置上。\\n\\n\\n-----\\n\\n常见的电源类型包括 SFX 、 ATX 和 TFX 。由于不同规格的电源在形状和大小上有所不同，必须确认\\n\\n机箱的电源仓是否适合所选电源的尺寸。\\n\\n\\n-----'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63198bff-a70c-46e8-a271-be3b93715876",
   "metadata": {},
   "source": [
    "&emsp;&emsp;保存为本地的.csv文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02b0c666-2d2f-4e8d-8a72-311179853a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: E:\\01_木羽研发\\03_RAG正课\\5-1\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 打印当前工作目录\n",
    "print(\"当前工作目录:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c34558-e822-448b-b16b-f297baf43185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "csv_file_path = os.path.join('.', 'output.csv')\n",
    "\n",
    "# 保存 DataFrame 到 CSV 文件\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8511f-48b4-4395-827c-2d1ffcb763e9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们要做两件事：\n",
    "1. 依次遍历这个data数据的 Content 字段，调用一个大模型，对这段文本生成摘要，然后填充一个新的字段：Abstract\n",
    "2. 同时判断 Tags 是否为空，如何为空的话，提取 URL 相同的 Tags中的内容作为提示，让大模型生成Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80b2b7f-8922-48c7-bbf9-87ed6dc6463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"智谱开放，链接未来——与智能共舞，创想无限可能！\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI() # 填写您自己的APIKey\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"作为一名营销专家，请为智谱开放平台创作一个吸引人的slogan\"},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5657b95-cb29-4eb1-ad54-582802736130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "\n",
    "client = ZhipuAI() \n",
    "\n",
    "# 假设用于生成摘要的函数\n",
    "def generate_summary(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "        messages=[{\"role\": \"user\", \"content\": \"作为一名语言学专家，请根据如下的输入文本:\\n{} \\n生成一段摘要\".format(text)}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 假设这是你用于生成 Tags 的函数\n",
    "def generate_tags(prompt):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "        messages=[{\"role\": \"user\", \"content\": \"作为一名语言学专家，请根据类似的示例:\\n{} \\n生成这段文本的关键性标签，注意：一定要简短且能抓住重点\".format(prompt)}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d52e39db-2386-46fc-af1a-19cad0759238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 CSV 文件\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "\n",
    "# 遍历 DataFrame，生成摘要\n",
    "df['Abstract'] = df['Content'].apply(generate_summary)\n",
    "\n",
    "# 检查并填充空的 Tags\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['Tags']) or row['Tags'].strip() == \"\":\n",
    "        # 查找具有相同 URL 的其他记录的 Tags\n",
    "        similar_tags = df[df['URL'] == row['URL']]['Tags'].dropna()\n",
    "        if not similar_tags.empty:\n",
    "            tag_prompt = similar_tags.iloc[0]  # 使用第一个非空标签作为提示\n",
    "            generated_tags = generate_tags(tag_prompt)\n",
    "            df.at[index, 'Tags'] = generated_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "057a7ab5-5324-4722-bbfb-beda8c22786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModuleID</th>\n",
       "      <th>Course</th>\n",
       "      <th>URL</th>\n",
       "      <th>ModuleName</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Content</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>461b2584-6a57-4324-9d72-c0d2f0630f15</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>本地部署开源大模型</td>\n",
       "      <td>Ch.1 如何选择合适的硬件配置</td>\n",
       "      <td>## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型， **深入...</td>\n",
       "      <td>为了有效部署开源大模型，理解硬件与软件需求是关键。在硬件上，需配置高性能的个人计算机或租用配...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ce06c456-8d0d-497c-82b5-3affdb70cf2b</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>一、大模型应用需求分析</td>\n",
       "      <td>\"硬件配置选择指南\" 或 \"选择硬件关键因素\"</td>\n",
       "      <td>大模型的本地部署主要应用于三个方面： **训练（** **train** **）、高效微调（...</td>\n",
       "      <td>本地部署大模型主要涉及训练、高效微调和推理三个方面，各阶段算力消耗依次递减。由于从头训练大模...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2b731c2-f1c7-42a5-b261-5518ad403068</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>二、硬件配置的选择标准</td>\n",
       "      <td>2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡 4090 v...</td>\n",
       "      <td>无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...</td>\n",
       "      <td>本文讨论了深度学习和AI大模型所需的GPU硬件配置。首先介绍了不同应用场景对硬件配置的需求，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7ea70fff-2919-412b-90a4-9382acc186f0</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>三、组装计算机硬件选型策略</td>\n",
       "      <td>3.1 GPU 选型策略, 3.2 CPU 选型策略, 3.3 散热选型策略, 3.5 硬盘...</td>\n",
       "      <td>计算机八大件： CPU 、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于...</td>\n",
       "      <td>本文详细介绍了配置用于部署大模型系统的个人计算机的硬件选择策略。主要内容包括：\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ModuleID                      Course  \\\n",
       "0  461b2584-6a57-4324-9d72-c0d2f0630f15  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "1  ce06c456-8d0d-497c-82b5-3affdb70cf2b  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "2  e2b731c2-f1c7-42a5-b261-5518ad403068  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "3  7ea70fff-2919-412b-90a4-9382acc186f0  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "\n",
       "                                   URL     ModuleName  \\\n",
       "0  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']      本地部署开源大模型   \n",
       "1  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']    一、大模型应用需求分析   \n",
       "2  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']    二、硬件配置的选择标准   \n",
       "3  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']  三、组装计算机硬件选型策略   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                                   Ch.1 如何选择合适的硬件配置   \n",
       "1                            \"硬件配置选择指南\" 或 \"选择硬件关键因素\"   \n",
       "2  2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡 4090 v...   \n",
       "3  3.1 GPU 选型策略, 3.2 CPU 选型策略, 3.3 散热选型策略, 3.5 硬盘...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  ## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型， **深入...   \n",
       "1  大模型的本地部署主要应用于三个方面： **训练（** **train** **）、高效微调（...   \n",
       "2  无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...   \n",
       "3  计算机八大件： CPU 、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  为了有效部署开源大模型，理解硬件与软件需求是关键。在硬件上，需配置高性能的个人计算机或租用配...  \n",
       "1  本地部署大模型主要涉及训练、高效微调和推理三个方面，各阶段算力消耗依次递减。由于从头训练大模...  \n",
       "2  本文讨论了深度学习和AI大模型所需的GPU硬件配置。首先介绍了不同应用场景对硬件配置的需求，...  \n",
       "3  本文详细介绍了配置用于部署大模型系统的个人计算机的硬件选择策略。主要内容包括：\\n\\n1. ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ba39d54-affa-49c5-8e2e-8b244d721efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存更新后的 DataFrame 到新的 CSV 文件\n",
    "df.to_csv(\"updated_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29e225eb-d50e-45a9-945d-8fc190d7097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_course = pd.read_csv(\n",
    "    'updated_output.csv',\n",
    "    sep=',',\n",
    "    header=0)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a72c57-8ed8-45a7-9a1b-28494b51de07",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后，新增embedding_info列，用于构建Embedding向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b008398a-13a3-458a-9877-2d0482cbe7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModuleID</th>\n",
       "      <th>Course</th>\n",
       "      <th>URL</th>\n",
       "      <th>ModuleName</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Content</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>embedding_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>461b2584-6a57-4324-9d72-c0d2f0630f15</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>本地部署开源大模型</td>\n",
       "      <td>Ch.1 如何选择合适的硬件配置</td>\n",
       "      <td>## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型， **深入...</td>\n",
       "      <td>为了有效部署开源大模型，理解硬件与软件需求是关键。在硬件上，需配置高性能的个人计算机或租用配...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ce06c456-8d0d-497c-82b5-3affdb70cf2b</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>一、大模型应用需求分析</td>\n",
       "      <td>\"硬件配置选择指南\" 或 \"选择硬件关键因素\"</td>\n",
       "      <td>大模型的本地部署主要应用于三个方面： **训练（** **train** **）、高效微调（...</td>\n",
       "      <td>本地部署大模型主要涉及训练、高效微调和推理三个方面，各阶段算力消耗依次递减。由于从头训练大模...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2b731c2-f1c7-42a5-b261-5518ad403068</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>二、硬件配置的选择标准</td>\n",
       "      <td>2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡 4090 v...</td>\n",
       "      <td>无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...</td>\n",
       "      <td>本文讨论了深度学习和AI大模型所需的GPU硬件配置。首先介绍了不同应用场景对硬件配置的需求，...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7ea70fff-2919-412b-90a4-9382acc186f0</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>三、组装计算机硬件选型策略</td>\n",
       "      <td>3.1 GPU 选型策略, 3.2 CPU 选型策略, 3.3 散热选型策略, 3.5 硬盘...</td>\n",
       "      <td>计算机八大件： CPU 、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于...</td>\n",
       "      <td>本文详细介绍了配置用于部署大模型系统的个人计算机的硬件选择策略。主要内容包括：\\n\\n1. ...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ModuleID                      Course  \\\n",
       "0  461b2584-6a57-4324-9d72-c0d2f0630f15  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "1  ce06c456-8d0d-497c-82b5-3affdb70cf2b  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "2  e2b731c2-f1c7-42a5-b261-5518ad403068  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "3  7ea70fff-2919-412b-90a4-9382acc186f0  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "\n",
       "                                   URL     ModuleName  \\\n",
       "0  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']      本地部署开源大模型   \n",
       "1  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']    一、大模型应用需求分析   \n",
       "2  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']    二、硬件配置的选择标准   \n",
       "3  ['开源大模型课件\\\\Ch 1 开源大模型本地部署硬件指南.pdf']  三、组装计算机硬件选型策略   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                                   Ch.1 如何选择合适的硬件配置   \n",
       "1                            \"硬件配置选择指南\" 或 \"选择硬件关键因素\"   \n",
       "2  2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡 4090 v...   \n",
       "3  3.1 GPU 选型策略, 3.2 CPU 选型策略, 3.3 散热选型策略, 3.5 硬盘...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  ## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型， **深入...   \n",
       "1  大模型的本地部署主要应用于三个方面： **训练（** **train** **）、高效微调（...   \n",
       "2  无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...   \n",
       "3  计算机八大件： CPU 、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  为了有效部署开源大模型，理解硬件与软件需求是关键。在硬件上，需配置高性能的个人计算机或租用配...   \n",
       "1  本地部署大模型主要涉及训练、高效微调和推理三个方面，各阶段算力消耗依次递减。由于从头训练大模...   \n",
       "2  本文讨论了深度学习和AI大模型所需的GPU硬件配置。首先介绍了不同应用场景对硬件配置的需求，...   \n",
       "3  本文详细介绍了配置用于部署大模型系统的个人计算机的硬件选择策略。主要内容包括：\\n\\n1. ...   \n",
       "\n",
       "                                      embedding_info  \n",
       "0  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  \n",
       "1  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  \n",
       "2  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  \n",
       "3  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新增 embedding_info_info 列，合并`类别、子类别、标题、摘要`字符串\n",
    "concatenation_order = [\"Course\", \"ModuleName\", \"Abstract\"]\n",
    "\n",
    "embedding_info_list = df_course['embedding_info'] = df_course.apply(lambda row: '{ ' + ', '.join(\n",
    "    f'\"{col}\": \"{row[col]}\"' for col in concatenation_order) + ' }', axis=1)\n",
    "\n",
    "df_course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
