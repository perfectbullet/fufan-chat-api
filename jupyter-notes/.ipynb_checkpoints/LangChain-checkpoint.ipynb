{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73cc7fd0-3165-49b4-b81a-a18585d65f42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center>LangChain如何接入在线大模型并构建链路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecb82a-151b-4486-ad2b-751751cea0b5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LangChain归属于LangChain AI公司，LangChain作为其中的一个核心项目，开源发布在Gitub上：https://github.com/langchain-ai/langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de5edc-02a1-408c-9b77-c8e7758e3d81",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从LangChain的GitHub版本迭代历史上看，从2023年1月16日起已经经历了320个大小版本的迭代，并且仍然以高频率的更新在加速项目的功能上线，从整体上看，其关注度和社区活跃度是非常高的。LangChain给自身的定位是：用于开发由大语言模型支持的应用程序的框架。它的做法是：通过提供标准化且丰富的模块抽象，构建大语言模型的输入输入规范，利用其核心概念`chains`，灵活地连接整个应用开发流程。而针对每个功能模块，都源于对大模型领域的深入理解和实践经验，开发者提供出来的标准化流程和解决方案的抽象，再通过灵活的模块化组合，才有了目前这样一款在大模型应用开发领域内被普遍高度认可的通用框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c43a7b-43d6-496f-ae1c-e9b99dedaa33",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20240610175527397.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebc032-92f4-48ad-bfa7-0dc38f75d489",
   "metadata": {},
   "source": [
    "- **为什么需要这样做？**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dcafc-58ba-4601-b648-59dd3a6e0b2c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，我们需考虑当前大模型的发展态势。尽管OpenAI的GPT系列模型作为大模型领域的`领军人物`，在很大程度上了影响了大模型的使用规范和基于大模型进行应用开发的范式，但并不意味着所有大模型间的使用方式完全相同。例如，我们熟悉的OpenAI GPT模型API调用方式对于Baichuan2模型就不适用。因此，对于每个新模型都要花费大量时间学习其特定规范再进行应用探索，这种工作效率显然是十分低下的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0cc6f-5de8-4137-9455-3a9bdb3f15d1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其次，必须谈论的是大模型目前面临的局限，如知识更新的滞后性、外部API调用能力、私有数据连接方式以及输出结果的不稳定性等问题。在应用开发中，如何找到这些问题的有效解决策略？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c191e8a-747f-4f54-980b-81098e63165d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202403061934373.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741c1c7-ef13-4d73-b7cd-e194ed59e67e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述提到的每个限制都紧密关联于大模型本身的特性。尽管理论上可以通过重新训练、微调来增强模型的原生能力，这种方法确实有效，但实际上，大多数开发者并不具备进行这样操作所需的技术资源、时间和财力，选择这条路径一定会导致方向越来越偏离目标。比如关于大模型的Agents、函数调用等功能，每一步都需大量的研发投入，而且最终实现后的应用效果，也取决于研发人员的个人技术能力。在这种背景下，既然大家都有不同的想法和解决方案，那LangChain就来集中做这件事，提供一个统一的平台和明确的定义，来实现应用框架的快速搭建，这就是LangChain一直想要做到，且正在做的事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6eed2-4469-4a24-ba2d-49abf2b37de3",
   "metadata": {},
   "source": [
    "- **LangChain的做法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef4f7b-3e3a-47b0-9bc3-f82a8644d274",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从本质上分析，LangChain还是依然采用从大模型自身出发的策略，通过开发人员在实践过程中对大模型能力的深入理解及其在不同场景下的涌现潜力，使用模块化的方式进行高级抽象，设计出统一接口以适配各种大模型。到目前为止，LangChain抽象出最重要的核心模块如下：\n",
    "\n",
    "1. Model I/O ：标准化各个大模型的输入和输出，包含输入模版，模型本身和格式化输出；\n",
    "2. Retrieval ：检索外部数据，然后在执行生成步骤时将其传递到 LLM，包括文档加载、切割、Embedding等；\n",
    "3. Chains ：链条，LangChain框架中最重要的模块，链接多个模块协同构建应用，是实际运作很多功能的高级抽象；\n",
    "4. Memory ： 记忆模块，以各种方式构建历史信息，维护有关实体及其关系的信息；\n",
    "5. Agents ： 目前最热门的Agents开发实践，未来能够真正实现通用人工智能的落地方案；\n",
    "6. Callbacks ：回调系统，允许连接到 LLM 应用程序的各个阶段。用于日志记录、监控、流传输和其他任务；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e934cc8-c2a8-4863-90b7-e5b40435990f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20240610175822626.png\" width=70%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6477124-88d2-40e1-bcc0-a62e4dcd5b8e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从上图中可以看到，LangChain框架涵盖了模型输入输出的标准化、外部工具接入的规范、上下文记忆功能，以及对数据库、SQL、CSV等多种数据源的连接标准。通过核心的\"Chain\"高级抽象，定义了不同形式下标准的链接方法，这就能够允许开发者根据实际的应用需求和数据流向快速构建出一套完整的应用程序。这个过程类似于搭建积木，可以灵活适应不同的任务需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb2d0a-37f1-4179-a250-2cca1edd611e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202403071044331.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9db6ba-f067-4a08-afa6-29de20662a3f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;也正因为如此，LangChain中涉及的概念和模块化是非常多的，每个模块都有其独特的使用场景和使用方法，那么如何去搭这个“积木”，就需要我们对其每个核心模块都要有一个比较清楚的认知。所以我们课程的安排还是逐个拆解LangChain的功能模块，每一部分都尽可能的给大家做详细的介绍和实操，并在接下来的项目部分，进行整体的一个串联，届时大家将能够清晰的明确如何根据自己的实际业务情况，选择合适的构造模块和构造方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30533e5-0958-4dcb-8898-ddb32f28272d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在整体上理解了LangChain之后，我们首先从Model I/O模块进行深入的探讨和实践。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7899447-6424-45cc-a68d-85696bcac664",
   "metadata": {},
   "source": [
    "# 1. LangChain核心模块：Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2752f-fcb6-4467-8326-d8c7c716c8a0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LangChain的Model I/O模块提供了标准的、可扩展的接口实现与大语言模型的外部集成。所谓的Model I/O，包括模型输入（Prompts）、模型输出（OutPuts）和模型本身（Models），简单理解就是通过该模块，我们可以快速与某个大模型进行对话交互，整个内部逻辑就相当于我们最熟悉的这个过程：输入Prompt，得到大模型针对该Prompt的推理结果。如下示例为OpenAI的 GPT 系列模型的API 调用规范：\n",
    "\n",
    "```python\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"请问，什么是机器学习？\"}\n",
    "  ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18d9dd-c220-44b8-bb55-f392e54eb693",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而我们前面提到了，LangChain项目的定位为一个应用开发框架，所以如果仅仅集成到这样的对话交互程度，那相较于直接使用OpenAI的API调用又有何异呢？所以在这个模块中，LangChain同样抽象出一个`chain`，用于进一步简化和增强交互流程。在LangChain的Model I/O模块设计中，包含三个核心部分： Prompt Template（对应下图中的Format部分）， Model（对应下图中的Predict部分） 和Output Parser（对应下图中的Parse部分）。\n",
    "\n",
    "- **Format：即指代Prompts Template，通过模板化来管理大模型的输入；**\n",
    "- **Predict：即指代Models，使用通用接口调用不同的大语言模型；**\n",
    "- **Parse：即指代Output部分，用来从模型的推理中提取信息，并按照预先设定好的模版来规范化输出。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7b346-06ae-4cfc-955b-707df69a0138",
   "metadata": {},
   "source": [
    "&emsp;&emsp;整个Model I/O工作流如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd7809a-8d67-45cd-a359-db013c057650",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202403041734638.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7f742-3b47-4600-8331-ba7441835fe1",
   "metadata": {},
   "source": [
    "- **Format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3efe6-61d9-4dd3-a2cd-da803453b167",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于Prompt Template第一部分，传统上我们创建提示词是通过手工编写来实现的，在这个过程中会利用各种提示工程技巧，如Few-Shot、链式推理（CoT）等方法，以提高大模型的推理性能。然而，**在应用开发中，一个关键的考量是提示词不能是一成不变的。**其原因在于，应用开发需要适应多变的用户需求和场景。固定的提示词限制了模型的灵活性和适用范围。例如，如果我们正在开发一个天气查询应用，用户可能会以多种方式提出查询，如“今天的天气怎么样？”或“明天纽约的温度是多少度？”。如果提示词是固定的，它可能只能处理一种特定类型的查询，而无法适应这种多样性的需求。\n",
    "\n",
    "&emsp;&emsp;而Prompt Template，就像ReAct一样，将API的使用、问题解答过程等复杂逻辑封装成了一套结构化的格式。我们只需准备具体的外部函数信息和用户查询，即可生成定制化的提示词，引导模型按照既定逻辑进行思考和回答，从而实现外部函数的调用过程，即：\n",
    "```json\n",
    "# 将一个插件的关键信息拼接成一段文本的模版。\n",
    "TOOL_DESC = \"\"\"{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}\"\"\"\n",
    "\n",
    "# ReAct prompting 的 instruction 模版，将包含插件的详细信息。\n",
    "PROMPT_REACT = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n",
    "\n",
    "{tool_descs}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {query}\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830710e5-a332-4c2e-9cb4-e048292a6f7b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，引入Prompt Template可以支持变量和动态内容的插入，使得同一个应用可以根据不同的输入动态调整提示词，从而更好地响应用户的具体需求。LangChain通过这种方式来提高应用的通用性和用户体验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be434394-32af-4030-a77a-68c7dd06ffe3",
   "metadata": {},
   "source": [
    "- **Predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b493a5b-cce4-4e44-accf-c7764b9edef9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在Predict部分，实质上是处理模型从接收输入到执行推理的整个过程。考虑到存在两种主要类型的大模型——Base类模型和Chat类模型，LangChain在其Model I/O模块中对这两种模型都进行了抽象，分别归类为LLMs（Large Language Models）和Chat Models。我们还是以OpenAI 的 Completion 和 Chatcompletions方法为例："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef427cee-6566-4992-9e2e-505ed88d63a7",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Base类模型\n",
    "client.completions.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"Say this is a test\",\n",
    ")\n",
    "\n",
    "\n",
    "# 聊天模型\n",
    "client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"你是一位乐于助人的AI智能小助手\"},\n",
    "    {\"role\": \"user\", \"content\": \"你好，请你介绍一下你自己。\"}\n",
    "  ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99bf70-02d3-4dde-b43b-9ea2192c0605",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LLMs是简化的大语言模型抽象，即基于给定的Prompt提供内容生成的功能。而Chat Models则专注于聊天API的抽象，需要维护上下文的记忆（聊天记录），呈现出更接近对话或聊天形式的交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a625f-685a-482f-a1db-486426e8c47b",
   "metadata": {},
   "source": [
    "- **Parse**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70a049-ee05-4bf3-88ab-a886dd51a6d8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们知道，大模型的输出是不稳定的，同样的输入Prompt往往会得到不同形式的输出。在自然语言交互中，不同的语言表达方式通常不会造成理解上的障碍。但在应用开发中，大模型的输出可能是下一步逻辑处理的关键输入。因此，在这种情况下，规范化输出是必须要做的任务，以确保应用能够顺利进行后续的逻辑处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068ccb8-3ded-406a-b086-d14af4147b1d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;输出解析器 Output Parser就是一个帮助结构化语言模型响应的抽象，可以获取格式指令或者进行更深层次的解析。这我们会在后面的实践中直观的体验到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d6c16-202f-42ed-9e89-18e1a5f01cb7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;整体而言，在Model I/O模块的抽象中，其一能够让开发者快速的接入不同的大模型，比如OpenAI、ChatGLM、Qwen等，按照既定规范执行模型推理。其二通过输入和输出的模板化处理，使其更贴合于应用开发的最佳实践。接下来，我们就逐步的介绍上述三个流程在LangChain下是如何进行集成和操作的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468dbee-6d02-496b-a626-8f28b72e3739",
   "metadata": {},
   "source": [
    "# 2. Installing LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e462b38-6965-4748-9770-1c5c79070e79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "> LangChain Docs：https://python.langchain.com/v0.2/docs/how_to/installation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b1a188-02c4-4c1d-af6f-606ae7608d28",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
      "  Using cached langchain_core-0.2.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.1.67-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.3-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.1)\n",
      "Using cached langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "Using cached langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
      "Using cached langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Using cached langsmith-0.1.67-py3-none-any.whl (124 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.10.3-cp311-none-win_amd64.whl (138 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: packaging, orjson, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed jsonpatch-1.33 langchain-0.2.1 langchain-core-0.2.3 langchain-text-splitters-0.2.0 langsmith-0.1.67 orjson-3.10.3 packaging-23.2\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e585e-55ed-4571-950a-a4ed249fa826",
   "metadata": {},
   "source": [
    "# 3. Build LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b00688-4f8d-484c-95d5-c20b21a675fd",
   "metadata": {},
   "source": [
    "> LangChain Docs：https://python.langchain.com/v0.2/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27505a-7281-45e5-8bfd-83479709bd75",
   "metadata": {},
   "source": [
    "## 3.1 LLMs如何构建链路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91005e7-f374-4a30-b02c-2b963ed14e52",
   "metadata": {},
   "source": [
    "> LangChain Docs：https://python.langchain.com/v0.2/docs/concepts/#chat-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215df08-dcfa-4bdc-9b9c-f2275214b5d7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LLMs 表示的是补全类的大模型， 它是将字符串作为输入并返回字符串的语言模型。LangChain中LLMs 的集成页面：https://python.langchain.com/v0.2/docs/integrations/llms/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dac187-66d9-416f-a0be-8785a56762b5",
   "metadata": {},
   "source": [
    "> OpenAI Completion Docs: https://platform.openai.com/docs/api-reference/completions/create?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f11eb3-0771-4fa4-99ae-198a9cc6b927",
   "metadata": {},
   "source": [
    "- **需要理解的第一个概念：如何调用LangChain的Completions 模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02130c6-6078-4ffe-a0f8-5dcbff29ac68",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因为GLM并没有开发补全模型，所以我们这里使用OpenAI的instrut模型进行进行演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b5c376-919e-4156-904f-c7c8ae9b1fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# get a token: https://platform.openai.com/account/api-keys\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69940a49-6f66-4865-a964-c8b2715d5ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id='cmpl-9W1zu2TXP3iU13WS9GZnAgvivA3bt', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\n\\n快乐星球')], created=1717421202, model='gpt-3.5-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.completions.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"请问，什么是快乐星球？\",\n",
    "  max_tokens=7,\n",
    "  temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a9711-4126-4f36-b6f3-f7ce6da77c82",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们看，当langChain应用框架集成了OpenAI的 completions 接口后，就可以用LangChain定义的规范来进行调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2570e95c-8f90-4a19-ad9d-d099f6d20088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa245d-35e3-4d8a-83ec-305c8cc10fc2",
   "metadata": {},
   "source": [
    "- **需要理解的第二个概念：如何在LangChain中调用LLMs Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28a8900-b41b-43b4-b7a5-32416c307725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb69632-a584-4080-a350-791b4b7e4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cf687b0-5050-46ce-81dc-4895a9c21aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eddd3a1-61d5-4569-900f-70236f395302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n快乐星球是一个虚构的理想世界，是指一个充满和平、友爱、幸福和快乐的星球。它是人们憧憬的理想社会，没有战争、贫穷、疾病和歧视，人们生活在和谐、平等和自由的环境中。快乐星球象征着人类对美好未来的向往和追求，也是人们对和平与幸福的渴望和希望。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"请问，什么是快乐星球？\"\n",
    "\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e6f7f-f0d2-4791-863d-711a110494e8",
   "metadata": {},
   "source": [
    "- **需要理解的第三个概念：什么是PromptTemplate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5f40c-b26a-45b3-bb8c-36d692709ee8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于应用流程来说，输入大模型的关键信息往往是不确定的。也就是说：一个Prompt的主体可以固定，但关键位置的信息通常是使用一个或多个变量来做占位。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c0cc946-3ba9-435f-885f-fa33fa949ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"问题: {question}\n",
    "\n",
    "答案: 请一步一步的思考。\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354c3d1-f49d-44a6-b5a1-203f099e174c",
   "metadata": {},
   "source": [
    "对于这种情况，LangChain就抽象了一个提示模板，用来将用户输入和参数转换为语言模型的指令。用于指导大模型的响应，帮助其理解上下文并生成相关且连贯的基于语言的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b1d64d9-a24c-4215-95f1-87ea1b8c14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49ce0a22-1c69-4829-af37-2952d6cc26f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='问题: {question}\\n\\n答案: 请一步一步的思考。')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e97e288d-ceae-4d63-8217-84158eabde41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='问题: 什么是快乐星球？\\n\\n答案: 请一步一步的思考。')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({\"question\": \"什么是快乐星球？\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a62526-730d-48fd-9e79-1beea0d7f4c6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于多变量是一样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d762b87-e852-4bc3-8521-4c4e6b21160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1 = \"\"\"问题: {question}\n",
    "\n",
    "请用{output}回答\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ecd5fb4-b59b-453c-9d83-596438f306a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = PromptTemplate.from_template(template_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "086268bb-8c45-43ef-91cf-41abbcc7702c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['output', 'question'], template='问题: {question}\\n\\n请用{output}回答')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d776876f-b26a-4d7a-b7fd-e10e33b35ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='问题: 什么是快乐星球？\\n\\n请用英语回答')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template1.invoke({\"question\": \"什么是快乐星球？\", \"output\":\"英语\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad203f-76f8-42ff-9942-5de31c844b36",
   "metadata": {},
   "source": [
    "- **需要理解的第三个概念：LLMChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f2aec-1b6d-4e54-999b-05acfcdbc5ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202403041734638.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "093b8a23-54d3-41cc-aeca-cfe5f8051c33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9b4ba-9674-4965-a825-9ddf4c97a10d",
   "metadata": {},
   "source": [
    "> LangChain Docs：https://api.python.langchain.com/en/v0.1/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "36b5611c-a0a4-4615-b585-4ecb158bc591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '什么是快乐星球',\n",
       " 'text': '\\n\\n快乐星球是一个想象中的虚拟星球，它是一个充满快乐和幸福的地方。在这个星球上，每个人都可以自由地追求自己的梦想，没有痛苦和压力的存在。人与人之间和谐相处，没有战争和冲突。这个星球上的环境也是非常美好的，充满了绚丽的色彩和美妙的音乐。在快乐星球上，每个人都能够找到自己的快乐，享受生活的乐趣。它是人们对于理想社会的向往和梦想。'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "template = \"\"\"问题: {question}\n",
    "\n",
    "答案: 请一步一步的思考。\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "\n",
    "chain.invoke(\"什么是快乐星球\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e05e5c-241e-4fdb-906a-bf5a1ed2e3c6",
   "metadata": {},
   "source": [
    "- **需要理解的第四个概念：什么是LangChain的 LCEL声明语法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81aabb-73cf-4ece-b15a-81e78f3260b6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述是旧版本LangChain构建链路的一种形式,主要还是基于传统类的构建方法.但LangChain的迭代,是在向新的一种链路声明方式转变,即LCE.LangChain 表达式语言（LCEL）是一种链接 LangChain 组件的声明性方式。 无需更改代码，可以实现从最简单的“提示 + LLM”链到最复杂的链（我们已经看到人们成功运行了 100 秒的 LCEL 链）生产步骤）."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f343536-c03c-462b-9545-40a9e9337b5e",
   "metadata": {},
   "source": [
    "> LangChain Docs:https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "111f79e6-6f54-42ca-806e-76ef3be8172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "template = \"\"\"问题: {question}\n",
    "\n",
    "答案: 请一步一步的思考。\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1eb25aff-49d9-4aaa-a901-47a1e96b0c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n快乐星球可以理解为一个想象中的幻想世界，它是一个充满欢笑、快乐和和谐的星球。在这个星球上，所有的生物都拥有幸福快乐的生活，没有战争、疾病和贫穷的困扰。这里的人们相互关爱、友善和包容，生活在和平与幸福的环境中。快乐星球是人们向往的理想国，也是我们应当努力追求的目标。'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"什么是快乐星球\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d07e0c-e908-4e14-a148-b1df55276ea6",
   "metadata": {},
   "source": [
    "## 3.2 Chat Model 如何构建链路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b02656-dfa9-4156-b125-d6a8fbc91907",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Chat Models 类模型往往是基于 LLMs 类模型针对对话形式特殊微调过的模型.更适用于人类的对话习惯.所以它往往是使用消息序列作为输入并返回聊天消息作为输出（而不是使用纯文本）的语言模型。聊天模型支持为对话消息分配不同的角色，有助于区分来自 AI、用户和​​系统消息等指令的消息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fda994-8d49-4f0e-8b49-5a9c1ee9e6cb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;比如目前OpenAI的GPT系列模型、GLM4 系列模型，现在是全面支持对话类模型:https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d831d6-2980-4c33-8eb3-987c5c7a6219",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同样我们快速的测试一下,如果使用GLM 4 官方提供的接口来调用GLM 4 系列模型. GLM-4作为在线大模型,其调用方法被抽象在LangChain中的Chat Models模块中."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a3133-f69b-4624-b824-34008ab78185",
   "metadata": {},
   "source": [
    "> langChain Docs:https://python.langchain.com/v0.2/docs/integrations/chat/zhipuai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c832fab-a7d5-4a94-909b-6dab23369719",
   "metadata": {},
   "source": [
    "- **需要理解的第五个概念：如何在LangChain中调用Chat Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116c3b9-38a7-456d-9e78-e2ab90181325",
   "metadata": {},
   "source": [
    "> GLM 4 API Docs: https://open.bigmodel.cn/dev/api#glm-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76346b3d-f9bd-4218-b904-77b19530c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade httpx httpx-sse PyJWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb112728-2d98-4289-a1a0-9f44db962b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b31d1e-e245-451c-b727-620c7ffb3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"ZHIPUAI_API_KEY\"] = \"zhipuai_api_key\"\n",
    "\n",
    "zhipuai_api_key = \"3fd2ee07b86207a7e0f7e79ee459fbfa.9iiQgqVfddDOobbS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591a002e-1ced-4d25-a735-1f0c2c4f8d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat = ChatZhipuAI(\n",
    "    zhipuai_api_key = zhipuai_api_key,\n",
    "    model=\"glm-4\",\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64241214-b81a-4229-95de-d888f2a5e32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！我是智谱清言，是清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。', response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 10, 'total_tokens': 82}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-e9d8504a-104c-402e-b623-2d2281ae6d1b-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"你好，请你介绍一下你自己\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef54894d-9b57-4c87-a6f2-55bf16dfc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一位诗人\"),\n",
    "    HumanMessage(content=\"请给我写一首关于小鸭子的师\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f722645a-d86a-42ac-af01-03f89fb2d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小鸭子之歌\n",
      "\n",
      "悠悠水面上，\n",
      "碧波荡漾间，\n",
      "一群小鸭子，\n",
      "嬉戏舞翩翩。\n",
      "\n",
      "毛茸茸的身体，\n",
      "黄色的小嘴，\n",
      "活泼又可爱，\n",
      "宛如水中仙。\n",
      "\n",
      "阳光洒金辉，\n",
      "清风送凉爽，\n",
      "小鸭子们欢歌，\n",
      "庆祝成长的时光。\n",
      "\n",
      "翅膀渐丰满，\n",
      "离巢欲飞翔，\n",
      "勇敢向前行，\n",
      "追逐梦想的方向。\n",
      "\n",
      "愿你们永保纯真，\n",
      "无畏风浪起，\n",
      "用歌声传递爱，\n",
      "温暖每一个心房。\n",
      "\n",
      "小鸭子们啊，\n",
      "让我们同行，\n",
      "在生活的海洋，\n",
      "谱写出美丽的篇章。\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(messages)\n",
    "print(response.content)  # Displays the AI-generated poem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e810523-5de6-4516-ad3c-4906429ab26f",
   "metadata": {},
   "source": [
    "- **需要理解的第六个概念：什么是ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417d0ca6-ed06-4936-beee-0829978a0969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='你是一位乐于助人的小助理'), HumanMessage(content='什么是 快乐星球')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一位乐于助人的小助理\"),\n",
    "    (\"user\", \"什么是 {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"快乐星球\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9fc4f-6dbc-4079-8cb1-d33e10a43148",
   "metadata": {},
   "source": [
    "- **需要理解的第七个概念:如何构建Chat Models 的LLMChain链路**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ed5024-4b8f-4e5e-bdb5-f3eed6246968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snowb\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一位乐于助人的小助理\"),\n",
    "    (\"user\", \"什么是 {topic}\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "chat = ChatZhipuAI(\n",
    "    zhipuai_api_key = zhipuai_api_key,\n",
    "    model=\"glm-4\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(prompt=prompt_template, llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc4b817-5150-45f9-9920-7a71f7756c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': '什么是快乐星球?',\n",
       " 'text': '\"什么是快乐星球\"通常指的是一个概念或者是一种虚构的设定，它可以在不同的语境和作品中具有不同的含义。\\n\\n在您提供的参考信息中，\"快乐星球\"有几个不同的含义：\\n\\n1. 在一部少儿科幻电视剧《快乐星球》中，它是一个虚构的中等质量行星，位于一个大型星云中，围绕一颗红巨星旋转。这个星球环境适宜人类居住，有着和地球类似的自转和公转周期。在这部剧中，快乐星球被描绘成一个充满科幻色彩和童趣的地方。\\n\\n2. 在动画电影《快乐星球》立项的梗概中，乐乐这个角色在老顽童爷爷的培养下成为了一名AI科学家，并开发了一款同名的沉浸式AI科普空间，旨在通过学习文化科技知识，让孩子们开启智慧之门，并预警未来可能面临的挑战。\"快乐星球\"在这里代表的是一种寓教于乐、启发思考的科普教育理念。\\n\\n3. 在网络文化和流行语境中，\"快乐星球\"也可能是一个用于表达心情或状态的比喻，它代表的是一个让人感到满足和幸福的状态或者地方。\\n\\n综合以上信息，\"什么是快乐星球\"可以理解为一个寓意着快乐、知识、探索和梦想的概念，它可以是物理意义上的一个虚构星球，也可以是心理层面上的一种理想状态或环境。在不同的文化和语境中，它被赋予了不同的内涵和象征意义。'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\":\"什么是快乐星球?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40199a36-0314-43dc-aea0-4f8f5b4242e2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LCEL的语法结构就是:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "191d09a0-77c7-467f-81fe-0825df6dee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一位乐于助人的小助理\"),\n",
    "    (\"user\", \"什么是 {topic}\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "chat = ChatZhipuAI(\n",
    "    zhipuai_api_key = zhipuai_api_key,\n",
    "    model=\"glm-4\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "chain = prompt_template | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ee1d7a-596c-489c-a283-a6cfecc34f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"什么是快乐星球？\"这个问题原本来源于一部深受中国儿童喜爱的少儿科幻电视剧《快乐星球》。在电视剧的第五季中，有一首插曲名为《什么是快乐星球》，由主演马嘉祺演唱。这首歌曲因其朗朗上口的旋律和有趣的歌词，在网络上迅速流行开来，成为了一种文化现象。\\n\\n在更广泛的文化语境中，“什么是快乐星球”这句歌词被用来表达一种对无忧无虑、纯真快乐的追求和对童年美好记忆的怀念。它不仅仅是一句歌词，还成为了网络上的一种梗，人们用它来表达对生活中简单快乐的向往。\\n\\n而在2024年，根据上述参考信息，动画电影《快乐星球》正式立项，讲述了主角乐乐在老顽童爷爷的培养下成为一名AI科学家，并开发了同名的大型沉浸式AI科普空间，旨在通过学科学习，让孩子们了解文化和科技知识，开启智慧之门，同时预警未来可能面临的挑战。\\n\\n综上所述，“什么是快乐星球”既是一个源自少儿电视剧的流行文化现象，也指代了一个充满知识和智慧、能够带给孩子快乐成长体验的虚拟空间。这个概念鼓励孩子们对科学探索充满好奇，并通过寓教于乐的方式，激发他们对未来的想象力和创造力。', response_metadata={'token_usage': {'completion_tokens': 245, 'prompt_tokens': 1674, 'total_tokens': 1919}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-f26fc25d-bef1-41c0-8eff-5ab1dbce2201-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\":\"什么是快乐星球?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bff69-7018-47dd-82bd-53d56e6e5a51",
   "metadata": {},
   "source": [
    "> LangChain Docs:https://python.langchain.com/v0.2/docs/integrations/chat/zhipuai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2614811-5867-42af-84d8-9f8777e493c1",
   "metadata": {},
   "source": [
    "# 4. Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200aa33f-cc0f-49a6-a92c-e8112f80926b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Callbacks（回调函数）是一种编程模式，Callbacks的意义在于：允许程序在某个任务完成时自动执行另一个函数，而不必阻塞等待某个长时间运行的操作完成。所以它会在处理异步操作，如网络请求、文件读写或任何可能需要等待的操作时频繁被使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfc2d9-b691-4b39-8a8b-a5789a97d0fd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于与大模型的连续交互场景，一般采用的都是流式输出。但在做流式输出前，需要的就是回调 Callbacks。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4f97b-f50b-498f-8d27-f11ff628c3ce",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LangChain 提供了一个回调系统，其官网文档地址：https://python.langchain.com/v0.2/docs/concepts/#callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169518c-5d87-43dc-a2c7-16495313df47",
   "metadata": {},
   "source": [
    "&emsp;&emsp;回调处理程序可以是 sync（同步） 或 async （异步）的。同步回调和异步回调是编程中常用的两种处理事件或数据的方式，它们在执行时机和用途上有明显的区别：\n",
    "\n",
    "1. **同步回调（Synchronous Callbacks）**:\n",
    "   - **执行时机**：同步回调是在主程序流程中直接调用和执行的。当一个同步回调函数被触发时，程序会立即执行这个回调函数，并且在回调函数执行完成之前，主程序流程会被阻塞。\n",
    "   - **用途**：同步回调通常用于确保某些操作必须在程序继续执行前完成。例如，在访问数组中的每个元素并对其应用函数时，你可能会使用数组的`.forEach()`方法，这是一个同步的回调实现。\n",
    "\n",
    "2. **异步回调（Asynchronous Callbacks）**:\n",
    "   - **执行时机**：异步回调不会立即执行，它们通常被放置在事件队列中，等待当前执行堆栈清空后才开始执行。这意味着程序的主流程不会等待异步回调的完成，可以继续执行其他任务。\n",
    "   - **用途**：异步回调通常用在不希望阻塞主程序流程的情况下，例如处理I/O操作（如网络请求、文件读写），或者在执行大量计算时不影响用在并发编程和现代Web开发中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edb530-fbc7-46b4-8bdc-d92afda9e50d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在并发编程的Web开发中，交互式应用对话场景，肯定采用的是异步的回调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f3532-d317-44d6-b1c8-7007d0b8b1ee",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LangChain中的异步回调接口：https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0eb746-d4dd-40b8-a0a7-5b292a143d23",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当我们在执行运行时使用 callbacks 关键字 arg 传递 CallbackHandlers 时，这些回调将由执行中涉及的所有嵌套对象发出。例如，当处理程序传递给代理时，它将用于与代理相关的所有回调以及代理执行中涉及的所有对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7babc0b-4d99-4b60-8f9a-ee17f024f872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain RunnableSequence started\n",
      "Chain ChatPromptTemplate started\n",
      "Chain ended, outputs: messages=[HumanMessage(content='1 + 2等于多少?')]\n",
      "Chat model started\n",
      "Chat model ended, response: generations=[[ChatGeneration(text='1 + 2 等于 3。', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='1 + 2 等于 3。', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 12, 'total_tokens': 25}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-3330eeef-6755-4c7a-ba99-963028459089-0'))]] llm_output={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 12, 'total_tokens': 25}, 'model_name': 'glm-4'} run=None\n",
      "Chain ended, outputs: content='1 + 2 等于 3。' response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 12, 'total_tokens': 25}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run-3330eeef-6755-4c7a-ba99-963028459089-0'\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class LoggingHandler(BaseCallbackHandler):\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs\n",
    "    ) -> None:\n",
    "        print(\"Chat model started\")\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        print(f\"Chat model ended, response: {response}\")\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs\n",
    "    ) -> None:\n",
    "        print(f\"Chain {serialized.get('name')} started\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:\n",
    "        print(f\"Chain ended, outputs: {outputs}\")\n",
    "\n",
    "# 定义回调\n",
    "callbacks = [LoggingHandler()]\n",
    "\n",
    "# 实例化语言模型\n",
    "llm = ChatZhipuAI(\n",
    "    api_key=\"086a38e9141410d76e393ec52105c83b.7vBwRS4srgxpMRXU\",\n",
    "    model=\"glm-4\",)\n",
    "\n",
    "# 定义提示模板\n",
    "prompt = ChatPromptTemplate.from_template(\"1 + {number}等于多少?\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"number\": \"2\"}, config={\"callbacks\": callbacks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a8afc92-8fa3-48f8-9a96-d21b36c3cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1 + 2 等于 3。' response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 12, 'total_tokens': 25}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run-29c9593f-e1fe-4c86-a521-d47d6a1f993e-0'\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640df53c-4672-41af-a661-cb2009a82e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daa848c2-9b90-4006-a599-33cce80cab4d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;那我们尝试使用回调机制去流式调用GLM-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54747489-066a-47fe-8bba-6c4372f8d441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install httpx_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701dde22-4b22-4d03-951a-4114021af6b2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;httpx_sse是一个基于httpx的Server-Sent Events (SSE) 客户端库。SSE是一种服务器推送技术，允许服务器通过HTTP连接向客户端发送推送消息。这种技术常用于实时消息传递和通知，如股票价格更新、新闻更新或其他需要从服务器到客户端实时通信的应用场景。`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b917245-82cc-46c0-bc87-7935d09d9896",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# langchain_community\\chat_models\\zhipuai.py\n",
    "\n",
    "def connect_sse(client: Any, method: str, url: str, **kwargs: Any) -> Iterator \n",
    "    from httpx_sse import EventSour  \n",
    "        with client.stream(method, url, **kwargs) as response:\n",
    "            yield EventSource(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0558369f-ccb1-4f24-a1b2-0da280a0daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My custom handler, token: 好的\n",
      "My custom handler, token: ，\n",
      "My custom handler, token: 这里\n",
      "My custom handler, token: 有一个\n",
      "My custom handler, token: 关于\n",
      "My custom handler, token: 熊\n",
      "My custom handler, token: 的\n",
      "My custom handler, token: 笑话\n",
      "My custom handler, token: ：\n",
      "\n",
      "有一天\n",
      "My custom handler, token: ，\n",
      "My custom handler, token: 一只\n",
      "My custom handler, token: 熊\n",
      "My custom handler, token: 走进\n",
      "My custom handler, token: 了一家\n",
      "My custom handler, token: 餐厅\n",
      "My custom handler, token: ，\n",
      "My custom handler, token: 坐\n",
      "My custom handler, token: 下来\n",
      "My custom handler, token: 点\n",
      "My custom handler, token: 了一份\n",
      "My custom handler, token: 汉堡\n",
      "My custom handler, token: 。\n",
      "My custom handler, token: 服务员\n",
      "My custom handler, token: 看着\n",
      "My custom handler, token: 熊\n",
      "My custom handler, token: ，\n",
      "My custom handler, token: 惊讶\n",
      "My custom handler, token: 地问\n",
      "My custom handler, token: ：“\n",
      "My custom handler, token: 我们不\n",
      "My custom handler, token: 常见\n",
      "My custom handler, token: 到\n",
      "My custom handler, token: 熊\n",
      "My custom handler, token: 来\n",
      "My custom handler, token: 这儿\n",
      "My custom handler, token: 吃饭\n",
      "My custom handler, token: 。”\n",
      "My custom handler, token: \n",
      "熊\n",
      "My custom handler, token: 回答说\n",
      "My custom handler, token: ：“\n",
      "My custom handler, token: 我也\n",
      "My custom handler, token: 不会再\n",
      "My custom handler, token: 来了\n",
      "My custom handler, token: ，\n",
      "My custom handler, token: 你们的\n",
      "My custom handler, token: 菜单\n",
      "My custom handler, token: 上\n",
      "My custom handler, token: 没有\n",
      "My custom handler, token: ‘\n",
      "My custom handler, token: 熊\n",
      "My custom handler, token: 掌\n",
      "My custom handler, token: ’\n",
      "My custom handler, token: 汉堡\n",
      "My custom handler, token: 。”\n",
      "\n",
      "希望\n",
      "My custom handler, token: 这个\n",
      "My custom handler, token: 笑话\n",
      "My custom handler, token: 能\n",
      "My custom handler, token: 给你\n",
      "My custom handler, token: 带来\n",
      "My custom handler, token: 欢乐\n",
      "My custom handler, token: ！\n",
      "My custom handler, token: \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"My custom handler, token: {token}\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"请给我讲一个关于{animal}的笑话。\"])\n",
    "\n",
    "\n",
    "llm = ChatZhipuAI(\n",
    "    api_key=\"086a38e9141410d76e393ec52105c83b.7vBwRS4srgxpMRXU\",\n",
    "    model=\"glm-4\",\n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomHandler()],\n",
    "    )\n",
    "\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"animal\": \"熊\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f1569-9946-4740-b9d8-7b8438ea7d61",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述是我们自己定义的一个简单的流式输出，因为大模型普遍都需要流式输出需求，所以LangChain官方已经默认抽象出了一个流式输出的回调函数，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79083c68-724d-435c-9b80-60d22ea98d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b6c3030-e1d2-4cb7-8417-5d5fbb3ea2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_chat = ChatZhipuAI(\n",
    "    api_key=\"086a38e9141410d76e393ec52105c83b.7vBwRS4srgxpMRXU\",\n",
    "    model=\"glm-4\",\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c96cdade-c752-4f89-984e-f5d6c49f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    AIMessage(content=\"你好\"),\n",
    "    SystemMessage(content=\"你是一位诗人\"),\n",
    "    HumanMessage(content=\"请你根据我的输入,帮我写一首关于小鸭子落水的诗\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "943ed206-fc10-450e-8703-673077f98099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='你好'),\n",
       " SystemMessage(content='你是一位诗人'),\n",
       " HumanMessage(content='请你根据我的输入,帮我写一首关于小鸭子落水的诗')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "591b704d-2c5c-4f38-954b-df31f2fc84e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小鸭子落水，波纹荡漾间，\n",
      "毛茸茸身躯，轻盈似羽毛。\n",
      "湖光倒影里，它惊慌失措，\n",
      "振翅欲高飞，却跌入涟漪。\n",
      "\n",
      "冷水涌上岸，小鸭心惧怕，\n",
      "慌乱扑腾间，寻觅同伴影。\n",
      "援助之手伸，温暖抚心头，\n",
      "援救将它抱，安抚轻唤声。\n",
      "\n",
      "湖泊映晴空，小鸭重展翅，\n",
      "感恩救助者，勇气重燃起。\n",
      "从此翱翔天，不再惧风浪，\n",
      "成长岁月里，铭记恩情长。"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='小鸭子落水，波纹荡漾间，\\n毛茸茸身躯，轻盈似羽毛。\\n湖光倒影里，它惊慌失措，\\n振翅欲高飞，却跌入涟漪。\\n\\n冷水涌上岸，小鸭心惧怕，\\n慌乱扑腾间，寻觅同伴影。\\n援助之手伸，温暖抚心头，\\n援救将它抱，安抚轻唤声。\\n\\n湖泊映晴空，小鸭重展翅，\\n感恩救助者，勇气重燃起。\\n从此翱翔天，不再惧风浪，\\n成长岁月里，铭记恩情长。', response_metadata={'finish_reason': 'stop'}, id='run-54193938-49d5-4d18-8892-2a1d0e08f1a6-0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe064728-784b-4fae-9da5-bfc86be8dff6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4aa10-f482-4005-8c16-eeb165ae6c15",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`Memory`作为存储记忆数据的一个是抽象模块，其作为一个独立模块使用是没有任何意义的，因为本质上它的定位就是一个存储对话数据的空间。先抛开其内部实现的复杂性，我们可以回想一下：在定义链路的时候，每个链的内部都会根据其接收到的输入去定义其核心执行逻辑，比如在链内如何去调用外部工具，如何解析返回的数据格式等。其中链接收到的输入，可以直接来自用户，同时，也可以来自`Memory`模块。所以在这个过程中，一个链如果接入了`Memory`模块，其内部会与`Memory`模块进行两次交互：\n",
    "\n",
    "1. 收到用户输入之后，执行核心逻辑之前，链会读取`Memory`模块，拿到对应的数据，与用户输入的Prompt放在一起，执行接下来的逻辑。\n",
    "2. 执行核心逻辑之后，返回响应之前，链会将这个过程中产生的信息，写入`Memory`模块，以便在其他场景下能够引用到这些记忆数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7963f99-50e5-4f28-8c2e-d0172c1b0800",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由此可见，`Memory`模块和`Chains`模块是一对相互协作的关系，就像我们定义Prompt一样，不同的需求，需要构建不同的Prompt Template。那我们想记录下什么哪些数据，不论是用户输入的prompt，还是大模型的响应结果，亦或是链路的中间过程生成的数据，全部就由`Memory`这个抽象模块来完成。所以这个模块最**核心干的就是两件事：如何存储数据和如何提取数据，对应着就是两个基本操作：读和写。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130dbc51-b2ec-47e8-b9f7-19916db31489",
   "metadata": {},
   "source": [
    "&emsp;&emsp;理解了上述说明后，大家就能非常容易理解下面这张 LangChain Memory 模块的整体设计结构图："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae25903-ba4e-4c0a-8300-6942af175a94",
   "metadata": {},
   "source": [
    "> LangChain Memory Docs：https://python.langchain.com/docs/modules/memory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8808ded-4983-4e0c-956e-08fdc7360de1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202403271023031.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734574d-0caa-4c43-aba2-d4eaefc59b54",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在上述流程图中，`Model I/O`过程的本质上就是一个链路（chain），其配置时会设定`Prompt`、`Model`和`Output Parser`作为链路的主要逻辑。这个链路可以处理直接来自用户的`{question}`输入，也可以处理来自`Memory`模块读取的`{past_passages}`作为输入。执行完毕后，正常情况下会直接输出`{answer}`。但一旦集成了Memory模块，输出就会根据`Memory`中定义的逻辑被存储起来，供其他组件或流程使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b780cc-3b46-45d5-be92-c107bee05f53",
   "metadata": {},
   "source": [
    "&emsp;&emsp;详细的参数说明如下：\n",
    "\n",
    "```\n",
    "# 类继承关系：\n",
    "BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\n",
    "```\n",
    "\n",
    "```python\n",
    "class BaseMemory(Serializable, ABC):\n",
    "    \"\"\"Chains 中记忆的抽象基类。\n",
    "\n",
    "    记忆指的是 Chains 中的状态。记忆可用于存储关于 Chains 过去执行的信息，并将该信息注入到未来执行的 Chains 输入中。\n",
    "    例如，对于对话 Chains，记忆可用于存储对话并自动将其添加到未来模型提示中，以便模型具有必要的上下文来连贯地响应最新的输入。\n",
    "     \"\"\"\n",
    "    \n",
    "    # 下面是一些必须由子类实现的方法：\n",
    "    \n",
    "    \n",
    "    # 定义一个属性，任何从BaseMemory派生的子类都需要实现此方法。\n",
    "    # 此方法应返回该记忆类将添加到链输入的字符串键。\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def memory_variables(self) -> List[str]:\n",
    "        \"\"\"此记忆类将添加到链输入的字符串键列表。\"\"\"\n",
    "\n",
    "        \n",
    "    # 定义一个抽象方法。任何从BaseMemory派生的子类都需要实现此方法。\n",
    "    # 此方法基于给定的链输入返回键值对。\n",
    "    @abstractmethod\n",
    "    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"根据链的文本输入返回键值对。\"\"\"\n",
    "\n",
    "    \n",
    "    # 定义一个抽象方法。任何从BaseMemory派生的子类都需要实现此方法。\n",
    "    # 此方法将此链运行的上下文保存到内存。\n",
    "    @abstractmethod\n",
    "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n",
    "        \"\"\"将此链运行的上下文保存到记忆中。\"\"\"\n",
    "\n",
    "    # 定义一个抽象方法。任何从BaseMemory派生的子类都需要实现此方法。\n",
    "    # 此方法清除内存内容。\n",
    "    @abstractmethod\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"清除记忆内容。\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fc3172-70df-4a82-a2a5-259b271549aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181b7ca9-380e-49d9-9902-060150d418ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "\n",
    "chat = ChatZhipuAI(\n",
    "    api_key=\"086a38e9141410d76e393ec52105c83b.7vBwRS4srgxpMRXU\",\n",
    "    model=\"glm-4\",\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f00097c-1403-40f1-956b-e5536ce24caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"你好\")\n",
    "memory.chat_memory.add_ai_message(\"你好，请问有什么可以帮你的?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ad0f9a-2e94-4060-9919-434035a93dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 你好\\nAI: 你好，请问有什么可以帮你的?'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08c2ab0-e1e2-432f-a994-522df3f202d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "memory.chat_memory.add_user_message(\"你好\")\n",
    "memory.chat_memory.add_ai_message(\"你好，请问有什么可以帮你的?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7cfd95-6188-45a5-873d-e29c72a4d0a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human: 你好\\nAI: 你好，请问有什么可以帮你的?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45e67dd9-6390-46ac-ae50-f3a409d42cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "chat = ChatZhipuAI(\n",
    "    api_key=\"086a38e9141410d76e393ec52105c83b.7vBwRS4srgxpMRXU\",\n",
    "    model=\"glm-4\",\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"你是一个具有上下文记忆能力的对话机器人\"\n",
    "        ),\n",
    " \n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2ffb300-104c-4ff5-adf7-3956ccd72e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d345ca0-a9b9-4be8-8249-633111e2f725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 你是一个具有上下文记忆能力的对话机器人\n",
      "Human: 你好\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '你好',\n",
       " 'chat_history': [HumanMessage(content='你好'),\n",
       "  AIMessage(content='是的，作为一个先进的对话机器人，我具备一定的上下文记忆能力，能够在一定程度上理解和记住我们之前的对话内容，以便提供更加连贯和个性化的交流体验。不过，请注意，我的记忆能力是有限的，特别是在不同的会话中，我可能不会记住之前的所有信息。如果你有任何问题或需要帮助，请随时告诉我！')],\n",
       " 'text': '是的，作为一个先进的对话机器人，我具备一定的上下文记忆能力，能够在一定程度上理解和记住我们之前的对话内容，以便提供更加连贯和个性化的交流体验。不过，请注意，我的记忆能力是有限的，特别是在不同的会话中，我可能不会记住之前的所有信息。如果你有任何问题或需要帮助，请随时告诉我！'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"question\": \"你好\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8123f344-91dc-41a1-be01-a2033c22e80b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 你是一个具有上下文记忆能力的对话机器人\n",
      "Human: 你好\n",
      "AI: 是的，作为一个先进的对话机器人，我具备一定的上下文记忆能力，能够在一定程度上理解和记住我们之前的对话内容，以便提供更加连贯和个性化的交流体验。不过，请注意，我的记忆能力是有限的，特别是在不同的会话中，我可能不会记住之前的所有信息。如果你有任何问题或需要帮助，请随时告诉我！\n",
      "Human: 今天的天气怎么样？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '今天的天气怎么样？',\n",
       " 'chat_history': [HumanMessage(content='你好'),\n",
       "  AIMessage(content='是的，作为一个先进的对话机器人，我具备一定的上下文记忆能力，能够在一定程度上理解和记住我们之前的对话内容，以便提供更加连贯和个性化的交流体验。不过，请注意，我的记忆能力是有限的，特别是在不同的会话中，我可能不会记住之前的所有信息。如果你有任何问题或需要帮助，请随时告诉我！'),\n",
       "  HumanMessage(content='今天的天气怎么样？'),\n",
       "  AIMessage(content='根据我之前提供的信息，今天北京的天气是阴天，气温高达34.8摄氏度，湿度为35%，空气质量为130（可能属于轻度污染）。此外，东南风的风力为1级。请注意，这些信息是基于2024年6月12日的数据，如果你需要最新的天气信息，建议查看最近的天气预报。')],\n",
       " 'text': '根据我之前提供的信息，今天北京的天气是阴天，气温高达34.8摄氏度，湿度为35%，空气质量为130（可能属于轻度污染）。此外，东南风的风力为1级。请注意，这些信息是基于2024年6月12日的数据，如果你需要最新的天气信息，建议查看最近的天气预报。'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"question\": \"今天的天气怎么样？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "473aca0e-61d0-47ba-8997-d483037c76c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 你是一个具有上下文记忆能力的对话机器人\n",
      "Human: 你好\n",
      "AI: 是的，作为一个先进的对话机器人，我具备一定的上下文记忆能力，能够在一定程度上理解和记住我们之前的对话内容，以便提供更加连贯和个性化的交流体验。不过，请注意，我的记忆能力是有限的，特别是在不同的会话中，我可能不会记住之前的所有信息。如果你有任何问题或需要帮助，请随时告诉我！\n",
      "Human: 今天的天气怎么样？\n",
      "AI: 根据我之前提供的信息，今天北京的天气是阴天，气温高达34.8摄氏度，湿度为35%，空气质量为130（可能属于轻度污染）。此外，东南风的风力为1级。请注意，这些信息是基于2024年6月12日的数据，如果你需要最新的天气信息，建议查看最近的天气预报。\n",
      "Human: 现在适合出去玩吗？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '现在适合出去玩吗？',\n",
       " 'chat_history': [HumanMessage(content='你好'),\n",
       "  AIMessage(content='是的，作为一个先进的对话机器人，我具备一定的上下文记忆能力，能够在一定程度上理解和记住我们之前的对话内容，以便提供更加连贯和个性化的交流体验。不过，请注意，我的记忆能力是有限的，特别是在不同的会话中，我可能不会记住之前的所有信息。如果你有任何问题或需要帮助，请随时告诉我！'),\n",
       "  HumanMessage(content='今天的天气怎么样？'),\n",
       "  AIMessage(content='根据我之前提供的信息，今天北京的天气是阴天，气温高达34.8摄氏度，湿度为35%，空气质量为130（可能属于轻度污染）。此外，东南风的风力为1级。请注意，这些信息是基于2024年6月12日的数据，如果你需要最新的天气信息，建议查看最近的天气预报。'),\n",
       "  HumanMessage(content='现在适合出去玩吗？'),\n",
       "  AIMessage(content='根据您之前提供的信息，我无法提供具体的实时天气情况。但是，一般来说，是否适合出去玩取决于多种因素，包括天气状况、季节、个人喜好等。\\n\\n如果是炎热的夏天，早上或傍晚可能是外出游玩的最佳时间，因为这时气温相对较低。如果是阴天或者温度适宜，那么整天都可能适合户外活动。\\n\\n如果您担心天气或人流量，可以考虑以下建议：\\n- 检查最新的天气预报，了解当天的温度、降水概率、风力等信息。\\n- 选择室内活动或避开高峰时段，如果天气热或人太多。\\n- 如果是节假日，考虑选择人少的地方，以避免拥挤。\\n\\n总的来说，如果您计划出去玩，最好是提前规划并根据最新的天气情况做出决定。')],\n",
       " 'text': '根据您之前提供的信息，我无法提供具体的实时天气情况。但是，一般来说，是否适合出去玩取决于多种因素，包括天气状况、季节、个人喜好等。\\n\\n如果是炎热的夏天，早上或傍晚可能是外出游玩的最佳时间，因为这时气温相对较低。如果是阴天或者温度适宜，那么整天都可能适合户外活动。\\n\\n如果您担心天气或人流量，可以考虑以下建议：\\n- 检查最新的天气预报，了解当天的温度、降水概率、风力等信息。\\n- 选择室内活动或避开高峰时段，如果天气热或人太多。\\n- 如果是节假日，考虑选择人少的地方，以避免拥挤。\\n\\n总的来说，如果您计划出去玩，最好是提前规划并根据最新的天气情况做出决定。'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"question\": \"现在适合出去玩吗？\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
